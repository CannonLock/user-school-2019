{
    "docs": [
        {
            "location": "/",
            "text": "(Image copyright Ann Althouse, licensed under Creative Commons, \nhttp://www.flickr.com/photos/althouse/141467660/\n)\n\n\nOSG User School 2019\n\u00b6\n\n\nCould you transform your research with vast amounts of computing?\n\n\n\nCome spend a week at the beautiful University of Wisconsin\u2013Madison and learn how.\n\n\nDuring the school, July 15\u201319, you will learn to use high-throughput computing (HTC) systems\u00a0\u2014 at your\nown campus or using the national Open Science Grid (OSG)\u00a0\u2014 to run large-scale computing applications that are\nat the heart of today\u2019s cutting-edge science.  Through lectures, discussions, and \nlots of hands-on activities\n\nwith experienced OSG staff, you will learn how HTC systems work, how to run and manage lots of jobs and huge datasets to\nimplement a scientific computing workflow, and where to turn for more information and help.  Take a look at\n\nthe high-level curriculum and syllabus\n for more details.\n\n\nThe school is ideal for graduate students in any science or research domain where large-scale computing is a vital part\nof the research process, plus we will consider applications from advanced undergraduates, post-doctoral students,\nfaculty, and staff.  Students accepted to this program will receive financial support for basic travel and local costs\nassociated with the School.\n\n\nApplications\n\u00b6\n\n\nApplications are now closed and all applicants have been notified of their status.  We received a large number of\napplications this year, so thanks to everyone for your enthusiastic interest!\n\n\nInformation for Participants\n\u00b6\n\n\n\n\nCurriculum\n\n\nHigh-level curriculum\n\n\nDetailed schedule\n\n\nMaterials\n\n\n\n\n\n\nLogistics\n\n\nGeneral information and travel schedule\n\n\nVisa requirements for non-resident aliens\n\n\nTravel planning to and from Madison\n\n\nHotel information\n\n\nLocal transportation within Madison\n\n\nSchool location\n\n\nMeals at the School\n\n\nFun things to do in Madison\n and \nWednesday activities\n\n\n\n\n\n\n\n\nContact Us\n\u00b6\n\n\nThe OSG User School is part of the \nOSG Outreach Area\n\u00a0\u2014 please visit\nthat site to learn about other OSG Outreach activities.\n\n\nIf you have any questions about the School, the application process, or anything else, feel free to email us:\n\n\nuser-school@opensciencegrid.org\n\n\n \u00a0 \n \u00a0 OSGUserSchool",
            "title": "User School 2019"
        },
        {
            "location": "/#osg-user-school-2019",
            "text": "Could you transform your research with vast amounts of computing?  Come spend a week at the beautiful University of Wisconsin\u2013Madison and learn how.  During the school, July 15\u201319, you will learn to use high-throughput computing (HTC) systems\u00a0\u2014 at your\nown campus or using the national Open Science Grid (OSG)\u00a0\u2014 to run large-scale computing applications that are\nat the heart of today\u2019s cutting-edge science.  Through lectures, discussions, and  lots of hands-on activities \nwith experienced OSG staff, you will learn how HTC systems work, how to run and manage lots of jobs and huge datasets to\nimplement a scientific computing workflow, and where to turn for more information and help.  Take a look at the high-level curriculum and syllabus  for more details.  The school is ideal for graduate students in any science or research domain where large-scale computing is a vital part\nof the research process, plus we will consider applications from advanced undergraduates, post-doctoral students,\nfaculty, and staff.  Students accepted to this program will receive financial support for basic travel and local costs\nassociated with the School.",
            "title": "OSG User School 2019"
        },
        {
            "location": "/#applications",
            "text": "Applications are now closed and all applicants have been notified of their status.  We received a large number of\napplications this year, so thanks to everyone for your enthusiastic interest!",
            "title": "Applications"
        },
        {
            "location": "/#information-for-participants",
            "text": "Curriculum  High-level curriculum  Detailed schedule  Materials    Logistics  General information and travel schedule  Visa requirements for non-resident aliens  Travel planning to and from Madison  Hotel information  Local transportation within Madison  School location  Meals at the School  Fun things to do in Madison  and  Wednesday activities",
            "title": "Information for Participants"
        },
        {
            "location": "/#contact-us",
            "text": "The OSG User School is part of the  OSG Outreach Area \u00a0\u2014 please visit\nthat site to learn about other OSG Outreach activities.  If you have any questions about the School, the application process, or anything else, feel free to email us:  user-school@opensciencegrid.org   \u00a0   \u00a0 OSGUserSchool",
            "title": "Contact Us"
        },
        {
            "location": "/curriculum/overview/",
            "text": "Curriculum and Syllabus\n\u00b6\n\n\nCurriculum Goals\n\u00b6\n\n\nAt a high level, the goal of the School is to help students learn to:\n\n\n\n\nDescribe the basic elements and architecture of a distributed computing system\n\n\nUse basic distributed computing tools to run jobs and manage data\n\n\nSelect reasonable tools and methods to solve scientific computing problems using distributed computing\n\n\nOutline the role of distributed computing, its history, current state and issues, and hopes for the future\n\n\nIdentify resources for support, further study, and development opportunities in distributed computing\n\n\n\n\nSyllabus\n\u00b6\n\n\nThe high-level syllabus for 2019 is below; a more detailed schedule will be published later as we get closer to the\nschool.\n\n\n\n\n\n\n\n\n\n\nMorning\n\n\nAfternoon\n\n\n\n\n\n\n\n\n\n\nMonday\n\n\nWelcome\nIntroduction to high-throughput computing\nRunning jobs locally with Condor\n\n\nIntroduction to distributed high-throughput computing\n\n\n\n\n\n\nTuesday\n\n\nBasic troubleshooting\nSecurity\n\n\nDealing with real software\n(Especially Python and MATLAB, but others as well)\n\n\n\n\n\n\nWednesday\n\n\nSoftware in the OSG\n\n\nFree choice:\n\u2022\u00a0Advanced topics (HTCondor and Python, GPUs)\n\u2022\u00a0Get one-on-one help with your computing work\n\u2022\u00a0Take a break and visit Madison\n\n\n\n\n\n\nThursday\n\n\nIntroduction to distributed storage\nUsing remote storage systems\n\n\nManaging large, distributed data\nIntroduction to high-throughput workflows\n\n\n\n\n\n\nFriday\n\n\nTurning scientific computing needs into HTC jobs\nEstimating resource needs, decomposing and running large jobs\nStrategies and technologies for handling large workflows\n\n\nPrinciples of high-throughput computing\nScientific computing showcase\nWhere to go and what to do next \u2014 resources, funding, etc.",
            "title": "High-level overview"
        },
        {
            "location": "/curriculum/overview/#curriculum-and-syllabus",
            "text": "",
            "title": "Curriculum and Syllabus"
        },
        {
            "location": "/curriculum/overview/#curriculum-goals",
            "text": "At a high level, the goal of the School is to help students learn to:   Describe the basic elements and architecture of a distributed computing system  Use basic distributed computing tools to run jobs and manage data  Select reasonable tools and methods to solve scientific computing problems using distributed computing  Outline the role of distributed computing, its history, current state and issues, and hopes for the future  Identify resources for support, further study, and development opportunities in distributed computing",
            "title": "Curriculum Goals"
        },
        {
            "location": "/curriculum/overview/#syllabus",
            "text": "The high-level syllabus for 2019 is below; a more detailed schedule will be published later as we get closer to the\nschool.      Morning  Afternoon      Monday  Welcome Introduction to high-throughput computing Running jobs locally with Condor  Introduction to distributed high-throughput computing    Tuesday  Basic troubleshooting Security  Dealing with real software (Especially Python and MATLAB, but others as well)    Wednesday  Software in the OSG  Free choice: \u2022\u00a0Advanced topics (HTCondor and Python, GPUs) \u2022\u00a0Get one-on-one help with your computing work \u2022\u00a0Take a break and visit Madison    Thursday  Introduction to distributed storage Using remote storage systems  Managing large, distributed data Introduction to high-throughput workflows    Friday  Turning scientific computing needs into HTC jobs Estimating resource needs, decomposing and running large jobs Strategies and technologies for handling large workflows  Principles of high-throughput computing Scientific computing showcase Where to go and what to do next \u2014 resources, funding, etc.",
            "title": "Syllabus"
        },
        {
            "location": "/curriculum/detailed-schedule/",
            "text": "table.schedule { border-collapse: collapse; margin: 1em 0 1em 2em; }\n  th, td { padding: 2px 0; vertical-align: top; }\n  th + th, th + td, td + td { padding-left: 1em; }\n  th.side { text-align: right; }\n  td.time { text-align: right; }\n  .hi { color: #F60; /\\* font-weight: bold; \\*/ }\n  tr.meal { background-color: #FFEEBF; }\n  tr.break { background-color: #D9F0FF; }\n\n\n\n\nOSG User School Schedule\n\u00b6\n\n\nSchedule subject to change prior to the start of the school.\n\n\nAll events happen in the Computer Sciences building (1210 West Dayton Street), Room 1240, except when noted otherwise.\n\n\nSunday, 14 July 2019\n\u00b6\n\n\nWelcome Dinner for Students and Staff\n\n\n\n    \n\n        \nLocation\n\n        \nUnion South (1308 W Dayton St)\n, 3rd Floor, Northwoods Room\n\n    \n\n    \n\n        \nTime\n\n        \n6:30 p.m.\n\n    \n\n\n\n\n\nWe will have instructors at the hotel lobbies prior to dinner, and walk with\ngroups to Union South.  Join us for the walk if you want, or otherwise meet us\nin the dinner room by 6:30 p.m.\n\n\nFor those joining us to walk over together, instructors will be at the hotel\nlobby by ~5:40\u00a0p.m. and will leave for Union South at around 6:00 p.m.\n\n\nMonday, 15 July 2018\n\u00b6\n\n\nMain Idea:\n Introduction to High Throughput Computing\n\n\n\n\n\n\n\n\nStart\n\n\nEnd\n\n\nEvent\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n8:00\n\n\n8:45\n\n\nBreakfast and group discussion\n\n\n\n\n\n\n\n\n9:00\n\n\n9:15\n\n\nWelcome\n\n\nTim Cartwright\n\n\n\n\n\n\n9:15\n\n\n9:45\n\n\nIntroduction to High Throughput Computing (lecture)\n\n\nLauren Michael\n\n\n\n\n\n\n9:45\n\n\n10:30\n\n\nRunning jobs locally with HTCondor, Part 1 (hands-on)\n\n\n\"\n\n\n\n\n\n\n10:30\n\n\n10:45\n\n\nBreak\n\n\n\n\n\n\n\n\n10:45\n\n\n11:15\n\n\nHigh Throughput Computing with HTCondor (lecture)\n\n\nLauren Michael\n\n\n\n\n\n\n11:15\n\n\n12:30\n\n\nRunning jobs locally with HTCondor, Part 2 (hands-on)\n\n\n\"\n\n\n\n\n\n\n12:30\n\n\n1:30\n\n\nLunch\n\n\n\n\n\n\n\n\n1:30\n\n\n2:00\n\n\nHTCondor job matching and automation (lecture)\n\n\nLauren Michael\n\n\n\n\n\n\n2:00\n\n\n3:00\n\n\nJob attributes and automation (hands-on)\n\n\n\"\n\n\n\n\n\n\n3:00\n\n\n3:15\n\n\nBreak\n\n\n\n\n\n\n\n\n3:15\n\n\n4:00\n\n\nIntroduction to DHTC, overlay systems, and OSG (lecture)\n\n\nBrian Lin\n\n\n\n\n\n\n4:00\n\n\n5:00\n\n\nRunning jobs on OSG (hands-on)\n\n\n\"\n\n\n\n\n\n\n5:00\n\n\n\u2014\n\n\nOn your own\n\n\n\n\n\n\n\n\n7:00\n\n\n9:00\n\n\nEvening work session (optional)\nUnion South, room TBD\n\n\nTBD\n\n\n\n\n\n\n\n\nTuesday, 16 July 2019\n\u00b6\n\n\nMain Ideas:\n Refining Your HTC Skills: Troubleshooting, Security, Software\n\n\n\n\n\n\n\n\nStart\n\n\nEnd\n\n\nEvent\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n8:00\n\n\n8:45\n\n\nBreakfast and group discussion\n\n\n\n\n\n\n\n\n9:00\n\n\n9:30\n\n\nTroubleshooting (lecture)\n\n\nBrian Lin\n\n\n\n\n\n\n9:30\n\n\n10:30\n\n\nTroubleshooting (hands-on)\n\n\n\"\n\n\n\n\n\n\n10:30\n\n\n10:45\n\n\nBreak\n\n\n\n\n\n\n\n\n10:45\n\n\n12:00\n\n\nHigh Throughput Computing in action (interactive)\n\n\nGroup\n\n\n\n\n\n\n12:00\n\n\n12:15\n\n\nSecurity in the OSG (lecture)\n\n\nBrian Lin\n\n\n\n\n\n\n12:15\n\n\n1:15\n\n\nLunch\n\n\n\n\n\n\n\n\n1:15\n\n\n1:45\n\n\nSoftware portability considerations\n\n\nChristina Koch\n\n\n\n\n\n\n1:45\n\n\n3:00\n\n\nCompiled software and wrapper scripts (hands-on)\n\n\n\"\n\n\n\n\n\n\n3:00\n\n\n3:15\n\n\nBreak\n\n\n\n\n\n\n\n\n3:15\n\n\n3:45\n\n\nPortability for non-compiled languages (lecture)\n\n\nChristina Koch\n\n\n\n\n\n\n3:45\n\n\n5:00\n\n\nMatlab and Python portability (hands-on)\n\n\n\"\n\n\n\n\n\n\n5:00\n\n\n\u2014\n\n\nOn your own\n\n\n\n\n\n\n\n\n7:00\n\n\n9:00\n\n\nEvening work session (optional, terrace if good weather)\nUnion South, room TBD\n\n\nTBD\n\n\n\n\n\n\n\n\nWednesday, 17 July 2019\n\u00b6\n\n\nMain Ideas:\n More Software Techniques\n\n\n\n\n\n\n\n\nStart\n\n\nEnd\n\n\nEvent\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n8:00\n\n\n8:45\n\n\nBreakfast and group discussion\n\n\n\n\n\n\n\n\n9:00\n\n\n9:30\n\n\nOSG-available software (lecture)\n\n\nChristina Koch\n\n\n\n\n\n\n9:30\n\n\n10:45\n\n\nUsing OASIS modules (hands-on)\n\n\n\"\n\n\n\n\n\n\n10:45\n\n\n11:00\n\n\nBreak\n\n\n\n\n\n\n\n\n11:00\n\n\n11:15\n\n\nContainers and licensing challenges (lecture)\n\n\nChristina Koch\n\n\n\n\n\n\n11:15\n\n\n12:15\n\n\nRunning jobs in containers (hands-on; optional or catch up)\n\n\n\"\n\n\n\n\n\n\n12:15\n\n\n1:15\n\n\nLunch\n\n\n\n\n\n\n\n\n1:15\n\n\n5:00\n\n\nFree choice (many options provided or suggested)\n\n\nGroup\n\n\n\n\n\n\n5:00\n\n\n\u2014\n\n\nOn your own\n\n\n\n\n\n\n\n\n7:00\n\n\n9:00\n\n\nEvening work session (optional, or concert)\nUnion South, room TBD\n\n\nTBD\n\n\n\n\n\n\n\n\nThursday, 18 July 2019\n\u00b6\n\n\nMain Ideas:\n Data (Big, and Otherwise); Workflows\n\n\n\n\n\n\n\n\nStart\n\n\nEnd\n\n\nEvent\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n8:00\n\n\n8:45\n\n\nBreakfast and group discussion\n\n\n\n\n\n\n\n\n9:00\n\n\n9:30\n\n\nHTC considerations for \"big data\"? (lecture)\n\n\nBrian Lin\n\n\n\n\n\n\n9:30\n\n\n10:30\n\n\nDetermining data requirements (hands-on)\n\n\n\"\n\n\n\n\n\n\n10:30\n\n\n10:45\n\n\nBreak\n\n\n\n\n\n\n\n\n10:45\n\n\n11:15\n\n\nSolutions for large input data (lecture)\n\n\nBrian Lin\n\n\n\n\n\n\n11:15\n\n\n12:30\n\n\nHandling large input data (hands-on)\n\n\n\"\n\n\n\n\n\n\n12:30\n\n\n1:30\n\n\nLunch\n\n\n\n\n\n\n\n\n1:30\n\n\n2:00\n\n\nLarge output and local file systems (lecture)\n\n\nBrian Lin\n\n\n\n\n\n\n2:00\n\n\n3:00\n\n\nLeveraging shared file systems (hands-on)\n\n\n\"\n\n\n\n\n\n\n3:00\n\n\n3:15\n\n\nBreak\n\n\n\n\n\n\n\n\n3:15\n\n\n3:45\n\n\nWorkflows with HTCondor DAGMan (lecture)\n\n\nLauren Michael\n\n\n\n\n\n\n3:45\n\n\n5:00\n\n\nRunning workflows locally with DAGMan (hands-on)\n\n\n\"\n\n\n\n\n\n\n5:00\n\n\n\u2014\n\n\nOn your own\n\n\n\n\n\n\n\n\n7:00\n\n\n9:00\n\n\nEvening work session (optional)\nUnion South, room TBD\n\n\nTBD\n\n\n\n\n\n\n\n\nFriday, 19 July 2019\n\u00b6\n\n\nMain Ideas:\n Science Workflows; HTC Showcase; HTC Principles\n\n\n\n\n\n\n\n\nStart\n\n\nEnd\n\n\nEvent\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n8:00\n\n\n8:45\n\n\nBreakfast and group discussion\n\n\n\n\n\n\n\n\n9:00\n\n\n9:30\n\n\nFrom science to workflow (lecture)\n\n\nChristina Koch\n\n\n\n\n\n\n9:30\n\n\n10:15\n\n\nWorkflow development (hands-on)\n\n\n\"\n\n\n\n\n\n\n10:15\n\n\n10:30\n\n\nBreak\n\n\n\n\n\n\n\n\n10:30\n\n\n10:50\n\n\nFrom workflow to production (lecture)\n\n\nChristina Koch\n\n\n\n\n\n\n10:50\n\n\n11:55\n\n\nWorkflow execution (hands-on)\n\n\n\"\n\n\n\n\n\n\n11:55\n\n\n12:40\n\n\nLunch\n\n\n\n\n\n\n\n\n12:40\n\n\n1:05\n\n\nGroup photo; Tour of Wisconsin Institutes for Discovery\n\n\nLauren/Christina\n\n\n\n\n\n\n1:05\n\n\n1:15\n\n\nWalk back to Comp Sci\n\n\n\n\n\n\n\n\n1:15\n\n\n2:55\n\n\nHigh-throughput computing showcase\n\n\n\n\n\n\n\n\n1:20\n\n\n1:40\n\n\nHigh Throughput Computing in Your Backyard: Urban Hydrology Applications\n\n\nCarolyn Voter\n\n\n\n\n\n\n1:45\n\n\n2:05\n\n\nUsing HTC to Understand and Communicate Appropriate Uses of Cross-Validation in Psychological Science\n\n\nSarah Sant\u2019Ana\n\n\n\n\n\n\n2:10\n\n\n2:30\n\n\nA Self-Stabilizing Metasurface Laser Sail to Explore the Stars\n\n\nJoel Siegel\n\n\n\n\n\n\n2:35\n\n\n2:55\n\n\nMeasuring Plant Phenotypes with High Throughput Computing\n\n\nEdgar Spalding\n\n\n\n\n\n\n2:55\n\n\n3:25\n\n\nBreak and Q&A\n\n\n\n\n\n\n\n\n3:25\n\n\n4:40\n\n\nPrinciples for High Throughput Computing (lecture)\n\n\nTim Cartwright\n\n\n\n\n\n\n4:40\n\n\n4:45\n\n\nShort break\n\n\n\n\n\n\n\n\n4:45\n\n\n5:15\n\n\nWhere to go and what to do next?\n\n\nTim Cartwright\n\n\n\n\n\n\n5:15\n\n\n6:30\n\n\nOn your own\n\n\n\n\n\n\n\n\n\n\nClosing Dinner for Students and Staff\n\n\n\n  \n\n    \nLocation\n\n    \nFluno Center (601 University Avenue)\n, Skyview Room\n\n  \n\n  \n\n    \nTime\n\n    \n6:30 p.m.",
            "title": "Detailed schedule"
        },
        {
            "location": "/curriculum/detailed-schedule/#osg-user-school-schedule",
            "text": "Schedule subject to change prior to the start of the school.  All events happen in the Computer Sciences building (1210 West Dayton Street), Room 1240, except when noted otherwise.",
            "title": "OSG User School Schedule"
        },
        {
            "location": "/curriculum/detailed-schedule/#sunday-14-july-2019",
            "text": "Welcome Dinner for Students and Staff  \n     \n         Location \n         Union South (1308 W Dayton St) , 3rd Floor, Northwoods Room \n     \n     \n         Time \n         6:30 p.m. \n       We will have instructors at the hotel lobbies prior to dinner, and walk with\ngroups to Union South.  Join us for the walk if you want, or otherwise meet us\nin the dinner room by 6:30 p.m.  For those joining us to walk over together, instructors will be at the hotel\nlobby by ~5:40\u00a0p.m. and will leave for Union South at around 6:00 p.m.",
            "title": "Sunday, 14 July 2019"
        },
        {
            "location": "/curriculum/detailed-schedule/#monday-15-july-2018",
            "text": "Main Idea:  Introduction to High Throughput Computing     Start  End  Event  Instructor      8:00  8:45  Breakfast and group discussion     9:00  9:15  Welcome  Tim Cartwright    9:15  9:45  Introduction to High Throughput Computing (lecture)  Lauren Michael    9:45  10:30  Running jobs locally with HTCondor, Part 1 (hands-on)  \"    10:30  10:45  Break     10:45  11:15  High Throughput Computing with HTCondor (lecture)  Lauren Michael    11:15  12:30  Running jobs locally with HTCondor, Part 2 (hands-on)  \"    12:30  1:30  Lunch     1:30  2:00  HTCondor job matching and automation (lecture)  Lauren Michael    2:00  3:00  Job attributes and automation (hands-on)  \"    3:00  3:15  Break     3:15  4:00  Introduction to DHTC, overlay systems, and OSG (lecture)  Brian Lin    4:00  5:00  Running jobs on OSG (hands-on)  \"    5:00  \u2014  On your own     7:00  9:00  Evening work session (optional) Union South, room TBD  TBD",
            "title": "Monday, 15 July 2018"
        },
        {
            "location": "/curriculum/detailed-schedule/#tuesday-16-july-2019",
            "text": "Main Ideas:  Refining Your HTC Skills: Troubleshooting, Security, Software     Start  End  Event  Instructor      8:00  8:45  Breakfast and group discussion     9:00  9:30  Troubleshooting (lecture)  Brian Lin    9:30  10:30  Troubleshooting (hands-on)  \"    10:30  10:45  Break     10:45  12:00  High Throughput Computing in action (interactive)  Group    12:00  12:15  Security in the OSG (lecture)  Brian Lin    12:15  1:15  Lunch     1:15  1:45  Software portability considerations  Christina Koch    1:45  3:00  Compiled software and wrapper scripts (hands-on)  \"    3:00  3:15  Break     3:15  3:45  Portability for non-compiled languages (lecture)  Christina Koch    3:45  5:00  Matlab and Python portability (hands-on)  \"    5:00  \u2014  On your own     7:00  9:00  Evening work session (optional, terrace if good weather) Union South, room TBD  TBD",
            "title": "Tuesday, 16 July 2019"
        },
        {
            "location": "/curriculum/detailed-schedule/#wednesday-17-july-2019",
            "text": "Main Ideas:  More Software Techniques     Start  End  Event  Instructor      8:00  8:45  Breakfast and group discussion     9:00  9:30  OSG-available software (lecture)  Christina Koch    9:30  10:45  Using OASIS modules (hands-on)  \"    10:45  11:00  Break     11:00  11:15  Containers and licensing challenges (lecture)  Christina Koch    11:15  12:15  Running jobs in containers (hands-on; optional or catch up)  \"    12:15  1:15  Lunch     1:15  5:00  Free choice (many options provided or suggested)  Group    5:00  \u2014  On your own     7:00  9:00  Evening work session (optional, or concert) Union South, room TBD  TBD",
            "title": "Wednesday, 17 July 2019"
        },
        {
            "location": "/curriculum/detailed-schedule/#thursday-18-july-2019",
            "text": "Main Ideas:  Data (Big, and Otherwise); Workflows     Start  End  Event  Instructor      8:00  8:45  Breakfast and group discussion     9:00  9:30  HTC considerations for \"big data\"? (lecture)  Brian Lin    9:30  10:30  Determining data requirements (hands-on)  \"    10:30  10:45  Break     10:45  11:15  Solutions for large input data (lecture)  Brian Lin    11:15  12:30  Handling large input data (hands-on)  \"    12:30  1:30  Lunch     1:30  2:00  Large output and local file systems (lecture)  Brian Lin    2:00  3:00  Leveraging shared file systems (hands-on)  \"    3:00  3:15  Break     3:15  3:45  Workflows with HTCondor DAGMan (lecture)  Lauren Michael    3:45  5:00  Running workflows locally with DAGMan (hands-on)  \"    5:00  \u2014  On your own     7:00  9:00  Evening work session (optional) Union South, room TBD  TBD",
            "title": "Thursday, 18 July 2019"
        },
        {
            "location": "/curriculum/detailed-schedule/#friday-19-july-2019",
            "text": "Main Ideas:  Science Workflows; HTC Showcase; HTC Principles     Start  End  Event  Instructor      8:00  8:45  Breakfast and group discussion     9:00  9:30  From science to workflow (lecture)  Christina Koch    9:30  10:15  Workflow development (hands-on)  \"    10:15  10:30  Break     10:30  10:50  From workflow to production (lecture)  Christina Koch    10:50  11:55  Workflow execution (hands-on)  \"    11:55  12:40  Lunch     12:40  1:05  Group photo; Tour of Wisconsin Institutes for Discovery  Lauren/Christina    1:05  1:15  Walk back to Comp Sci     1:15  2:55  High-throughput computing showcase     1:20  1:40  High Throughput Computing in Your Backyard: Urban Hydrology Applications  Carolyn Voter    1:45  2:05  Using HTC to Understand and Communicate Appropriate Uses of Cross-Validation in Psychological Science  Sarah Sant\u2019Ana    2:10  2:30  A Self-Stabilizing Metasurface Laser Sail to Explore the Stars  Joel Siegel    2:35  2:55  Measuring Plant Phenotypes with High Throughput Computing  Edgar Spalding    2:55  3:25  Break and Q&A     3:25  4:40  Principles for High Throughput Computing (lecture)  Tim Cartwright    4:40  4:45  Short break     4:45  5:15  Where to go and what to do next?  Tim Cartwright    5:15  6:30  On your own      Closing Dinner for Students and Staff  \n   \n     Location \n     Fluno Center (601 University Avenue) , Skyview Room \n   \n   \n     Time \n     6:30 p.m.",
            "title": "Friday, 19 July 2019"
        },
        {
            "location": "/curriculum/assignment/",
            "text": "OSG User School 2019 Final Assignment\n\u00b6\n\n\nThe School focused on using high-throughput computing (HTC) to support and transform scientific inquiry.  Your final\nassignment asks you to demonstrate what you have learned by describing how you would apply your new \nknowledge to a challenge in your scientific domain that requires significant\ncomputation.  Our goals for this assignment are: \n\n\n\n\nTo reinforce and consolidate what you learned at the School\n\n\nPrepare you to take real action on your large-scale computational challenge(s)\n\n\nDemonstrate the value of the School to our funding agencies and to your advisor, colleagues, etc.\n\n\nGuide us as we try to improve the School\n\n\n\n\nFormat\n\u00b6\n\n\nThe assignment is a short paper.  Please follow these guidelines: \n\n\n\n\nThe paper should address the main content points listed in the Content section below. \n\n\nThere are no precise length requirements, but 1000\u20131500 words is a good length to aim for\n\n\nPictures, charts, and diagrams are good, if they are appropriate and clear\n\n\nThe paper does not need to be journal-ready, but should be good quality and ready for public display\n\n\n\n\nThis does not have to be a formal research paper or journal article - while \nyou are welcome to include references and other information, our expectation is more \nalong the lines of an informal whitepaper, proposal, or internal report. \n\n\nSubmit your paper in as a PDF \u2014 no Word or LaTeX documents, please!\n\n\n\n\nIf you are not sure how to make a PDF, consult your department or campus IT staff for help\n\n\nEmail the PDF to the \nuser-school@opensciencegrid.org\n list\n\n\nIf the PDF is really huge, contact us and we will find another way to transfer the file (we \nshould\n be able to manage large data, right??)\n\n\n\n\nContent\n\u00b6\n\n\nThis paper should describe how you would use large-scale computational resources (including, but not \nlimited to the OSG) to approach a relevant scientific challenge. \n\n\nFirst, choose a challenge or project to present.  An ideal topic:\n\n\n\n\nIs important to you and your advisor, team, department, or field\n\n\nRepresents work that is in progress or is planned to start soon\n\n\nRequires significant computational resources\n\n\n\n\nOften, school participants use whatever research project they are currently working on \nor planning to begin soon. \n\n\nNext, think about your topic and how to apply what you learned during the School.  Think about how to approach the\ncomputational needs of the project using local HTC resources or the Open Science Grid (OSG).  We are not asking you to\nimplement the system!  Just imagine how you would do it.  One approach is fine, but more than one approach is fine, too.\nImagine that you will run on the resources available to you at your own institution.  If your institution does not have\na HTC system available, then think about what kind of resources you would want or how you could get access to resources\nvia the OSG.\n\n\nHaving considered your project and how to approach it computationally, your assignment \nshould then specifically address the following topics (you can use these points as an\noutline to structure your assignment, if you like): \n\n\n\n\nThe science challenge (\nabout 1/3 of the assignment\n) -- described for a general scientific audience, \nnot\n people from your field\n\n\nWhat science do you work on?\n\n\nWithin that topic, what specific challenge or question do you (want to) work on?\n\n\nWhy does that challenge or question require significant computing resources to solve? Be specific! \n\n\n\n\n\n\nThe computational plan (\nabout 2/3 of the assignment\n)\n\n\nHow would you approach your scientific challenge with computing? Summarize your approach and explain why you think it is good. \n\n\nEstimate the resources (CPUs, time, memory, disk, etc.) that you need to work on your challenge. \n\n\nDescribe in some detail a plan or proposal to use computing tools to work on your challenge (more than one plan is OK, especially if you need resources beyond the OSG)\n\n\nMake sure to highlight specific practices and HTCondor features that you need and that you learned in the School\n\n\n\n\n\n\n\n\nIn touching on each of the points above, it may be helpful to include information that addresses\nquestions like these: \n\n\n\n\nWhat local resources do you have access to?\n\n\nWhich parts of the work will be prepared or run on your laptop or on non-OSG resources, and which parts can run on OSG?\n\n\nHow would you turn your project into actual jobs?\n\n\nWhat are the resource needs of the jobs themselves?\n\n\nWhat sort of workflow, if any, would you use? Are there manual steps in your overall workflow? Could they be automated (e.g., with DAGMan)?\n\n\nHow much data do you need to move around? Which type of data situation do you have? What is your plan for data management?\n\n\nDo you think your project is better suited for HTC or HPC? Why?\n\n\nWhat security or privacy concerns do you have with your project? Do you need to do anything special regarding security?\n\n\nHow would your science be transformed by increasing the amount of computation you can use?\n\n\n\n\nDeadline\n\u00b6\n\n\nThe paper is due 31 August 2019.  We will consider individual requests for a time extension, but you need a good reason.\nTalk to us about the deadline, if it seems like a problem.\n\n\nQuestions?\n\u00b6\n\n\nIf you have any questions or comments about the assignment, please contact us at the \nuser-school@opensciencegrid.org\n\nmailing list.",
            "title": "Final assignment"
        },
        {
            "location": "/curriculum/assignment/#osg-user-school-2019-final-assignment",
            "text": "The School focused on using high-throughput computing (HTC) to support and transform scientific inquiry.  Your final\nassignment asks you to demonstrate what you have learned by describing how you would apply your new \nknowledge to a challenge in your scientific domain that requires significant\ncomputation.  Our goals for this assignment are:    To reinforce and consolidate what you learned at the School  Prepare you to take real action on your large-scale computational challenge(s)  Demonstrate the value of the School to our funding agencies and to your advisor, colleagues, etc.  Guide us as we try to improve the School",
            "title": "OSG User School 2019 Final Assignment"
        },
        {
            "location": "/curriculum/assignment/#format",
            "text": "The assignment is a short paper.  Please follow these guidelines:    The paper should address the main content points listed in the Content section below.   There are no precise length requirements, but 1000\u20131500 words is a good length to aim for  Pictures, charts, and diagrams are good, if they are appropriate and clear  The paper does not need to be journal-ready, but should be good quality and ready for public display   This does not have to be a formal research paper or journal article - while \nyou are welcome to include references and other information, our expectation is more \nalong the lines of an informal whitepaper, proposal, or internal report.   Submit your paper in as a PDF \u2014 no Word or LaTeX documents, please!   If you are not sure how to make a PDF, consult your department or campus IT staff for help  Email the PDF to the  user-school@opensciencegrid.org  list  If the PDF is really huge, contact us and we will find another way to transfer the file (we  should  be able to manage large data, right??)",
            "title": "Format"
        },
        {
            "location": "/curriculum/assignment/#content",
            "text": "This paper should describe how you would use large-scale computational resources (including, but not \nlimited to the OSG) to approach a relevant scientific challenge.   First, choose a challenge or project to present.  An ideal topic:   Is important to you and your advisor, team, department, or field  Represents work that is in progress or is planned to start soon  Requires significant computational resources   Often, school participants use whatever research project they are currently working on \nor planning to begin soon.   Next, think about your topic and how to apply what you learned during the School.  Think about how to approach the\ncomputational needs of the project using local HTC resources or the Open Science Grid (OSG).  We are not asking you to\nimplement the system!  Just imagine how you would do it.  One approach is fine, but more than one approach is fine, too.\nImagine that you will run on the resources available to you at your own institution.  If your institution does not have\na HTC system available, then think about what kind of resources you would want or how you could get access to resources\nvia the OSG.  Having considered your project and how to approach it computationally, your assignment \nshould then specifically address the following topics (you can use these points as an\noutline to structure your assignment, if you like):    The science challenge ( about 1/3 of the assignment ) -- described for a general scientific audience,  not  people from your field  What science do you work on?  Within that topic, what specific challenge or question do you (want to) work on?  Why does that challenge or question require significant computing resources to solve? Be specific!     The computational plan ( about 2/3 of the assignment )  How would you approach your scientific challenge with computing? Summarize your approach and explain why you think it is good.   Estimate the resources (CPUs, time, memory, disk, etc.) that you need to work on your challenge.   Describe in some detail a plan or proposal to use computing tools to work on your challenge (more than one plan is OK, especially if you need resources beyond the OSG)  Make sure to highlight specific practices and HTCondor features that you need and that you learned in the School     In touching on each of the points above, it may be helpful to include information that addresses\nquestions like these:    What local resources do you have access to?  Which parts of the work will be prepared or run on your laptop or on non-OSG resources, and which parts can run on OSG?  How would you turn your project into actual jobs?  What are the resource needs of the jobs themselves?  What sort of workflow, if any, would you use? Are there manual steps in your overall workflow? Could they be automated (e.g., with DAGMan)?  How much data do you need to move around? Which type of data situation do you have? What is your plan for data management?  Do you think your project is better suited for HTC or HPC? Why?  What security or privacy concerns do you have with your project? Do you need to do anything special regarding security?  How would your science be transformed by increasing the amount of computation you can use?",
            "title": "Content"
        },
        {
            "location": "/curriculum/assignment/#deadline",
            "text": "The paper is due 31 August 2019.  We will consider individual requests for a time extension, but you need a good reason.\nTalk to us about the deadline, if it seems like a problem.",
            "title": "Deadline"
        },
        {
            "location": "/curriculum/assignment/#questions",
            "text": "If you have any questions or comments about the assignment, please contact us at the  user-school@opensciencegrid.org \nmailing list.",
            "title": "Questions?"
        },
        {
            "location": "/materials/",
            "text": "OSG User School 2019 Materials\n\u00b6\n\n\nMonday\n\u00b6\n\n\nMonday Morning: Introduction to HTC and HTCondor\n\u00b6\n\n\n\n\nLecture: Introduction to HTC (\nPDF\n;\nPPT\n)\n\n\nExercise 1.1: Log in to the local submit machine and look around\n\n\nExercise 1.2: Experiment with basic HTCondor commands\n\n\nExercise 1.3: Run jobs!\n\n\nExercise 1.4: Read and interpret log files\n\n\nExercise 1.5: Determining Resource Needs\n\n\nExercise 1.6: Remove jobs from the queue\n\n\nBonus Exercise 1.7: Compile and run some C code\n\n\n\n\nMonday Morning: Running Many HTC Jobs\n\u00b6\n\n\n\n\nLecture: More HTCondor (\nPDF\n;\nPPT\n)\n\n\nExercise 2.1: Work with input and output files\n\n\nExercise 2.2: Use \nqueue N\n, \n$(Cluster)\n, and \n$(Process)\n\n\nExercise 2.3: Use \nqueue matching\n with a custom variable\n\n\nExercise 2.4: Use \nqueue from\n with custom variables\n\n\n\n\nMonday Afternoon: Job Attributes and Handling\n\u00b6\n\n\n\n\nLecture: Intermediate HTCondor: Workflows (\nPDF\n;\nPPT\n)\n\n\nExercise 3.1: Explore \ncondor_q\n\n\nExercise 3.2: Explore \ncondor_status\n\n\nExercise 3.3: A job that needs retries\n\n\n\n\nMonday Afternoon: Introduction to Distributed HTC\n\u00b6\n\n\n\n\nLecture: Introduction to DHTC (\nPDF\n;\n  \nPPT\n)\n\n\nExercise 4.1: Refresher - Submitting multiple jobs\n\n\nExercise 4.2: Log in to the OSG submit machine\n\n\nExercise 4.3: Running jobs in the OSG\n\n\nExercise 4.4: Hardware differences in the OSG\n\n\nExercise 4.5: Software differences in the OSG\n\n\n\n\nTuesday\n\u00b6\n\n\nTuesday Morning: Troubleshooting jobs\n\u00b6\n\n\n\n\nLecture: Troubleshooting jobs (\nPDF\n;\n  \nPPT\n)\n\n\nExercise 1.1: Troubleshooting a DAG\n\n\n\n\nTuesday Afternoon: Software Portability\n\u00b6\n\n\n\n\nLecture: Software Portability for DHTC (\nPDF\n; \nPPT\n)\n\n\nExercise 3.1: Compiling programs for portability\n\n\nExercise 3.2: Using a pre-compiled binary\n\n\nExercise 3.3: Using a wrapper script\n\n\nExercise 3.4: Pre-packaging code\n\n\nBonus Exercise 3.5: Passing Arguments Through the Wrapper Script\n\n\nLecture: Interpreted Languages for DHTC\n\n\nExercise 2.1: Pre-packaging Python\n\n\nExercise 2.2: In-job installation of Python\n\n\n\n\nWednesday\n\u00b6\n\n\nWednesday Morning: Software Modules, Licensing\n\u00b6\n\n\n\n\nLecture: Considerations for licensing and programming packages\n  (\nPDF\n; \nPPT\n)\n\n\nExercise 1.1: Try an OSG Connect software module\n\n\nExercise 1.2: Compile and run Matlab code\n\n\n\n\nWednesday Morning: Containers\n\u00b6\n\n\n\n\nExercise 2.1: Use Singularity from OSG Connect\n\n\nExercise 2.2: Use Singularity to Run Tensorflow (Optional)\n\n\nExercise 2.3: Using Docker\n\n\n\n\nThursday\n\u00b6\n\n\nThursday Morning: Data Handling\n\u00b6\n\n\n\n\nLecture: Overall data considerations (\nPDF\n)\n\n\nExercise 1.1: Understanding your data requirements\n\n\nExercise 1.2: HTCondor file transfer and compression\n\n\nExercise 1.3: Splitting large input data\n\n\n\n\nThursday Morning: Data Handling (continued)\n\u00b6\n\n\n\n\nLecture: Solutions for large input data (\nPDF\n)\n\n\nExercise 2.1: Using a web proxy for large, shared input\n\n\nExercise 2.2: Using StashCache for large, shared input\n\n\nExercise 2.3: Using StashCache for large, unique input\n\n\n\n\nThursday Afternoon: Data Handling (continued)\n\u00b6\n\n\n\n\nLecture: Large output and shared file systems; Data summary\n  (\nPDF\n)\n\n\nExercise 3.1: Using a local shared filesystem for large input files\n\n\nExercise 3.2: Using a local shared filesystem for large output files\n\n\n\n\nThursday Afternoon: Automating Workflows with HTCondor's DAGMan\n\u00b6\n\n\n\n\nLecture: HTCondor: More on Workflows (\nPDF\n;\nPPT\n)\n\n\nExercise 4.1: Coordinating set of jobs: A simple DAG\n\n\nExercise 4.2: A brief detour through the Mandelbrot set\n\n\nExercise 4.3: A more complex DAG\n\n\nExercise 4.4: Handling jobs that fail with DAGMan\n\n\nBonus Exercise 4.5: HTCondor challenges\n (If and only if you have time)\n\n\n\n\n\n\n\nFriday\n\u00b6\n\n\nFriday Morning: From Science to Production Workflows\n\u00b6\n\n\n\n- \nExercise 1.1: Learn about Joe\u2019s Desired Computing Work\n\n- \nExercise 1.2: Plan Overall Workflow\n\n\nFriday Morning: From Science to Production Workflows\n\u00b6\n\n\n\n- \nExercise 1.3: Execute Joe\u2019s Workflow\n\n- \nBonus Exercise 1.4: Further Optimization and Scaling",
            "title": "Overview"
        },
        {
            "location": "/materials/#osg-user-school-2019-materials",
            "text": "",
            "title": "OSG User School 2019 Materials"
        },
        {
            "location": "/materials/#monday",
            "text": "",
            "title": "Monday"
        },
        {
            "location": "/materials/#monday-morning-introduction-to-htc-and-htcondor",
            "text": "Lecture: Introduction to HTC ( PDF ; PPT )  Exercise 1.1: Log in to the local submit machine and look around  Exercise 1.2: Experiment with basic HTCondor commands  Exercise 1.3: Run jobs!  Exercise 1.4: Read and interpret log files  Exercise 1.5: Determining Resource Needs  Exercise 1.6: Remove jobs from the queue  Bonus Exercise 1.7: Compile and run some C code",
            "title": "Monday Morning: Introduction to HTC and HTCondor"
        },
        {
            "location": "/materials/#monday-morning-running-many-htc-jobs",
            "text": "Lecture: More HTCondor ( PDF ; PPT )  Exercise 2.1: Work with input and output files  Exercise 2.2: Use  queue N ,  $(Cluster) , and  $(Process)  Exercise 2.3: Use  queue matching  with a custom variable  Exercise 2.4: Use  queue from  with custom variables",
            "title": "Monday Morning: Running Many HTC Jobs"
        },
        {
            "location": "/materials/#monday-afternoon-job-attributes-and-handling",
            "text": "Lecture: Intermediate HTCondor: Workflows ( PDF ; PPT )  Exercise 3.1: Explore  condor_q  Exercise 3.2: Explore  condor_status  Exercise 3.3: A job that needs retries",
            "title": "Monday Afternoon: Job Attributes and Handling"
        },
        {
            "location": "/materials/#monday-afternoon-introduction-to-distributed-htc",
            "text": "Lecture: Introduction to DHTC ( PDF ;\n   PPT )  Exercise 4.1: Refresher - Submitting multiple jobs  Exercise 4.2: Log in to the OSG submit machine  Exercise 4.3: Running jobs in the OSG  Exercise 4.4: Hardware differences in the OSG  Exercise 4.5: Software differences in the OSG",
            "title": "Monday Afternoon: Introduction to Distributed HTC"
        },
        {
            "location": "/materials/#tuesday",
            "text": "",
            "title": "Tuesday"
        },
        {
            "location": "/materials/#tuesday-morning-troubleshooting-jobs",
            "text": "Lecture: Troubleshooting jobs ( PDF ;\n   PPT )  Exercise 1.1: Troubleshooting a DAG",
            "title": "Tuesday Morning: Troubleshooting jobs"
        },
        {
            "location": "/materials/#tuesday-afternoon-software-portability",
            "text": "Lecture: Software Portability for DHTC ( PDF ;  PPT )  Exercise 3.1: Compiling programs for portability  Exercise 3.2: Using a pre-compiled binary  Exercise 3.3: Using a wrapper script  Exercise 3.4: Pre-packaging code  Bonus Exercise 3.5: Passing Arguments Through the Wrapper Script  Lecture: Interpreted Languages for DHTC  Exercise 2.1: Pre-packaging Python  Exercise 2.2: In-job installation of Python",
            "title": "Tuesday Afternoon: Software Portability"
        },
        {
            "location": "/materials/#wednesday",
            "text": "",
            "title": "Wednesday"
        },
        {
            "location": "/materials/#wednesday-morning-software-modules-licensing",
            "text": "Lecture: Considerations for licensing and programming packages\n  ( PDF ;  PPT )  Exercise 1.1: Try an OSG Connect software module  Exercise 1.2: Compile and run Matlab code",
            "title": "Wednesday Morning: Software Modules, Licensing"
        },
        {
            "location": "/materials/#wednesday-morning-containers",
            "text": "Exercise 2.1: Use Singularity from OSG Connect  Exercise 2.2: Use Singularity to Run Tensorflow (Optional)  Exercise 2.3: Using Docker",
            "title": "Wednesday Morning: Containers"
        },
        {
            "location": "/materials/#thursday",
            "text": "",
            "title": "Thursday"
        },
        {
            "location": "/materials/#thursday-morning-data-handling",
            "text": "Lecture: Overall data considerations ( PDF )  Exercise 1.1: Understanding your data requirements  Exercise 1.2: HTCondor file transfer and compression  Exercise 1.3: Splitting large input data",
            "title": "Thursday Morning: Data Handling"
        },
        {
            "location": "/materials/#thursday-morning-data-handling-continued",
            "text": "Lecture: Solutions for large input data ( PDF )  Exercise 2.1: Using a web proxy for large, shared input  Exercise 2.2: Using StashCache for large, shared input  Exercise 2.3: Using StashCache for large, unique input",
            "title": "Thursday Morning: Data Handling (continued)"
        },
        {
            "location": "/materials/#thursday-afternoon-data-handling-continued",
            "text": "Lecture: Large output and shared file systems; Data summary\n  ( PDF )  Exercise 3.1: Using a local shared filesystem for large input files  Exercise 3.2: Using a local shared filesystem for large output files",
            "title": "Thursday Afternoon: Data Handling (continued)"
        },
        {
            "location": "/materials/#thursday-afternoon-automating-workflows-with-htcondors-dagman",
            "text": "Lecture: HTCondor: More on Workflows ( PDF ; PPT )  Exercise 4.1: Coordinating set of jobs: A simple DAG  Exercise 4.2: A brief detour through the Mandelbrot set  Exercise 4.3: A more complex DAG  Exercise 4.4: Handling jobs that fail with DAGMan  Bonus Exercise 4.5: HTCondor challenges  (If and only if you have time)",
            "title": "Thursday Afternoon: Automating Workflows with HTCondor's DAGMan"
        },
        {
            "location": "/materials/#friday",
            "text": "",
            "title": "Friday"
        },
        {
            "location": "/materials/#friday-morning-from-science-to-production-workflows",
            "text": "-  Exercise 1.1: Learn about Joe\u2019s Desired Computing Work \n-  Exercise 1.2: Plan Overall Workflow",
            "title": "Friday Morning: From Science to Production Workflows"
        },
        {
            "location": "/materials/#friday-morning-from-science-to-production-workflows_1",
            "text": "-  Exercise 1.3: Execute Joe\u2019s Workflow \n-  Bonus Exercise 1.4: Further Optimization and Scaling",
            "title": "Friday Morning: From Science to Production Workflows"
        },
        {
            "location": "/materials/day1/part1-ex1-login/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 1.1: Log In and Look Around\n\u00b6\n\n\nThe goal of this first exercise is simply to log in to the local submit server and look around a little bit, which will take only a few minutes. \n\n\nIf you have trouble getting SSH access to the submit server, ask the instructors right away! Gaining access is critical for all remaining exercises.\n\n\nLogging In\n\u00b6\n\n\nToday, you will use a submit server named \nlearn.chtc.wisc.edu\n, which will allow you to submit jobs to our local HTCondor pool in CHTC.\n\n\nTo log in, use a \nSecure Shell\n (SSH) client.\n\n\n\n\nFrom a Mac or Linux computer, run the Terminal app and use the \nssh\n command, like so:\n\n\n\n\nusername@learn $\n ssh \n<USERNAME>\n@learn.chtc.wisc.edu\n\n\n\n\n\n\n\nOn Windows, we recommend a free client called \nPuTTY\n, but any SSH client should be fine.\n\n\n\n\nIf you need help finding or using an SSH client, ask the instructors for help right away\n!\n\n\nAbout Your Password\n\u00b6\n\n\n\n\nYour username and initial password are located on the Accounts sheet of paper that you received this morning\n\n\nWhile the \npasswd\n command will work (and will change your password temporarily), your initial password will be automatically reset for you on an hourly basis. (So you probably don't want to change your password, in the first place, and definitely want to keep your sheet of paper or memorize the password).\n\n\n\n\nRunning Commands\n\u00b6\n\n\nIn the exercises, we will show commands that you are supposed to type or copy into the command line, like this:\n\n\nusername@learn $\n hostname\n\nlearn.chtc.wisc.edu\n\n\n\n\n\n\n\n\nNote\n\n\nIn the first line of the example above, the \nusername@learn $\n part is meant to show the Linux command-line prompt.\nYou do not type this part! Further, your actual prompt probably is a bit different, and that is expected.\nSo in the example above, the command that you type at your own prompt is just the eight characters \nhostname\n.\nThe second line of the example, without the prompt, shows the output of the command; you do not type this part,\neither.\n\n\n\n\nHere are a few other commands that you can try (the examples below do not show the output from each command):\n\n\nusername@learn $\n whoami\n\nusername@learn $\n date\n\nusername@learn $\n uname -a\n\n\n\n\n\nA suggestion for the day: Try typing into the command line as many of the commands as you can. Copy-and-paste is fine, of course, but \nyou WILL learn more if you take the time to type each command, yourself.\n\n\nOrganizing Your Workspace\n\u00b6\n\n\nYou will be doing many different exercises over the next few days, many of them on this submit server. Each exercise may use many files, once finished. To avoid confusion, it may be useful to create a separate directory for each exercise.\n\n\nFor instance, for the rest of this exercise, you may wish to create and use a directory named \nmonday-1.1-login\n, or something like that.\n\n\nusername@learn $\n mkdir Mon\n\nusername@learn $\n mkdir Mon/1.1\n\nusername@learn $\n \ncd\n Mon/1.1\n\n\n\n\n\nShowing the Version of HTCondor\n\u00b6\n\n\nHTCondor is installed on this server. But what version? You can ask HTCondor itself:\n\n\nusername@learn $\n condor_version\n\n$\nCondorVersion: \n8\n.9.2 May \n13\n \n2019\n BuildID: \n469000\n PackageID: \n8\n.9.2-0.469000 $\n\n$\nCondorPlatform: x86_64_RedHat7 $\n\n\n\n\n\nAs you can see from the output, we are using HTCondor 8.9.2.\n\n\nBackground information about HTCondor version numbers\n\u00b6\n\n\nHTCondor always has two types of releases at one time: stable and development. HTCondor 8.6.x and 8.8.x are considered stable releases, indicated by even-numbered second digits (e.g., 6 or 8 in these cases). Within one stable series, all versions have the same features (for example 8.6.0 and 8.6.8 have the same set of features) and differ only in bug and security fixes.\n\n\nHTCondor 8.9.2 is the latest development release series of HTCondor. You know that these are a development release because the second digit (i.e., 9) is an odd number.\n\n\nReference Materials\n\u00b6\n\n\nHere are a few links to reference materials that might be interesting after the school (or perhaps during).\n\n\n\n\nHTCondor home page\n\n\nHTCondor manuals\n; it is probably best to read the manual corresponding to the version of HTCondor that you use (8.9.2 for today)\n\n\nCenter for High Throughput Computing\n, our campus research computing center, and home to HTCondor and other development of distributed computing tools",
            "title": "Exercise 1.1"
        },
        {
            "location": "/materials/day1/part1-ex1-login/#monday-exercise-11-log-in-and-look-around",
            "text": "The goal of this first exercise is simply to log in to the local submit server and look around a little bit, which will take only a few minutes.   If you have trouble getting SSH access to the submit server, ask the instructors right away! Gaining access is critical for all remaining exercises.",
            "title": "Monday Exercise 1.1: Log In and Look Around"
        },
        {
            "location": "/materials/day1/part1-ex1-login/#logging-in",
            "text": "Today, you will use a submit server named  learn.chtc.wisc.edu , which will allow you to submit jobs to our local HTCondor pool in CHTC.  To log in, use a  Secure Shell  (SSH) client.   From a Mac or Linux computer, run the Terminal app and use the  ssh  command, like so:   username@learn $  ssh  <USERNAME> @learn.chtc.wisc.edu   On Windows, we recommend a free client called  PuTTY , but any SSH client should be fine.   If you need help finding or using an SSH client, ask the instructors for help right away !",
            "title": "Logging In"
        },
        {
            "location": "/materials/day1/part1-ex1-login/#about-your-password",
            "text": "Your username and initial password are located on the Accounts sheet of paper that you received this morning  While the  passwd  command will work (and will change your password temporarily), your initial password will be automatically reset for you on an hourly basis. (So you probably don't want to change your password, in the first place, and definitely want to keep your sheet of paper or memorize the password).",
            "title": "About Your Password"
        },
        {
            "location": "/materials/day1/part1-ex1-login/#running-commands",
            "text": "In the exercises, we will show commands that you are supposed to type or copy into the command line, like this:  username@learn $  hostname learn.chtc.wisc.edu    Note  In the first line of the example above, the  username@learn $  part is meant to show the Linux command-line prompt.\nYou do not type this part! Further, your actual prompt probably is a bit different, and that is expected.\nSo in the example above, the command that you type at your own prompt is just the eight characters  hostname .\nThe second line of the example, without the prompt, shows the output of the command; you do not type this part,\neither.   Here are a few other commands that you can try (the examples below do not show the output from each command):  username@learn $  whoami username@learn $  date username@learn $  uname -a  A suggestion for the day: Try typing into the command line as many of the commands as you can. Copy-and-paste is fine, of course, but  you WILL learn more if you take the time to type each command, yourself.",
            "title": "Running Commands"
        },
        {
            "location": "/materials/day1/part1-ex1-login/#organizing-your-workspace",
            "text": "You will be doing many different exercises over the next few days, many of them on this submit server. Each exercise may use many files, once finished. To avoid confusion, it may be useful to create a separate directory for each exercise.  For instance, for the rest of this exercise, you may wish to create and use a directory named  monday-1.1-login , or something like that.  username@learn $  mkdir Mon username@learn $  mkdir Mon/1.1 username@learn $   cd  Mon/1.1",
            "title": "Organizing Your Workspace"
        },
        {
            "location": "/materials/day1/part1-ex1-login/#showing-the-version-of-htcondor",
            "text": "HTCondor is installed on this server. But what version? You can ask HTCondor itself:  username@learn $  condor_version $ CondorVersion:  8 .9.2 May  13   2019  BuildID:  469000  PackageID:  8 .9.2-0.469000 $ $ CondorPlatform: x86_64_RedHat7 $  As you can see from the output, we are using HTCondor 8.9.2.",
            "title": "Showing the Version of HTCondor"
        },
        {
            "location": "/materials/day1/part1-ex1-login/#background-information-about-htcondor-version-numbers",
            "text": "HTCondor always has two types of releases at one time: stable and development. HTCondor 8.6.x and 8.8.x are considered stable releases, indicated by even-numbered second digits (e.g., 6 or 8 in these cases). Within one stable series, all versions have the same features (for example 8.6.0 and 8.6.8 have the same set of features) and differ only in bug and security fixes.  HTCondor 8.9.2 is the latest development release series of HTCondor. You know that these are a development release because the second digit (i.e., 9) is an odd number.",
            "title": "Background information about HTCondor version numbers"
        },
        {
            "location": "/materials/day1/part1-ex1-login/#reference-materials",
            "text": "Here are a few links to reference materials that might be interesting after the school (or perhaps during).   HTCondor home page  HTCondor manuals ; it is probably best to read the manual corresponding to the version of HTCondor that you use (8.9.2 for today)  Center for High Throughput Computing , our campus research computing center, and home to HTCondor and other development of distributed computing tools",
            "title": "Reference Materials"
        },
        {
            "location": "/materials/day1/part1-ex2-commands/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 1.2: Experiment With Basic HTCondor Commands\n\u00b6\n\n\nThe goal of this exercise is to use the basic informational HTCondor commands, \ncondor_q\n and \ncondor_status\n. They will be useful for monitoring your jobs and available slots (respectively) throughout the week.\n\n\nThis exercise should take only a few minutes.\n\n\nViewing Slots\n\u00b6\n\n\nAs discussed in the lecture, the \ncondor_status\n command is used to view the current state of slots in an HTCondor pool.\n\n\nAt its most basic, the command is very simple:\n\n\nusername@learn $\n condor_status\n\n\n\n\n\nThis command, running on our (CHTC) pool, will produce a lot of output; there is one line per slot, and we typically have over 10,000 slots. \nTIP: You can widen your terminal window, which may help you to see all details of the output better.\n\n\nHere is some example output (what you see will be longer):\n\n\nslot1_31@e437.chtc.wisc.edu        LINUX      X86_64 Unclaimed Idle      0.000 8053  0+01:14:34\n\n\nslot1_32@e437.chtc.wisc.edu        LINUX      X86_64 Unclaimed Idle      0.000 8053  0+03:57:00\n\n\nslot1_33@e437.chtc.wisc.edu        LINUX      X86_64 Unclaimed Idle      0.000 8053  1+00:05:17\n\n\nslot1@e438.chtc.wisc.edu           LINUX      X86_64 Owner     Idle      0.300  250  7+03:22:21\n\n\nslot1_1@e438.chtc.wisc.edu         LINUX      X86_64 Claimed   Busy      0.930 1024  0+02:42:08\n\n\nslot1_2@e438.chtc.wisc.edu         LINUX      X86_64 Claimed   Busy      3.530 1024  0+02:40:24\n\n\n\n\n\n\nThis output consists of 8 columns:\n\n\n\n\n\n\n\n\nCol\n\n\nExample\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nName\n\n\nslot1_1@e438.chtc.wisc.edu\n\n\nFull slot name (including the hostname)\n\n\n\n\n\n\nOpSys\n\n\nLINUX\n\n\nOperating system\n\n\n\n\n\n\nArch\n\n\nX86_64\n\n\nSlot architecture (e.g., Intel 64 bit)\n\n\n\n\n\n\nState\n\n\nClaimed\n\n\nState of the slot (\nUnclaimed\n is available, \nOwner\n is being used by the machine owner, \nClaimed\n is matched to a job)\n\n\n\n\n\n\nActivity\n\n\nBusy\n\n\nIs there activity on the slot?\n\n\n\n\n\n\nLoadAv\n\n\n0.930\n\n\nLoad average, a measure of CPU activity on the slot\n\n\n\n\n\n\nMem\n\n\n1024\n\n\nMemory available to the slot, in MB\n\n\n\n\n\n\nActvtyTime\n\n\n0+02:42:08\n\n\nAmount of time spent in current activity (days + hours:minutes:seconds)\n\n\n\n\n\n\n\n\nAt the end of the slot listing, there is a summary. Here is an example:\n\n\n                     Machines Owner Claimed Unclaimed Matched Preempting  Drain\n\n\n\n        X86_64/LINUX    10831     0   10194       631       0          0      6\n\n\n      X86_64/WINDOWS        2     2       0         0       0          0      0\n\n\n\n               Total    10833     2   10194       631       0          0      6\n\n\n\n\n\n\nThere is one row of summary for each machine (i.e. \"slot\") architecture/operating system combination with columns for the number of slots in each state. The final row gives a summary of slot states for the whole pool.\n\n\nQuestions:\n\u00b6\n\n\n\n\nWhen you run \ncondor_status\n, how many 64-bit Linux slots are available? (Hint: Unclaimed = available.)\n\n\nWhat percent of the total slots are currently claimed by a job? (Note: there is a rapid turnover of slots, which is what allows users with new submission to have jobs start quickly.)\n\n\nHow have these numbers changed (if at all) when you run the command again?\n\n\n\n\nViewing Whole Machines, Only\n\u00b6\n\n\nAlso try out the \n-compact\n for a slightly different view of whole machines (i.e. server hostnames), without the individual slots shown.\n\n\nusername@learn $\n condor_status -compact\n\n\n\n\n\nHow has the column information changed?\n\n\nViewing Jobs\n\u00b6\n\n\nThe \ncondor_q\n command lists jobs that are on this submit machine and that are running or waiting to run. The \n_q\n part of the name is meant to suggest the word \u201cqueue\u201d, or list of job sets \nwaiting\n to finish.\n\n\nViewing Your Own Jobs\n\u00b6\n\n\nThe simplest form of the command lists only your jobs:\n\n\nusername@learn $\n condor_q\n\n\n\n\n\nThe main part of the output (which will be empty, because you haven't submitted jobs yet) shows one set (\"batch\") of submitted jobs per line. If you had a single job in the queue, it would look something like the below:\n\n\n-- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/12/19 09:59:31\n\n\nOWNER  BATCH_NAME            SUBMITTED   DONE   RUN    IDLE  TOTAL JOB_IDS\n\n\naapohl CMD: run_ffmpeg.sh   7/12 09:58      _      _      1      1 18801.0               \n\n\n\n\n\n\nThis output consists of 8 (or 9) columns:\n\n\n\n\n\n\n\n\nCol\n\n\nExample\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nOWNER\n\n\naapohl\n\n\nThe user ID of the user who submitted the job\n\n\n\n\n\n\nBATCH_NAME\n\n\nrun_ffmpeg.sh\n\n\nThe executable or \"jobbatchname\" specified within the submit file(s)\n\n\n\n\n\n\nSUBMITTED\n\n\n7/12 09:58\n\n\nThe date and time when the job was submitted\n\n\n\n\n\n\nDONE\n\n\n_\n\n\nNumber of jobs in this batch that have completed\n\n\n\n\n\n\nRUN\n\n\n_\n\n\nNumber of jobs in this batch that are currently running\n\n\n\n\n\n\nIDLE\n\n\n1\n\n\nNumber of jobs in this batch that are idle, waiting for a match\n\n\n\n\n\n\nHOLD\n\n\n_\n\n\nColumn will show up if there are jobs on \"hold\" because something about the submission/setup needs to be corrected by the user\n\n\n\n\n\n\nTOTAL\n\n\n1\n\n\nTotal number of jobs in this batch\n\n\n\n\n\n\nJOB_IDS\n\n\n18801.0\n\n\nJob ID or range of Job IDs in this batch\n\n\n\n\n\n\n\n\nAt the end of the job listing, there is a summary. Here is a sample:\n\n\n1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended\n\n\n\n\n\n\nIt shows total counts of jobs in the different possible states.\n\n\nQuestions:\n\n\n\n\nFor the sample above, when was the job submitted?\n\n\nFor the sample above, was the job running or not yet? How can you tell?\n\n\n\n\nViewing Everyone\u2019s Jobs\n\u00b6\n\n\nBy default, the \ncondor_q\n command shows \nyour\n jobs only. To see everyone\u2019s jobs that are queued on the machine, add the \n-all\n option:\n\n\nusername@learn $\n condor_q -all\n\n\n\n\n\n\n\nHow many jobs are queued in total (i.e., running or waiting to run)?\n\n\nHow many jobs from this submit machine are running right now?\n\n\n\n\nViewing Jobs without the Default \"batch\" Mode\n\u00b6\n\n\nThe \ncondor_q\n output, by default, groups \"batches\" of jobs together (if they were submitted with the same submit file or \"jobbatchname\"). To see more information for EVERY job on a separate line of output, use the \n-nobatch\n option to \ncondor_q\n:\n\n\nusername@learn $\n condor_q -all -nobatch\n\n\n\n\n\nHow has the column information changed?\n (Below is an example of the top of the output.)\n\n\n-- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/12/19 11:58:44\n\n\n ID       OWNER            SUBMITTED     RUN_TIME ST PRI SIZE   CMD\n\n\n18203.0   s16_alirezakho  7/11 09:51   0+00:00:00 I  0      0.7 pascal\n\n\n18204.0   s16_alirezakho  7/11 09:51   0+00:00:00 I  0      0.7 pascal\n\n\n18801.0   aapohl          7/12 09:58   0+00:00:00 I  0      0.0 run_ffmpeg.sh\n\n\n18997.0   s16_martincum   7/12 10:59   0+00:00:32 I  0    733.0 runR.pl 1_0 run_perm.R 1 0 10\n\n\n19027.5   s16_martincum   7/12 11:06   0+00:09:20 I  0   2198.0 runR.pl 1_5 run_perm.R 1 5 1000\n\n\n\n\n\n\nThe \n-nobatch\n output shows a line for every job and consists of 8 columns:\n\n\n\n\n\n\n\n\nCol\n\n\nExample\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nID\n\n\n18801.0\n\n\nJob ID, which is the \ncluster\n, a dot character (\n.\n), and the \nprocess\n\n\n\n\n\n\nOWNER\n\n\naapohl\n\n\nThe user ID of the user who submitted the job\n\n\n\n\n\n\nSUBMITTED\n\n\n7/12 09:58\n\n\nThe date and time when the job was submitted\n\n\n\n\n\n\nRUN_TIME\n\n\n0+00:00:00\n\n\nTotal time spent running so far (days + hours:minutes:seconds)\n\n\n\n\n\n\nST\n\n\nI\n\n\nStatus of job: \nI\n is Idle (waiting to run), \nR\n is Running, \nH\n is Held, etc.\n\n\n\n\n\n\nPRI\n\n\n0\n\n\nJob priority (see next lecture)\n\n\n\n\n\n\nSIZE\n\n\n0.0\n\n\nCurrent run-time memory usage, in MB\n\n\n\n\n\n\nCMD\n\n\nrun_ffmpeg.sh\n\n\nThe executable command (with arguments) to be run\n\n\n\n\n\n\n\n\nIn future exercises, you'll want to switch between \ncondor_q\n and \ncondor_q -nobatch\n to see different types of information about YOUR jobs.\n\n\nExtra Information\n\u00b6\n\n\nBoth \ncondor_status\n and \ncondor_q\n have many command-line options, some of which significantly change their output. You will explore a few of the most useful options today and tomorrow, but if you want to experiment now, go ahead! There are a few ways to learn more about the commands:\n\n\n\n\nUse the (brief) built-in help for the commands, e.g.: \ncondor_q -h\n\n\nRead the installed man(ual) pages for the commands, e.g.: \nman condor_q\n\n\nFind the command in \nthe online manual\n; \nnote:\n the text online is the same as the \nman\n text, only formatted for the web",
            "title": "Exercise 1.2"
        },
        {
            "location": "/materials/day1/part1-ex2-commands/#monday-exercise-12-experiment-with-basic-htcondor-commands",
            "text": "The goal of this exercise is to use the basic informational HTCondor commands,  condor_q  and  condor_status . They will be useful for monitoring your jobs and available slots (respectively) throughout the week.  This exercise should take only a few minutes.",
            "title": "Monday Exercise 1.2: Experiment With Basic HTCondor Commands"
        },
        {
            "location": "/materials/day1/part1-ex2-commands/#viewing-slots",
            "text": "As discussed in the lecture, the  condor_status  command is used to view the current state of slots in an HTCondor pool.  At its most basic, the command is very simple:  username@learn $  condor_status  This command, running on our (CHTC) pool, will produce a lot of output; there is one line per slot, and we typically have over 10,000 slots.  TIP: You can widen your terminal window, which may help you to see all details of the output better.  Here is some example output (what you see will be longer):  slot1_31@e437.chtc.wisc.edu        LINUX      X86_64 Unclaimed Idle      0.000 8053  0+01:14:34  slot1_32@e437.chtc.wisc.edu        LINUX      X86_64 Unclaimed Idle      0.000 8053  0+03:57:00  slot1_33@e437.chtc.wisc.edu        LINUX      X86_64 Unclaimed Idle      0.000 8053  1+00:05:17  slot1@e438.chtc.wisc.edu           LINUX      X86_64 Owner     Idle      0.300  250  7+03:22:21  slot1_1@e438.chtc.wisc.edu         LINUX      X86_64 Claimed   Busy      0.930 1024  0+02:42:08  slot1_2@e438.chtc.wisc.edu         LINUX      X86_64 Claimed   Busy      3.530 1024  0+02:40:24   This output consists of 8 columns:     Col  Example  Meaning      Name  slot1_1@e438.chtc.wisc.edu  Full slot name (including the hostname)    OpSys  LINUX  Operating system    Arch  X86_64  Slot architecture (e.g., Intel 64 bit)    State  Claimed  State of the slot ( Unclaimed  is available,  Owner  is being used by the machine owner,  Claimed  is matched to a job)    Activity  Busy  Is there activity on the slot?    LoadAv  0.930  Load average, a measure of CPU activity on the slot    Mem  1024  Memory available to the slot, in MB    ActvtyTime  0+02:42:08  Amount of time spent in current activity (days + hours:minutes:seconds)     At the end of the slot listing, there is a summary. Here is an example:                       Machines Owner Claimed Unclaimed Matched Preempting  Drain          X86_64/LINUX    10831     0   10194       631       0          0      6        X86_64/WINDOWS        2     2       0         0       0          0      0                 Total    10833     2   10194       631       0          0      6   There is one row of summary for each machine (i.e. \"slot\") architecture/operating system combination with columns for the number of slots in each state. The final row gives a summary of slot states for the whole pool.",
            "title": "Viewing Slots"
        },
        {
            "location": "/materials/day1/part1-ex2-commands/#questions",
            "text": "When you run  condor_status , how many 64-bit Linux slots are available? (Hint: Unclaimed = available.)  What percent of the total slots are currently claimed by a job? (Note: there is a rapid turnover of slots, which is what allows users with new submission to have jobs start quickly.)  How have these numbers changed (if at all) when you run the command again?",
            "title": "Questions:"
        },
        {
            "location": "/materials/day1/part1-ex2-commands/#viewing-whole-machines-only",
            "text": "Also try out the  -compact  for a slightly different view of whole machines (i.e. server hostnames), without the individual slots shown.  username@learn $  condor_status -compact  How has the column information changed?",
            "title": "Viewing Whole Machines, Only"
        },
        {
            "location": "/materials/day1/part1-ex2-commands/#viewing-jobs",
            "text": "The  condor_q  command lists jobs that are on this submit machine and that are running or waiting to run. The  _q  part of the name is meant to suggest the word \u201cqueue\u201d, or list of job sets  waiting  to finish.",
            "title": "Viewing Jobs"
        },
        {
            "location": "/materials/day1/part1-ex2-commands/#viewing-your-own-jobs",
            "text": "The simplest form of the command lists only your jobs:  username@learn $  condor_q  The main part of the output (which will be empty, because you haven't submitted jobs yet) shows one set (\"batch\") of submitted jobs per line. If you had a single job in the queue, it would look something like the below:  -- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/12/19 09:59:31  OWNER  BATCH_NAME            SUBMITTED   DONE   RUN    IDLE  TOTAL JOB_IDS  aapohl CMD: run_ffmpeg.sh   7/12 09:58      _      _      1      1 18801.0                  This output consists of 8 (or 9) columns:     Col  Example  Meaning      OWNER  aapohl  The user ID of the user who submitted the job    BATCH_NAME  run_ffmpeg.sh  The executable or \"jobbatchname\" specified within the submit file(s)    SUBMITTED  7/12 09:58  The date and time when the job was submitted    DONE  _  Number of jobs in this batch that have completed    RUN  _  Number of jobs in this batch that are currently running    IDLE  1  Number of jobs in this batch that are idle, waiting for a match    HOLD  _  Column will show up if there are jobs on \"hold\" because something about the submission/setup needs to be corrected by the user    TOTAL  1  Total number of jobs in this batch    JOB_IDS  18801.0  Job ID or range of Job IDs in this batch     At the end of the job listing, there is a summary. Here is a sample:  1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended   It shows total counts of jobs in the different possible states.  Questions:   For the sample above, when was the job submitted?  For the sample above, was the job running or not yet? How can you tell?",
            "title": "Viewing Your Own Jobs"
        },
        {
            "location": "/materials/day1/part1-ex2-commands/#viewing-everyones-jobs",
            "text": "By default, the  condor_q  command shows  your  jobs only. To see everyone\u2019s jobs that are queued on the machine, add the  -all  option:  username@learn $  condor_q -all   How many jobs are queued in total (i.e., running or waiting to run)?  How many jobs from this submit machine are running right now?",
            "title": "Viewing Everyone\u2019s Jobs"
        },
        {
            "location": "/materials/day1/part1-ex2-commands/#viewing-jobs-without-the-default-batch-mode",
            "text": "The  condor_q  output, by default, groups \"batches\" of jobs together (if they were submitted with the same submit file or \"jobbatchname\"). To see more information for EVERY job on a separate line of output, use the  -nobatch  option to  condor_q :  username@learn $  condor_q -all -nobatch  How has the column information changed?  (Below is an example of the top of the output.)  -- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/12/19 11:58:44   ID       OWNER            SUBMITTED     RUN_TIME ST PRI SIZE   CMD  18203.0   s16_alirezakho  7/11 09:51   0+00:00:00 I  0      0.7 pascal  18204.0   s16_alirezakho  7/11 09:51   0+00:00:00 I  0      0.7 pascal  18801.0   aapohl          7/12 09:58   0+00:00:00 I  0      0.0 run_ffmpeg.sh  18997.0   s16_martincum   7/12 10:59   0+00:00:32 I  0    733.0 runR.pl 1_0 run_perm.R 1 0 10  19027.5   s16_martincum   7/12 11:06   0+00:09:20 I  0   2198.0 runR.pl 1_5 run_perm.R 1 5 1000   The  -nobatch  output shows a line for every job and consists of 8 columns:     Col  Example  Meaning      ID  18801.0  Job ID, which is the  cluster , a dot character ( . ), and the  process    OWNER  aapohl  The user ID of the user who submitted the job    SUBMITTED  7/12 09:58  The date and time when the job was submitted    RUN_TIME  0+00:00:00  Total time spent running so far (days + hours:minutes:seconds)    ST  I  Status of job:  I  is Idle (waiting to run),  R  is Running,  H  is Held, etc.    PRI  0  Job priority (see next lecture)    SIZE  0.0  Current run-time memory usage, in MB    CMD  run_ffmpeg.sh  The executable command (with arguments) to be run     In future exercises, you'll want to switch between  condor_q  and  condor_q -nobatch  to see different types of information about YOUR jobs.",
            "title": "Viewing Jobs without the Default \"batch\" Mode"
        },
        {
            "location": "/materials/day1/part1-ex2-commands/#extra-information",
            "text": "Both  condor_status  and  condor_q  have many command-line options, some of which significantly change their output. You will explore a few of the most useful options today and tomorrow, but if you want to experiment now, go ahead! There are a few ways to learn more about the commands:   Use the (brief) built-in help for the commands, e.g.:  condor_q -h  Read the installed man(ual) pages for the commands, e.g.:  man condor_q  Find the command in  the online manual ;  note:  the text online is the same as the  man  text, only formatted for the web",
            "title": "Extra Information"
        },
        {
            "location": "/materials/day1/part1-ex3-jobs/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 1.3: Run Jobs!\n\u00b6\n\n\nThe goal of this exercise is to submit jobs to HTCondor and have them run on the local pool (CHTC). This is a huge step in learning to use an HTC system!\n\n\nThis exercise will take longer than the first two, short ones. It is the essential part of this exercise time. If you are having any problems getting the jobs to run, please ask the instructors! It is very important that you know how to run simple jobs.\n\n\nRunning a Simple Job\n\u00b6\n\n\nNearly all of the time, when you want to run an HTCondor job, you first write an HTCondor submit file for it. In this section, you will run the same \nhostname\n command as in Exercise 1.1, but where this command will run within a job on one of the 'execute' servers in CHTC's local HTCondor pool.\n\n\nHere is a simple submit file for the \nhostname\n command:\n\n\nexecutable = /bin/hostname\n\noutput = simple.out\nerror = simple.err\nlog = simple.log\n\nrequest_cpus = 1\nrequest_memory = 1GB\nrequest_disk = 1MB\n\nqueue\n\n\n\n\n\nWrite those lines of text in a file named \nsimple.sub\n.\n\n\n\n\nNote\n\n\nThere is nothing magic about the name of an HTCondor submit file.\nIt can be any filename you want.\nIt's a good practice to always include the \n.sub\n extension, but it is not required.\nUltimately, a submit file is a text file\n\n\n\n\nThe lines of the submit file have the following meanings:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexecutable\n\n\nThe name of the program to run (relative to the directory from which you submit).\n\n\n\n\n\n\noutput\n\n\nThe filename where HTCondor will write the standard output from your job.\n\n\n\n\n\n\nerror\n\n\nThe filename where HTCondor will write the standard error from your job. This particular job is not likely to have any, but it is best to include this line for every job.\n\n\n\n\n\n\nlog\n\n\nThe filename where HTCondor will write information about your job run. Technically not required, it is a \nreally\n good idea to have a log file for every job.\n\n\n\n\n\n\nrequest_*\n\n\nTells HTCondor how many \ncpus\n and how much \nmemory\n and \ndisk\n we want, which is not much, because the 'hostname' executable is pretty simple\n\n\n\n\n\n\nqueue\n\n\nTells HTCondor to run your job with the settings above.\n\n\n\n\n\n\n\n\nNote that we are not using the \narguments\n or \ntransfer_input_files\n lines that were mentioned during lecture because the \nhostname\n program is all that needs to be transferred from the submit server, and we want to run it without any additional options.\n\n\nDouble-check your submit file, so that it matches the text above. Then, tell HTCondor to run your job:\n\n\nusername@learn $\n condor_submit simple.sub\n\nSubmitting job(s).\n\n\n1 job(s) submitted to cluster NNNN.\n\n\n\n\n\n\nThe actual cluster number will be shown instead of \nNNNN\n. \nIf, instead of the text above, there are error messages, read them carefully and then try to correct your submit file or ask for help.\n\n\nNotice that \ncondor_submit\n returns back to the shell prompt right away. It does \nnot\n wait for your job to run. Instead, as soon as it has finished submitting your job into the queue, the submit command finishes.\n\n\nView your job in the queue\n\u00b6\n\n\nNow, use \ncondor_q\n and \ncondor_q -nobatch\n to watch for your job in the queue! \n\n\nYou may not even catch the job in the \nR\n running state, because the \nhostname\n command runs very quickly. When the job itself is finished, it will 'leave' the queue and no longer be listed in the \ncondor_q\n output.\n\n\nAfter the job finishes, check for the \nhostname\n output in \nsimple.out\n, which is where job information printed to the terminal screen will be printed for the job.\n\n\nusername@learn $\n cat simple.out\n\ne171.chtc.wisc.edu\n\n\n\n\n\n\nThe \nsimple.err\n file should be empty, unless there were issues running the \nhostname\n executable after it was transferred to the slot. The \nsimple.log\n is more complex and will be the focus of a later exercise.\n\n\nRunning a Job With Arguments\n\u00b6\n\n\nVery often, when you run a command on the command line, it includes arguments (i.e. options) after the program name, as in the below examples:\n\n\nusername@learn $\n cat simple.out\n\nusername@learn $\n sleep \n60\n\n\nusername@learn $\n dc -e \n'6 7 * p'\n\n\n\n\n\n\nIn an HTCondor submit file, the program (or 'executable') name goes in the \nexecutable\n statement and \nall remaining arguments\n go into an \narguments\n statement. For example, if the full command is:\n\n\nusername@learn $\n sleep \n60\n\n\n\n\n\n\nThen in the submit file, we would put the location of the \"sleep\" program (you can find it with \nwhich sleep\n) as the job \nexecutable\n, and \n60\n as the job \narguments\n:\n\n\nexecutable = /bin/sleep\narguments = 60\n\n\n\n\n\nFor the command-line command:\n\n\nusername@learn $\n dc -e \n'6 7 * p'\n\n\n\n\n\n\nWe would put the following into the submit file, putting the \narguments\n statement in quotes, since it contains single quotes:\n\n\nexecutable = /usr/bin/dc\narguments = \"-e '6 7 * p'\"\n\n\n\n\n\nLet\u2019s try a job submission with arguments. We will use the \nsleep\n command shown above, which simply does nothing for the specified number of seconds, then exits normally. It is convenient for simulating a job that takes a while to run.\n\n\nCreate a new submit file (you name it this time) and save the following text in it.\n\n\nexecutable = /bin/sleep\narguments = 60\n\noutput = sleep.out\nerror = sleep.err\nlog = sleep.log\n\nrequest_cpus = 1\nrequest_memory = 1GB\nrequest_disk = 1MB\n\nqueue\n\n\n\n\n\nExcept for changing a few filenames, this submit file is nearly identical to the last one. But, see the extra \narguments\n line?\n\n\nSubmit this new job. Again, watch for it to run using \ncondor_q\n and \ncondor_q -nobatch\n; \ncheck once every 15 seconds or so. \nOnce the job starts running, it will take about 1 minute to run (because of the \nsleep\n command, right?), \nso you should be able to see it running for a bit. \nWhen the job finishes, it will disappear from the queue, but there will be no output in the output or error files, because \nsleep\n does not produce any output.\n\n\nRunning a Script Job From the Submit Directory\n\u00b6\n\n\nSo far, we have been running programs (executables) that come with the standard Linux system. \nMore frequently, you will want to run a program that exists within your directory \nor perhaps a simple shell script of commands that you'd like to run within a job. In this example, you will write a shell script and a submit file that runs the shell script within a job:\n\n\n\n\n\n\nPut the following contents into a file named \ntest-script.sh\n:\n\n\n#!/bin/sh\necho 'Date: ' `date` \necho 'Host: ' `hostname` \necho 'System: ' `uname -spo` \necho \"Program: $0\" \necho \"Args: $*\"\necho 'ls: ' `ls`\n# END\n\n\n\n\n\n\n\n\n\nAdd executable permissions to the file (so that it can be run as a program):\n\n\nusername@learn $\n chmod +x test-script.sh\n\n\n\n\n\n\n\n\n\nTest your script from the command line:\n\n\nusername@learn $\n ./test-script.sh hello \n42\n \n\nDate: Mon Jul 17 10:02:20 CDT 2017 \n\n\nHost: learn.chtc.wisc.edu \n\n\nSystem: Linux x86_64 GNU/Linux \n\n\nProgram: ./test-script.sh\n\n\nArgs: hello 42\n\n\nls: hostname.sub montage simple.err simple.log simple.out test-script.sh\n\n\n\n\n\n\nThis step is \nreally\n important! If you cannot run your executable from the command-line, HTCondor probably cannot run it on another machine, either. And debugging simple problems like this one is surprisingly difficult. So, if possible, test your \nexecutable\n and \narguments\n as a command at the command-line first.\n\n\n\n\n\n\nWrite the submit file (this should be getting easier by now):\n\n\nexecutable = \ntest-script.sh\n\narguments = foo bar baz\n\noutput = script.out\nerror = script.err\nlog = script.log\n\nrequest_cpus = 1\nrequest_memory = 1GB\nrequest_disk = 1MB\n\nqueue\n\n\n\n\n\nIn this example, the \nexecutable\n that was named in the submit file did \nnot\n start with a \n/\n, \n    so the location of the file is relative to the submit directory itself. \n    In other words, in this format the executable must be in the same directory as the submit file.\n\n\n\n\nNote\n\n\nAs this example shows, blank lines and spaces around the = sign do not matter to HTCondor.\nUse whitespace to make things clear to \nyou\n. What format do you prefer to read?\n\n\n\n\n\n\n\n\nSubmit the job, wait for it to finish, and check the output (and error, which should be empty).\n\n\nWhat do you notice about the lines returned for \"Program\" and \"ls\"? Remember that only files pertaining\nto \nthis\n job will be in the job working directory on the execute server. You're also seeing the effects\nof HTCondor's need to standardize some filenames when running your job, though they are named as you expect \nin the submission directory (per the submit file contents).\n\n\n\n\n\n\nExtra Challenge\n\u00b6\n\n\n\n\nNote\n\n\nThere are Extra Challenges throughout the school curriculum. You may be better off coming back to these after you've completed all other exercises for your current working session.\n\n\n\n\nBelow is a simple Python script that does something similar to the shell script above. Run this Python script using HTCondor.\n\n\n#!/usr/bin/env python\n\n\"\"\"Extra Challenge for OSG User School\nWritten by Tim Cartwright\nSubmitted to CHTC by #YOUR_NAME#\n\"\"\"\n\nimport getpass\nimport os\nimport platform\nimport socket\nimport sys\nimport time\n\narguments = None\nif len(sys.argv) > 1:\n    arguments = '\"' + ' '.join(sys.argv[1:]) + '\"'\n\nprint >> sys.stderr, __doc__\nprint 'Time    :', time.strftime('%Y-%m-%d (%a) %H:%M:%S %Z')\nprint 'Host    :', getpass.getuser(), '@', socket.gethostname()\nuname = platform.uname()\nprint \"System  :\", uname[0], uname[2], uname[4]\nprint \"Version :\", platform.python_version()\nprint \"Program :\", sys.executable\nprint 'Script  :', os.path.abspath(__file__)\nprint 'Args    :', arguments\n\n\n\n\n\n\n\nNote\n\n\nFor the Python script, above, you'll want to increase the memory request to at least 64MB.\nWe will talk about tuning resource requests, later.",
            "title": "Exercise 1.3"
        },
        {
            "location": "/materials/day1/part1-ex3-jobs/#monday-exercise-13-run-jobs",
            "text": "The goal of this exercise is to submit jobs to HTCondor and have them run on the local pool (CHTC). This is a huge step in learning to use an HTC system!  This exercise will take longer than the first two, short ones. It is the essential part of this exercise time. If you are having any problems getting the jobs to run, please ask the instructors! It is very important that you know how to run simple jobs.",
            "title": "Monday Exercise 1.3: Run Jobs!"
        },
        {
            "location": "/materials/day1/part1-ex3-jobs/#running-a-simple-job",
            "text": "Nearly all of the time, when you want to run an HTCondor job, you first write an HTCondor submit file for it. In this section, you will run the same  hostname  command as in Exercise 1.1, but where this command will run within a job on one of the 'execute' servers in CHTC's local HTCondor pool.  Here is a simple submit file for the  hostname  command:  executable = /bin/hostname\n\noutput = simple.out\nerror = simple.err\nlog = simple.log\n\nrequest_cpus = 1\nrequest_memory = 1GB\nrequest_disk = 1MB\n\nqueue  Write those lines of text in a file named  simple.sub .   Note  There is nothing magic about the name of an HTCondor submit file.\nIt can be any filename you want.\nIt's a good practice to always include the  .sub  extension, but it is not required.\nUltimately, a submit file is a text file   The lines of the submit file have the following meanings:           executable  The name of the program to run (relative to the directory from which you submit).    output  The filename where HTCondor will write the standard output from your job.    error  The filename where HTCondor will write the standard error from your job. This particular job is not likely to have any, but it is best to include this line for every job.    log  The filename where HTCondor will write information about your job run. Technically not required, it is a  really  good idea to have a log file for every job.    request_*  Tells HTCondor how many  cpus  and how much  memory  and  disk  we want, which is not much, because the 'hostname' executable is pretty simple    queue  Tells HTCondor to run your job with the settings above.     Note that we are not using the  arguments  or  transfer_input_files  lines that were mentioned during lecture because the  hostname  program is all that needs to be transferred from the submit server, and we want to run it without any additional options.  Double-check your submit file, so that it matches the text above. Then, tell HTCondor to run your job:  username@learn $  condor_submit simple.sub Submitting job(s).  1 job(s) submitted to cluster NNNN.   The actual cluster number will be shown instead of  NNNN .  If, instead of the text above, there are error messages, read them carefully and then try to correct your submit file or ask for help.  Notice that  condor_submit  returns back to the shell prompt right away. It does  not  wait for your job to run. Instead, as soon as it has finished submitting your job into the queue, the submit command finishes.",
            "title": "Running a Simple Job"
        },
        {
            "location": "/materials/day1/part1-ex3-jobs/#view-your-job-in-the-queue",
            "text": "Now, use  condor_q  and  condor_q -nobatch  to watch for your job in the queue!   You may not even catch the job in the  R  running state, because the  hostname  command runs very quickly. When the job itself is finished, it will 'leave' the queue and no longer be listed in the  condor_q  output.  After the job finishes, check for the  hostname  output in  simple.out , which is where job information printed to the terminal screen will be printed for the job.  username@learn $  cat simple.out e171.chtc.wisc.edu   The  simple.err  file should be empty, unless there were issues running the  hostname  executable after it was transferred to the slot. The  simple.log  is more complex and will be the focus of a later exercise.",
            "title": "View your job in the queue"
        },
        {
            "location": "/materials/day1/part1-ex3-jobs/#running-a-job-with-arguments",
            "text": "Very often, when you run a command on the command line, it includes arguments (i.e. options) after the program name, as in the below examples:  username@learn $  cat simple.out username@learn $  sleep  60  username@learn $  dc -e  '6 7 * p'   In an HTCondor submit file, the program (or 'executable') name goes in the  executable  statement and  all remaining arguments  go into an  arguments  statement. For example, if the full command is:  username@learn $  sleep  60   Then in the submit file, we would put the location of the \"sleep\" program (you can find it with  which sleep ) as the job  executable , and  60  as the job  arguments :  executable = /bin/sleep\narguments = 60  For the command-line command:  username@learn $  dc -e  '6 7 * p'   We would put the following into the submit file, putting the  arguments  statement in quotes, since it contains single quotes:  executable = /usr/bin/dc\narguments = \"-e '6 7 * p'\"  Let\u2019s try a job submission with arguments. We will use the  sleep  command shown above, which simply does nothing for the specified number of seconds, then exits normally. It is convenient for simulating a job that takes a while to run.  Create a new submit file (you name it this time) and save the following text in it.  executable = /bin/sleep\narguments = 60\n\noutput = sleep.out\nerror = sleep.err\nlog = sleep.log\n\nrequest_cpus = 1\nrequest_memory = 1GB\nrequest_disk = 1MB\n\nqueue  Except for changing a few filenames, this submit file is nearly identical to the last one. But, see the extra  arguments  line?  Submit this new job. Again, watch for it to run using  condor_q  and  condor_q -nobatch ; \ncheck once every 15 seconds or so. \nOnce the job starts running, it will take about 1 minute to run (because of the  sleep  command, right?), \nso you should be able to see it running for a bit. \nWhen the job finishes, it will disappear from the queue, but there will be no output in the output or error files, because  sleep  does not produce any output.",
            "title": "Running a Job With Arguments"
        },
        {
            "location": "/materials/day1/part1-ex3-jobs/#running-a-script-job-from-the-submit-directory",
            "text": "So far, we have been running programs (executables) that come with the standard Linux system. \nMore frequently, you will want to run a program that exists within your directory \nor perhaps a simple shell script of commands that you'd like to run within a job. In this example, you will write a shell script and a submit file that runs the shell script within a job:    Put the following contents into a file named  test-script.sh :  #!/bin/sh\necho 'Date: ' `date` \necho 'Host: ' `hostname` \necho 'System: ' `uname -spo` \necho \"Program: $0\" \necho \"Args: $*\"\necho 'ls: ' `ls`\n# END    Add executable permissions to the file (so that it can be run as a program):  username@learn $  chmod +x test-script.sh    Test your script from the command line:  username@learn $  ./test-script.sh hello  42   Date: Mon Jul 17 10:02:20 CDT 2017   Host: learn.chtc.wisc.edu   System: Linux x86_64 GNU/Linux   Program: ./test-script.sh  Args: hello 42  ls: hostname.sub montage simple.err simple.log simple.out test-script.sh   This step is  really  important! If you cannot run your executable from the command-line, HTCondor probably cannot run it on another machine, either. And debugging simple problems like this one is surprisingly difficult. So, if possible, test your  executable  and  arguments  as a command at the command-line first.    Write the submit file (this should be getting easier by now):  executable =  test-script.sh \narguments = foo bar baz\n\noutput = script.out\nerror = script.err\nlog = script.log\n\nrequest_cpus = 1\nrequest_memory = 1GB\nrequest_disk = 1MB\n\nqueue  In this example, the  executable  that was named in the submit file did  not  start with a  / , \n    so the location of the file is relative to the submit directory itself. \n    In other words, in this format the executable must be in the same directory as the submit file.   Note  As this example shows, blank lines and spaces around the = sign do not matter to HTCondor.\nUse whitespace to make things clear to  you . What format do you prefer to read?     Submit the job, wait for it to finish, and check the output (and error, which should be empty).  What do you notice about the lines returned for \"Program\" and \"ls\"? Remember that only files pertaining\nto  this  job will be in the job working directory on the execute server. You're also seeing the effects\nof HTCondor's need to standardize some filenames when running your job, though they are named as you expect \nin the submission directory (per the submit file contents).",
            "title": "Running a Script Job From the Submit Directory"
        },
        {
            "location": "/materials/day1/part1-ex3-jobs/#extra-challenge",
            "text": "Note  There are Extra Challenges throughout the school curriculum. You may be better off coming back to these after you've completed all other exercises for your current working session.   Below is a simple Python script that does something similar to the shell script above. Run this Python script using HTCondor.  #!/usr/bin/env python\n\n\"\"\"Extra Challenge for OSG User School\nWritten by Tim Cartwright\nSubmitted to CHTC by #YOUR_NAME#\n\"\"\"\n\nimport getpass\nimport os\nimport platform\nimport socket\nimport sys\nimport time\n\narguments = None\nif len(sys.argv) > 1:\n    arguments = '\"' + ' '.join(sys.argv[1:]) + '\"'\n\nprint >> sys.stderr, __doc__\nprint 'Time    :', time.strftime('%Y-%m-%d (%a) %H:%M:%S %Z')\nprint 'Host    :', getpass.getuser(), '@', socket.gethostname()\nuname = platform.uname()\nprint \"System  :\", uname[0], uname[2], uname[4]\nprint \"Version :\", platform.python_version()\nprint \"Program :\", sys.executable\nprint 'Script  :', os.path.abspath(__file__)\nprint 'Args    :', arguments   Note  For the Python script, above, you'll want to increase the memory request to at least 64MB.\nWe will talk about tuning resource requests, later.",
            "title": "Extra Challenge"
        },
        {
            "location": "/materials/day1/part1-ex4-logs/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 1.4: Read and Interpret Log Files\n\u00b6\n\n\nThe goal of this exercise is quite simple: \nLearn to understand the contents of a job log file, which is where HTCondor describes the steps \ntaken to run your job.\nWhen things go wrong with your job, the log is the best place to look for first pointers (in addition to the .err file).\n\n\nThis exercise is short, but you'll want to at least read over it before moving on (and come back later, if you can't run through it now).\n\n\nReading a Log File\n\u00b6\n\n\nFor this exercise, we can examine a log file for any previous job that you have run. The example output below is based on the \nsleep 60\n job.\n\n\nA job log file is updated throughout the life of a job, usually at key events. Each event starts with a heading that indicates what happened and when. Here are \nall\n of the event headings from the \nsleep\n job log (detailed output in between headings has been omitted here):\n\n\n000 (5739.000.000) 07/25 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...>\n001 (5739.000.000) 07/25 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...>\n006 (5739.000.000) 07/25 10:45:20 Image size of job updated: 72\n006 (5739.000.000) 07/25 10:46:11 Image size of job updated: 4072\n005 (5739.000.000) 07/25 10:46:11 Job terminated.\n\n\n\n\n\nThere is a lot of extra information in those lines, but you can see:\n\n\n\n\nThe job ID: cluster 5739, process 0 (written \n000\n)\n\n\nThe date and local time of each event\n\n\nA brief description of the event: submission, execution, some information updates, and termination\n\n\n\n\nSome events provide no information in addition to the heading. For example:\n\n\n000 (5739.000.000) 07/25 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...>\n...\n\n\n\n\n\nand\n\n\n001 (5739.000.000) 07/25 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...>\n...\n\n\n\n\n\n\n\nNote\n\n\nEach event ends with a line that contains only 3 dots: \n...\n\n\n\n\nBut the periodic information update event contains some additional information:\n\n\n006 (5739.000.000) 07/25 10:45:20 Image size of job updated: 72\n    1  -  MemoryUsage of job (MB)\n    72  -  ResidentSetSize of job (KB)\n...\n\n\n\n\n\nThese updates record the amount of memory that the job is using on the execute machine. This can be helpful information, so that in future runs of the job, you can tell HTCondor how much memory you will need.\n\n\nThe job termination event includes a great deal of additional information:\n\n\n005 (5739.000.000) 07/25 10:46:11 Job terminated.\n    (1) Normal termination (return value 0)\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage\n    0  -  Run Bytes Sent By Job\n    27848  -  Run Bytes Received By Job\n    0  -  Total Bytes Sent By Job\n    27848  -  Total Bytes Received By Job\n    Partitionable Resources :    Usage  Request Allocated\n       Cpus                 :                 1         1\n       Disk (KB)            :       40       30   4203309\n       Memory (MB)          :        1        1         1\n...\n\n\n\n\n\nProbably the most interesting information is:\n\n\n\n\nThe \nreturn value\n (\n0\n here, means the executable completed and didn't indicate any internal errors; non-zero usually means failure)\n\n\nThe total number of bytes transferred each way, which could be useful if your network is slow\n\n\nThe \nPartitionable Resources\n table, especially disk and memory usage, which will inform larger submissions.\n\n\n\n\nThere are many other kinds of events, but the ones above will occur in almost every job log.\n\n\nUnderstanding When Job Log Events Are Written\n\u00b6\n\n\nWhen are events written to the job log file? Let\u2019s find out. Read through the entire procedure below before starting, because some parts of the process are time sensitive.\n\n\n\n\nChange the \nsleep\n job submit file, so that the job sleeps for 2 minutes (= 120 seconds)\n\n\nSubmit the updated sleep job\n\n\nAs soon as the \ncondor_submit\n command finishes, hit the return key a few times, to create some blank lines\n\n\n\n\nRight away, run a command to show the log file and \nkeep showing\n updates as they occur:\n\n\nusername@learn $\n tail -f sleep.log\n\n\n\n\n\n\n\n\n\nWatch the output carefully. When do events appear in the log file?\n\n\n\n\nAfter the termination event appears, press Control-C to end the \ntail\n command and return to the shell prompt.\n\n\n\n\nUnderstanding How HTCondor Writes Files\n\u00b6\n\n\nWhen HTCondor writes the output, error, and log files, does it erase the previous contents of the file or does it add new lines onto the end? Let\u2019s find out!\n\n\nFor this exercise, we can use the \nhostname\n job from earlier.\n\n\n\n\nEdit the \nhostname\n submit file so that it uses new and unique filenames for output, error, and log files.\n\nAlternatively, delete any existing output, error, and log files from previous runs of the \nhostname\n job.\n\n\nSubmit the job three separate times in a row (there are better ways to do this, which we will cover in the next lecture)\n\n\nWait for all three jobs to finish\n\n\nExamine the output file: How many hostnames are there? Did HTCondor erase the previous contents for each job, or add new lines?\n\n\nExamine the log file\u2026 carefully: What happened there? Pay close attention to the times and job IDs of the events.\n\n\n\n\nIf you have questions about how HTCondor handles these files, you could try finding relevant sections of the manual (this is hard and not as useful as one would hope), discuss it with neighbors or instructors, or ask questions at the end of this session.",
            "title": "Exercise 1.4"
        },
        {
            "location": "/materials/day1/part1-ex4-logs/#monday-exercise-14-read-and-interpret-log-files",
            "text": "The goal of this exercise is quite simple: \nLearn to understand the contents of a job log file, which is where HTCondor describes the steps \ntaken to run your job.\nWhen things go wrong with your job, the log is the best place to look for first pointers (in addition to the .err file).  This exercise is short, but you'll want to at least read over it before moving on (and come back later, if you can't run through it now).",
            "title": "Monday Exercise 1.4: Read and Interpret Log Files"
        },
        {
            "location": "/materials/day1/part1-ex4-logs/#reading-a-log-file",
            "text": "For this exercise, we can examine a log file for any previous job that you have run. The example output below is based on the  sleep 60  job.  A job log file is updated throughout the life of a job, usually at key events. Each event starts with a heading that indicates what happened and when. Here are  all  of the event headings from the  sleep  job log (detailed output in between headings has been omitted here):  000 (5739.000.000) 07/25 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...>\n001 (5739.000.000) 07/25 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...>\n006 (5739.000.000) 07/25 10:45:20 Image size of job updated: 72\n006 (5739.000.000) 07/25 10:46:11 Image size of job updated: 4072\n005 (5739.000.000) 07/25 10:46:11 Job terminated.  There is a lot of extra information in those lines, but you can see:   The job ID: cluster 5739, process 0 (written  000 )  The date and local time of each event  A brief description of the event: submission, execution, some information updates, and termination   Some events provide no information in addition to the heading. For example:  000 (5739.000.000) 07/25 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...>\n...  and  001 (5739.000.000) 07/25 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...>\n...   Note  Each event ends with a line that contains only 3 dots:  ...   But the periodic information update event contains some additional information:  006 (5739.000.000) 07/25 10:45:20 Image size of job updated: 72\n    1  -  MemoryUsage of job (MB)\n    72  -  ResidentSetSize of job (KB)\n...  These updates record the amount of memory that the job is using on the execute machine. This can be helpful information, so that in future runs of the job, you can tell HTCondor how much memory you will need.  The job termination event includes a great deal of additional information:  005 (5739.000.000) 07/25 10:46:11 Job terminated.\n    (1) Normal termination (return value 0)\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage\n    0  -  Run Bytes Sent By Job\n    27848  -  Run Bytes Received By Job\n    0  -  Total Bytes Sent By Job\n    27848  -  Total Bytes Received By Job\n    Partitionable Resources :    Usage  Request Allocated\n       Cpus                 :                 1         1\n       Disk (KB)            :       40       30   4203309\n       Memory (MB)          :        1        1         1\n...  Probably the most interesting information is:   The  return value  ( 0  here, means the executable completed and didn't indicate any internal errors; non-zero usually means failure)  The total number of bytes transferred each way, which could be useful if your network is slow  The  Partitionable Resources  table, especially disk and memory usage, which will inform larger submissions.   There are many other kinds of events, but the ones above will occur in almost every job log.",
            "title": "Reading a Log File"
        },
        {
            "location": "/materials/day1/part1-ex4-logs/#understanding-when-job-log-events-are-written",
            "text": "When are events written to the job log file? Let\u2019s find out. Read through the entire procedure below before starting, because some parts of the process are time sensitive.   Change the  sleep  job submit file, so that the job sleeps for 2 minutes (= 120 seconds)  Submit the updated sleep job  As soon as the  condor_submit  command finishes, hit the return key a few times, to create some blank lines   Right away, run a command to show the log file and  keep showing  updates as they occur:  username@learn $  tail -f sleep.log    Watch the output carefully. When do events appear in the log file?   After the termination event appears, press Control-C to end the  tail  command and return to the shell prompt.",
            "title": "Understanding When Job Log Events Are Written"
        },
        {
            "location": "/materials/day1/part1-ex4-logs/#understanding-how-htcondor-writes-files",
            "text": "When HTCondor writes the output, error, and log files, does it erase the previous contents of the file or does it add new lines onto the end? Let\u2019s find out!  For this exercise, we can use the  hostname  job from earlier.   Edit the  hostname  submit file so that it uses new and unique filenames for output, error, and log files. \nAlternatively, delete any existing output, error, and log files from previous runs of the  hostname  job.  Submit the job three separate times in a row (there are better ways to do this, which we will cover in the next lecture)  Wait for all three jobs to finish  Examine the output file: How many hostnames are there? Did HTCondor erase the previous contents for each job, or add new lines?  Examine the log file\u2026 carefully: What happened there? Pay close attention to the times and job IDs of the events.   If you have questions about how HTCondor handles these files, you could try finding relevant sections of the manual (this is hard and not as useful as one would hope), discuss it with neighbors or instructors, or ask questions at the end of this session.",
            "title": "Understanding How HTCondor Writes Files"
        },
        {
            "location": "/materials/day1/part1-ex5-request/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 1.5: Declare Resource Needs\n\u00b6\n\n\nThe goal of this exercise is to demonstrate how to test and tune the \nrequest_X\n statements in a submit file for when you don't know what resources your job needs.\n\n\nThere are three special resource request statements that you can use (optionally) in an HTCondor submit file:\n\n\n\n\nrequest_cpus\n for the number of CPUs your job will use (most softwares will take an argument to control this number, and it's usually otherwise \"1\")\n\n\nrequest_memory\n for the maximum amount of run-time memory your job may use\n\n\nrequest_disk\n for the maximum amount of disk space your job may use (including the executable and all other data that may show up during the job)\n\n\n\n\nHTCondor defaults to certain reasonable values for these request settings, so you do not need to use them to get \nsmall\n jobs to run. \nHowever, it is in \nYOUR\n best interest to always estimate resource requests before submitting any job, but to definitely tune your requests before submitting multiple jobs. In many HTCondor pools:\n\n\n\n\nIf your job goes over the request values, it may be removed from the execute machine and held (status 'H' in the \ncondor_q\n output, awaiting action on your part) without saving any partial job output files. So it is a disadvantage to not declare your resource needs or if you underestimate them. \n\n\nConversely, if you overestimate them by too much, your jobs will match to fewer slots (and with a longer average wait time) \nand\n you'll be hogging up resources that you don't need, but that could be used for the jobs of other users. In the long run, it works better for all users of the pool if you declare what you really need.\n\n\n\n\nBut how do you know what to request? In particular, we are concerned with memory and disk here; requesting multiple CPUs and using them is covered a bit in later school materials, but true HTC splits work up into jobs that each use as few CPU cores as possible (one CPU core is always best to have the most jobs running and completing soonest).\n\n\nDetermining Resource Needs Before Running Any Jobs\n\u00b6\n\n\n\n\nNote\n\n\nIf you are running short on time, you can skip to \"Determining Resource Needs By Running Test Jobs\", below, but try to come back and read over this part at some point.\n\n\n\n\nIt can be very difficult to predict the memory needs of your running program without running tests. Typically, the memory size of a job changes over time, making the task even trickier. \nIf you have knowledge ahead of time about your job\u2019s maximum memory needs, use that, or a maybe a number that's just a bit higher, to be safe. Worst case scenario, you can request a fairly large amount of memory (as high as what's on your laptop or other server, if you know your program can run without crashing) for a first test job, OR you can run the program locally and 'watch' it:\n\n\nExamining a Running Program on a Local Computer\n\u00b6\n\n\nWhen working on a shared submit server, you should not run computationally-intensive work because it can use resources needed by HTCondor to manage the queue for all uses. \nHowever, you may have access to other computers (your laptop, for example, or another server) where you can observe the memory usage of a program. The downside is that you'll have to watch a program run for essentially the entire time, to make sure you catch the maximum memory usage.\n\n\nFor Memory:\n\u00b6\n\n\nOn Mac and Windows, for example, the \"Activity Monitor\" and \"Task Manager\" applications may be useful. On a Mac or Linux system, you can use the \nps\n command or the \ntop\n command in the Terminal to watch a running program and see (roughly) how much memory it is using. Full coverage of these tools is beyond the scope of this exercise, but here are two quick examples:\n\n\nUsing \nps\n:\n\n\nusername@learn $\n ps ux\n\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n\n\ncat      24342  0.0  0.0  90224  1864 ?        S    13:39   0:00 sshd: cat@pts/0  \n\n\ncat      24343  0.0  0.0  66096  1580 pts/0    Ss   13:39   0:00 -bash\n\n\ncat      25864  0.0  0.0  65624   996 pts/0    R+   13:52   0:00 ps ux\n\n\ncat      30052  0.0  0.0  90720  2456 ?        S    Jun22   0:00 sshd: cat@pts/2  \n\n\ncat      30053  0.0  0.0  66096  1624 pts/2    Ss+  Jun22   0:00 -bash\n\n\n\n\n\n\nThe Resident Set Size (\nRSS\n) column, highlighted above, gives a rough indication of the memory usage (in KB) of each running process. If your program runs long enough, you can run this command several times and note the greatest value.\n\n\nUsing \ntop\n:\n\n\nusername@learn $\n top -u \n<USERNAME>\n\n\ntop - 13:55:31 up 11 days, 20:59,  5 users,  load average: 0.12, 0.12, 0.09\n\n\nTasks: 198 total,   1 running, 197 sleeping,   0 stopped,   0 zombie\n\n\nCpu(s):  1.2%us,  0.1%sy,  0.0%ni, 98.5%id,  0.2%wa,  0.0%hi,  0.1%si,  0.0%st\n\n\nMem:   4001440k total,  3558028k used,   443412k free,   258568k buffers\n\n\nSwap:  4194296k total,      148k used,  4194148k free,  2960760k cached\n\n\n\n  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND\n\n\n24342 cat       15   0 90224 1864 1096 S  0.0  0.0   0:00.26 sshd\n\n\n24343 cat       15   0 66096 1580 1232 S  0.0  0.0   0:00.07 bash\n\n\n25927 cat       15   0 12760 1196  836 R  0.0  0.0   0:00.01 top\n\n\n30052 cat       16   0 90720 2456 1112 S  0.0  0.1   0:00.69 sshd\n\n\n30053 cat       18   0 66096 1624 1236 S  0.0  0.0   0:00.37 bash\n\n\n\n\n\n\nThe \ntop\n command (shown here with an option to limit the output to a single user ID) also shows information about running processes, but updates periodically by itself. Type the letter \nq\n to quit the interactive display. Again, the highlighted \nRES\n column shows an approximation of memory usage.\n\n\nFor Disk:\n\u00b6\n\n\nDetermining disk needs may be a bit simpler, because you can check on the size of files that a program is using while it runs. However, it is important to count all files that HTCondor counts to get an accurate size. HTCondor counts \neverything\n in your job sandbox toward your job\u2019s disk usage:\n\n\n\n\nThe executable itself\n\n\nAll \"input\" files (anything else that gets transferred TO the job, even if you don't think of it as \"input\")\n\n\nAll files created during the job (broadly defined as \"output\"), including the captured standard output and error files that you list in the submit file.\n\n\nAll temporary files created in the sandbox, even if they get deleted by the executable before it's done.\n\n\n\n\nIf you can run your program within a single directory on a local computer (not on the submit server), you should be able to view files and their sizes with the \nls\n and \ndu\n commands.\n\n\nDetermining Resource Needs By Running Test Jobs (BEST)\n\u00b6\n\n\nDespite the techniques mentioned above, by far the easiest approach to measuring your job\u2019s resource needs is to run one or a small number of sample jobs and have HTCondor itself tell you about the resources used during the runs.\n\n\nFor example, here is a strange Python script that does not do anything useful, but consumes some real resources while running:\n\n\n#!/usr/bin/env python\nimport time\nimport os\nsize = 1000000\nnumbers = []\nfor i in xrange(size): numbers.append(str(i))\ntempfile = open('temp', 'w')\ntempfile.write(' '.join(numbers))\ntempfile.close()\ntime.sleep(60)\nos.remove('temp')\n\n\n\n\n\nWithout trying to figure out what this code does or how many resources it uses, create a submit file for it, \nand run it once with HTCondor, starting with somewhat high memory requests (\"1GB\" for memory and disk is a good starting point, unless you think the job will use far more).\nWhen it is done, examine the log file. In particular, we care about these lines:\n\n\n    Partitionable Resources :    Usage  Request Allocated\n       Cpus                 :                 1         1\n       Disk (KB)            :     6739  1048576   8022934\n       Memory (MB)          :        3     1024      1024\n\n\n\n\n\nSo, now we know that HTCondor saw that the job used 6,739 KB of disk (= about 6.5 MB) and 3 MB of memory!\n\n\nThis is a great technique for determining the real resource needs of your job. If you think resource needs vary from run to run, submit a few sample jobs and look at all the results. And it never hurts to round up your resource requests a little, just in case your job occasionally uses more resources.\n\n\nSetting Resource Requirements\n\u00b6\n\n\nOnce you know your job\u2019s resource requirements, it is easy to declare them in your submit file. For example, taking our results above as an example, we might slightly increase our requests above what was used, just to be safe:\n\n\nrequest_memory = 4MB  \n# rounded up from 3 MB\n\nrequest_disk = 7MB  \n# rounded up from 6.5 MB\n\n\n\n\n\n\nPay close attention to units:\n\n\n\n\nWithout explicit units, \nrequest_memory\n is in MB (megabytes)\n\n\nWithout explicit units, \nrequest_disk\n is in KB (kilobytes)\n\n\nAllowable units are \nKB\n (kilobytes), \nMB\n (megabytes), \nGB\n (gigabytes), and \nTB\n (terabytes)\n\n\n\n\nHTCondor translates these requirements into attributes that become part of the job's \nrequirements\n expression. However, do not put your CPU, memory, and disk requirements directly into the \nrequirements\n expression; use the \nrequest_XXX\n statements instead.\n\n\nIf you still have time in this working session, Add these requirements to your submit file for the Python script, rerun the job, and confirm in the log file that your requests were used.\n\n\nAfter changing the requirements in your submit file, did your job run successfully? If not, why?\n(Hint: HTCondor polls a job's resource use on a timer. How long are these jobs running for?)",
            "title": "Exercise 1.5"
        },
        {
            "location": "/materials/day1/part1-ex5-request/#monday-exercise-15-declare-resource-needs",
            "text": "The goal of this exercise is to demonstrate how to test and tune the  request_X  statements in a submit file for when you don't know what resources your job needs.  There are three special resource request statements that you can use (optionally) in an HTCondor submit file:   request_cpus  for the number of CPUs your job will use (most softwares will take an argument to control this number, and it's usually otherwise \"1\")  request_memory  for the maximum amount of run-time memory your job may use  request_disk  for the maximum amount of disk space your job may use (including the executable and all other data that may show up during the job)   HTCondor defaults to certain reasonable values for these request settings, so you do not need to use them to get  small  jobs to run. \nHowever, it is in  YOUR  best interest to always estimate resource requests before submitting any job, but to definitely tune your requests before submitting multiple jobs. In many HTCondor pools:   If your job goes over the request values, it may be removed from the execute machine and held (status 'H' in the  condor_q  output, awaiting action on your part) without saving any partial job output files. So it is a disadvantage to not declare your resource needs or if you underestimate them.   Conversely, if you overestimate them by too much, your jobs will match to fewer slots (and with a longer average wait time)  and  you'll be hogging up resources that you don't need, but that could be used for the jobs of other users. In the long run, it works better for all users of the pool if you declare what you really need.   But how do you know what to request? In particular, we are concerned with memory and disk here; requesting multiple CPUs and using them is covered a bit in later school materials, but true HTC splits work up into jobs that each use as few CPU cores as possible (one CPU core is always best to have the most jobs running and completing soonest).",
            "title": "Monday Exercise 1.5: Declare Resource Needs"
        },
        {
            "location": "/materials/day1/part1-ex5-request/#determining-resource-needs-before-running-any-jobs",
            "text": "Note  If you are running short on time, you can skip to \"Determining Resource Needs By Running Test Jobs\", below, but try to come back and read over this part at some point.   It can be very difficult to predict the memory needs of your running program without running tests. Typically, the memory size of a job changes over time, making the task even trickier. \nIf you have knowledge ahead of time about your job\u2019s maximum memory needs, use that, or a maybe a number that's just a bit higher, to be safe. Worst case scenario, you can request a fairly large amount of memory (as high as what's on your laptop or other server, if you know your program can run without crashing) for a first test job, OR you can run the program locally and 'watch' it:",
            "title": "Determining Resource Needs Before Running Any Jobs"
        },
        {
            "location": "/materials/day1/part1-ex5-request/#examining-a-running-program-on-a-local-computer",
            "text": "When working on a shared submit server, you should not run computationally-intensive work because it can use resources needed by HTCondor to manage the queue for all uses. \nHowever, you may have access to other computers (your laptop, for example, or another server) where you can observe the memory usage of a program. The downside is that you'll have to watch a program run for essentially the entire time, to make sure you catch the maximum memory usage.",
            "title": "Examining a Running Program on a Local Computer"
        },
        {
            "location": "/materials/day1/part1-ex5-request/#for-memory",
            "text": "On Mac and Windows, for example, the \"Activity Monitor\" and \"Task Manager\" applications may be useful. On a Mac or Linux system, you can use the  ps  command or the  top  command in the Terminal to watch a running program and see (roughly) how much memory it is using. Full coverage of these tools is beyond the scope of this exercise, but here are two quick examples:  Using  ps :  username@learn $  ps ux USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND  cat      24342  0.0  0.0  90224  1864 ?        S    13:39   0:00 sshd: cat@pts/0    cat      24343  0.0  0.0  66096  1580 pts/0    Ss   13:39   0:00 -bash  cat      25864  0.0  0.0  65624   996 pts/0    R+   13:52   0:00 ps ux  cat      30052  0.0  0.0  90720  2456 ?        S    Jun22   0:00 sshd: cat@pts/2    cat      30053  0.0  0.0  66096  1624 pts/2    Ss+  Jun22   0:00 -bash   The Resident Set Size ( RSS ) column, highlighted above, gives a rough indication of the memory usage (in KB) of each running process. If your program runs long enough, you can run this command several times and note the greatest value.  Using  top :  username@learn $  top -u  <USERNAME>  top - 13:55:31 up 11 days, 20:59,  5 users,  load average: 0.12, 0.12, 0.09  Tasks: 198 total,   1 running, 197 sleeping,   0 stopped,   0 zombie  Cpu(s):  1.2%us,  0.1%sy,  0.0%ni, 98.5%id,  0.2%wa,  0.0%hi,  0.1%si,  0.0%st  Mem:   4001440k total,  3558028k used,   443412k free,   258568k buffers  Swap:  4194296k total,      148k used,  4194148k free,  2960760k cached    PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND  24342 cat       15   0 90224 1864 1096 S  0.0  0.0   0:00.26 sshd  24343 cat       15   0 66096 1580 1232 S  0.0  0.0   0:00.07 bash  25927 cat       15   0 12760 1196  836 R  0.0  0.0   0:00.01 top  30052 cat       16   0 90720 2456 1112 S  0.0  0.1   0:00.69 sshd  30053 cat       18   0 66096 1624 1236 S  0.0  0.0   0:00.37 bash   The  top  command (shown here with an option to limit the output to a single user ID) also shows information about running processes, but updates periodically by itself. Type the letter  q  to quit the interactive display. Again, the highlighted  RES  column shows an approximation of memory usage.",
            "title": "For Memory:"
        },
        {
            "location": "/materials/day1/part1-ex5-request/#for-disk",
            "text": "Determining disk needs may be a bit simpler, because you can check on the size of files that a program is using while it runs. However, it is important to count all files that HTCondor counts to get an accurate size. HTCondor counts  everything  in your job sandbox toward your job\u2019s disk usage:   The executable itself  All \"input\" files (anything else that gets transferred TO the job, even if you don't think of it as \"input\")  All files created during the job (broadly defined as \"output\"), including the captured standard output and error files that you list in the submit file.  All temporary files created in the sandbox, even if they get deleted by the executable before it's done.   If you can run your program within a single directory on a local computer (not on the submit server), you should be able to view files and their sizes with the  ls  and  du  commands.",
            "title": "For Disk:"
        },
        {
            "location": "/materials/day1/part1-ex5-request/#determining-resource-needs-by-running-test-jobs-best",
            "text": "Despite the techniques mentioned above, by far the easiest approach to measuring your job\u2019s resource needs is to run one or a small number of sample jobs and have HTCondor itself tell you about the resources used during the runs.  For example, here is a strange Python script that does not do anything useful, but consumes some real resources while running:  #!/usr/bin/env python\nimport time\nimport os\nsize = 1000000\nnumbers = []\nfor i in xrange(size): numbers.append(str(i))\ntempfile = open('temp', 'w')\ntempfile.write(' '.join(numbers))\ntempfile.close()\ntime.sleep(60)\nos.remove('temp')  Without trying to figure out what this code does or how many resources it uses, create a submit file for it, \nand run it once with HTCondor, starting with somewhat high memory requests (\"1GB\" for memory and disk is a good starting point, unless you think the job will use far more).\nWhen it is done, examine the log file. In particular, we care about these lines:      Partitionable Resources :    Usage  Request Allocated\n       Cpus                 :                 1         1\n       Disk (KB)            :     6739  1048576   8022934\n       Memory (MB)          :        3     1024      1024  So, now we know that HTCondor saw that the job used 6,739 KB of disk (= about 6.5 MB) and 3 MB of memory!  This is a great technique for determining the real resource needs of your job. If you think resource needs vary from run to run, submit a few sample jobs and look at all the results. And it never hurts to round up your resource requests a little, just in case your job occasionally uses more resources.",
            "title": "Determining Resource Needs By Running Test Jobs (BEST)"
        },
        {
            "location": "/materials/day1/part1-ex5-request/#setting-resource-requirements",
            "text": "Once you know your job\u2019s resource requirements, it is easy to declare them in your submit file. For example, taking our results above as an example, we might slightly increase our requests above what was used, just to be safe:  request_memory = 4MB   # rounded up from 3 MB \nrequest_disk = 7MB   # rounded up from 6.5 MB   Pay close attention to units:   Without explicit units,  request_memory  is in MB (megabytes)  Without explicit units,  request_disk  is in KB (kilobytes)  Allowable units are  KB  (kilobytes),  MB  (megabytes),  GB  (gigabytes), and  TB  (terabytes)   HTCondor translates these requirements into attributes that become part of the job's  requirements  expression. However, do not put your CPU, memory, and disk requirements directly into the  requirements  expression; use the  request_XXX  statements instead.  If you still have time in this working session, Add these requirements to your submit file for the Python script, rerun the job, and confirm in the log file that your requests were used.  After changing the requirements in your submit file, did your job run successfully? If not, why?\n(Hint: HTCondor polls a job's resource use on a timer. How long are these jobs running for?)",
            "title": "Setting Resource Requirements"
        },
        {
            "location": "/materials/day1/part1-ex6-remove/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 1.6: Remove Jobs From the Queue\n\u00b6\n\n\nThe goal of this exercise is to show you how to remove jobs from the queue. This is helpful if you make a mistake, do not want to wait for a job to complete, or otherwise need to fix things. For example, if some test jobs go on hold for using too much memory or disk, you may want to just remove them, edit the submit files, and then submit again.\n\n\nSkip this exercise and come back to it if you are short on time, or until you need to remove jobs for other exercises\n\n\n\n\nNote\n\n\nPlease remember to remove any jobs from the queue that you have given up on. Otherwise, the queue will start to get very long with jobs that will waste resources (and decrease your priority), or that may never run (if they're on hold, or have other issues keeping them from matching).\n\n\n\n\nThis exercise is very short, but if you are out of time, you can come back to it later.\n\n\nRemoving a Job or Cluster From the Queue\n\u00b6\n\n\nTo practice removing jobs from the queue, you need a job in the queue!\n\n\n\n\nSubmit a job from an earlier exercise\n\n\nDetermine the job ID (\ncluster.process\n) from the \ncondor_submit\n output or from \ncondor_q\n\n\n\n\nRemove the job:\n\n\nusername@learn $\n condor_rm \n<JOB.ID>\n\n\n\n\n\n\nUse the full job ID this time, e.g. \n5759.0\n.\n\n\n\n\n\n\nDid the job leave the queue immediately? If not, about how long did it take?\n\n\n\n\n\n\nSo far, we have created job clusters that contain only one job process (the \n.0\n part of the job ID). That will change soon, so it is good to know how to remove a specific job ID. However, it is possible to remove all jobs that are part of a cluster at once. Simply omit the job process (the \n.0\n part of the job ID) in the \ncondor_rm\n command:\n\n\nusername@learn $\n condor_rm \n<CLUSTER>\n\n\n\n\n\n\nFinally, you can include many job clusters and full job IDs in a single \ncondor_rm\n command. For example:\n\n\nusername@learn $\n condor_rm \n5768\n \n5769\n \n5770\n.0 \n5771\n.2\n\n\n\n\n\nRemoving All of Your Jobs\n\u00b6\n\n\nIf you really want to remove all of your jobs at once, you can do that with:\n\n\nusername@learn $\n condor_rm \n<USERNAME>\n\n\n\n\n\n\nIf you want to test it: (optional, though you'll likely need this in the future)\n1.  Quickly submit several jobs from past exercises\n1.  View the jobs in the queue with \ncondor_q\n\n1.  Remove them all with the above command\n1.  Use \ncondor_q\n to track progress\n\n\nIn case you are wondering, you can remove only your own jobs. HTCondor administrators can remove anyone\u2019s jobs, so be nice to them.",
            "title": "Exercise 1.6"
        },
        {
            "location": "/materials/day1/part1-ex6-remove/#monday-exercise-16-remove-jobs-from-the-queue",
            "text": "The goal of this exercise is to show you how to remove jobs from the queue. This is helpful if you make a mistake, do not want to wait for a job to complete, or otherwise need to fix things. For example, if some test jobs go on hold for using too much memory or disk, you may want to just remove them, edit the submit files, and then submit again.  Skip this exercise and come back to it if you are short on time, or until you need to remove jobs for other exercises   Note  Please remember to remove any jobs from the queue that you have given up on. Otherwise, the queue will start to get very long with jobs that will waste resources (and decrease your priority), or that may never run (if they're on hold, or have other issues keeping them from matching).   This exercise is very short, but if you are out of time, you can come back to it later.",
            "title": "Monday Exercise 1.6: Remove Jobs From the Queue"
        },
        {
            "location": "/materials/day1/part1-ex6-remove/#removing-a-job-or-cluster-from-the-queue",
            "text": "To practice removing jobs from the queue, you need a job in the queue!   Submit a job from an earlier exercise  Determine the job ID ( cluster.process ) from the  condor_submit  output or from  condor_q   Remove the job:  username@learn $  condor_rm  <JOB.ID>   Use the full job ID this time, e.g.  5759.0 .    Did the job leave the queue immediately? If not, about how long did it take?    So far, we have created job clusters that contain only one job process (the  .0  part of the job ID). That will change soon, so it is good to know how to remove a specific job ID. However, it is possible to remove all jobs that are part of a cluster at once. Simply omit the job process (the  .0  part of the job ID) in the  condor_rm  command:  username@learn $  condor_rm  <CLUSTER>   Finally, you can include many job clusters and full job IDs in a single  condor_rm  command. For example:  username@learn $  condor_rm  5768   5769   5770 .0  5771 .2",
            "title": "Removing a Job or Cluster From the Queue"
        },
        {
            "location": "/materials/day1/part1-ex6-remove/#removing-all-of-your-jobs",
            "text": "If you really want to remove all of your jobs at once, you can do that with:  username@learn $  condor_rm  <USERNAME>   If you want to test it: (optional, though you'll likely need this in the future)\n1.  Quickly submit several jobs from past exercises\n1.  View the jobs in the queue with  condor_q \n1.  Remove them all with the above command\n1.  Use  condor_q  to track progress  In case you are wondering, you can remove only your own jobs. HTCondor administrators can remove anyone\u2019s jobs, so be nice to them.",
            "title": "Removing All of Your Jobs"
        },
        {
            "location": "/materials/day1/part1-ex7-compile/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Bonus Exercise 1.7: Compile and Run Some C Code\n\u00b6\n\n\nThe goal of this exercise is to show that compiled code works just fine in HTCondor. It is mainly of interest to people who have their own C code to run (or C++, or really any compiled code, although Java would be handled a bit differently).\n\n\nPreparing a C Executable\n\u00b6\n\n\nWhen preparing a C program for HTCondor, it is best to compile and link the executable statically, so that it does not depend on external libraries and their particular versions. Why is this important? When your compiled C program is sent to another machine for execution, that machine may not have the same libraries that you have on your submit machine (or wherever you compile the program). If the libraries are not available or are the wrong versions, your program may fail or, perhaps worse, silently produce the wrong results.\n\n\nHere is a simple C program to try using (thanks, Alain Roy):\n\n\n#include <stdio.h>\n\nint main(int argc, char **argv)\n{\n    int sleep_time;\n    int input;\n    int failure;\n\n    if (argc != 3) {\n        printf(\"Usage: simple <sleep-time> <integer>\\n\");\n        failure = 1;\n    } else {\n        sleep_time = atoi(argv[1]);\n        input      = atoi(argv[2]);\n\n        printf(\"Thinking really hard for %d seconds...\\n\", sleep_time);\n        sleep(sleep_time);\n        printf(\"We calculated: %d\\n\", input * 2);\n        failure = 0;\n    }\n    return failure;\n}\n\n\n\n\n\nSave that code to a file, for example, \nsimple.c\n.\n\n\nCompile the program with static linking:\n\n\nusername@learn $\n gcc -static -o simple simple.c\n\n\n\n\n\nAs always, test that you can run your command from the command line first. First, without arguments to make sure it fails correctly:\n\n\nusername@learn $\n ./simple\n\n\n\n\n\nand then with valid arguments:\n\n\nusername@learn $\n ./simple \n5\n \n21\n\n\n\n\n\n\nRunning a Compiled C Program\n\u00b6\n\n\nThe rest is simple. In fact, it is no different than running any other program. Here is a basic submit file for the C program (call it simple.sub):\n\n\nexecutable = simple\narguments = \"60 64\"\n\noutput = c-program.out\nerror = c-program.err\nlog = c-program.log\n\nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\n\nqueue\n\n\n\n\n\nThen submit the job as usual!\n\n\nIn summary, it is easy to work with statically linked compiled code. It \nis\n possible to handle dynamically linked compiled code, but it is trickier. We will only mention this topic briefly on Wednesday.",
            "title": "Bonus Exercise 1.7"
        },
        {
            "location": "/materials/day1/part1-ex7-compile/#monday-bonus-exercise-17-compile-and-run-some-c-code",
            "text": "The goal of this exercise is to show that compiled code works just fine in HTCondor. It is mainly of interest to people who have their own C code to run (or C++, or really any compiled code, although Java would be handled a bit differently).",
            "title": "Monday Bonus Exercise 1.7: Compile and Run Some C Code"
        },
        {
            "location": "/materials/day1/part1-ex7-compile/#preparing-a-c-executable",
            "text": "When preparing a C program for HTCondor, it is best to compile and link the executable statically, so that it does not depend on external libraries and their particular versions. Why is this important? When your compiled C program is sent to another machine for execution, that machine may not have the same libraries that you have on your submit machine (or wherever you compile the program). If the libraries are not available or are the wrong versions, your program may fail or, perhaps worse, silently produce the wrong results.  Here is a simple C program to try using (thanks, Alain Roy):  #include <stdio.h>\n\nint main(int argc, char **argv)\n{\n    int sleep_time;\n    int input;\n    int failure;\n\n    if (argc != 3) {\n        printf(\"Usage: simple <sleep-time> <integer>\\n\");\n        failure = 1;\n    } else {\n        sleep_time = atoi(argv[1]);\n        input      = atoi(argv[2]);\n\n        printf(\"Thinking really hard for %d seconds...\\n\", sleep_time);\n        sleep(sleep_time);\n        printf(\"We calculated: %d\\n\", input * 2);\n        failure = 0;\n    }\n    return failure;\n}  Save that code to a file, for example,  simple.c .  Compile the program with static linking:  username@learn $  gcc -static -o simple simple.c  As always, test that you can run your command from the command line first. First, without arguments to make sure it fails correctly:  username@learn $  ./simple  and then with valid arguments:  username@learn $  ./simple  5   21",
            "title": "Preparing a C Executable"
        },
        {
            "location": "/materials/day1/part1-ex7-compile/#running-a-compiled-c-program",
            "text": "The rest is simple. In fact, it is no different than running any other program. Here is a basic submit file for the C program (call it simple.sub):  executable = simple\narguments = \"60 64\"\n\noutput = c-program.out\nerror = c-program.err\nlog = c-program.log\n\nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\n\nqueue  Then submit the job as usual!  In summary, it is easy to work with statically linked compiled code. It  is  possible to handle dynamically linked compiled code, but it is trickier. We will only mention this topic briefly on Wednesday.",
            "title": "Running a Compiled C Program"
        },
        {
            "location": "/materials/day1/part2-ex1-files/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 2.1: Work With Input and Output Files\n\u00b6\n\n\nThe goal of this exercise is make input files available to your job on the execute machine, and return output files back. This small change significantly adds to the kinds of jobs that you can run.\n\n\nViewing a Job Sandbox\n\u00b6\n\n\nBefore you learn to transfer files to and from your job, it is good to understand a bit more about the environment in which your job runs. When the HTCondor \nstarter\n process prepares to run your job, it creates a new directory for your job and all of its files. We call this directory the \njob sandbox\n, because it is your job\u2019s private space to play. Let\u2019s see what is in the job sandbox for a very simple job with no special input or output files.\n\n\n\n\n\n\nSave the script below in a file named \nsandbox.sh\n:\n\n\n#!/bin/sh\n\n\necho\n \n'Date: '\n \n`\ndate\n`\n\n\necho\n \n'Host: '\n \n`\nhostname\n`\n \n\necho\n \n'Sandbox: '\n \n`\npwd\n`\n \nls -alF\n\n# END\n\n\n\n\n\n\n\n\n\n\nCreate a submit file for this script and submit it.\n\n\n\n\nWhen the job finishes, look at the contents of the output file.\n\n\n\n\nIn the output file, note the \nSandbox:\n line: That is the full path to your job sandbox for the run. It was created just for your job, and it was removed as soon as your job finished.\n\n\nNext, look at the output that appears after the \nSandbox:\n line; it is the output from the \nls\n command in the script. It shows all of the files in your job sandbox, as they existed at the end of the execution of \nsandbox.sh\n. The files are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.chirp.config\n\n\nConfiguration for an advanced feature\n\n\n\n\n\n\n.job.ad\n\n\nThe job ClassAd\n\n\n\n\n\n\n.machine.ad\n\n\nThe machine ClassAd\n\n\n\n\n\n\n_condor_stderr\n\n\nSaved standard error from the job\n\n\n\n\n\n\n_condor_stdout\n\n\nSaved standard output from the job\n\n\n\n\n\n\ncondor_exec.exe\n\n\nThe executable, renamed from \nsandbox.sh\n\n\n\n\n\n\ntmp/\n\n\nA directory in which to put temporary files\n\n\n\n\n\n\n\n\nSo, HTCondor wrote copies of the job and machine ads (for use by the job, if desired), transferred your executable (\nsandbox.sh\n), renamed it (\ncondor_exec.exe\n), ran it, and saved its standard output and standard error into files. Notice that your submit file, which was in the same directory on the submit machine as your executable, was \nnot\n transferred, nor were any other files that happened to be in directory with the submit file.\n\n\nNow that we know something about the sandbox, we can transfer more files to and from it.\n\n\nRunning a Job With Input Files\n\u00b6\n\n\nNext, you will run a job that requires an input file. Remember, the initial job sandbox will contain only the renamed job executable, unless you tell HTCondor explicitly about every other file that needs to be transferred. Fortunately, this is easy.\n\n\nHere is a simple Python script that takes the name of an input file (containing one word per line) from the command line, counts the number of times each (lowercased) word occurs in the text, and prints out the final list of words and their counts.\n\n\n#!/usr/bin/env python\n\nimport os\nimport sys\n\nif len(sys.argv) != 2:\n    print 'Usage: %s DATA' % (os.path.basename(sys.argv[0]))\n    sys.exit(1)\ninput_filename = sys.argv[1]\n\nwords = {}\n\nmy_file = open(input_filename, 'r')\nfor line in my_file:\n    word = line.strip().lower()\n    if word in words:\n        words[word] += 1\n    else:\n        words[word] = 1\nmy_file.close()\n\nfor word in sorted(words.keys()):\n    print '%8d %s' % (words[word], word)\n\n\n\n\n\n\n\nSave the Python script in a file named \nfreq.py\n.\n\n\n\n\nDownload the input file for the script (263K lines, ~1.4 MB) and save it in your submit directory:\n\n\nusername@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/mon-2.1-words.txt\n\n\n\n\n\n\n\n\n\nCreate a basic submit file for the \nfreq.py\n executable.\n\n\n\n\n\n\nAdd a line to tell HTCondor to transfer the input file:\n\n\ntransfer_input_files = mon-2.1-words.txt\n\n\n\n\n\nAs with all submit file commands, it does not matter where this line goes, as long as it comes before the word \nqueue\n.\n\n\n\n\n\n\nDo not forget to add a line to name the input file as the argument to the Python script.\n\n\n\n\nSubmit the job, wait for it to finish, and check the output!\n\n\n\n\nIf things do not work the first time, keep trying! At this point in the exercises, we are telling you less and less explicitly how to do steps that you have done before. If you get stuck, ask a neighbor or one of the instructors.\n\n\n\n\nNote\n\n\nIf you want to transfer more than one input file, list all of them on a single \ntransfer_input_files\n command,\nseparated by commas.\nFor example, if there are three input files:\n\n\ntransfer_input_files = a.txt, b.txt, c.txt\n\n\n\n\n\n\n\nTransferring Output Files\n\u00b6\n\n\nSo far, we have relied on programs that send their output to the standard output and error streams, which HTCondor captures, saves, and returns back to the submit directory. But what if your program writes one or more files for its output? How do you tell HTCondor to bring them back?\n\n\nLet\u2019s start by exploring what happens to files that a job creates in the sandbox. We will use a very simple method for creating a new file: We will copy an input file to another name.\n\n\n\n\nFind or create a small input file (it is fine to use any small file from a previous exercise).\n\n\nCreate a submit file that transfers the input file and copies it to another name (as if doing \n/bin/cp input.txt output.txt\n on the command line)\n\n\nMake the output filename different than any filenames that are in your submit directory\n\n\nWhat is the \nexecutable\n line?\n\n\nWhat is the \narguments\n line?\n\n\nHow do you tell HTCondor to transfer the input file?\n\n\nAs always, use \noutput\n, \nerror\n, and \nlog\n filenames that are different from previous exercises\n\n\n\n\n\n\nSubmit the job and wait for it to finish.\n\n\n\n\nWhat happened? Can you tell what HTCondor did with the output file that was created (did it end up back on the submit server?), after it was created in the job sandbox? Look carefully at the list of files in your submit directory now.\n\n\nTransferring Specific Output Files\n\u00b6\n\n\nAs you saw in the last exercise, by default HTCondor transfers files that are created in the job sandbox back to the submit directory when the job finishes. In fact, HTCondor will also transfer back \nchanged\n input files, too. But, this only works for files that are in the top-level sandbox directory, and \nnot\n for ones contained in subdirectories.\n\n\nWhat if you want to bring back only \nsome\n output files, or output files contained in subdirectories?\n\n\nHere is a simple shell script that creates several files, including a copy of an input file in a new subdirectory:\n\n\n#!/bin/sh\nif [ $# -ne 1 ]; then echo \"Usage: $0 INPUT\"; exit 1; fi\ndate > output-timestamp.txt\ncal > output-calendar.txt\nmkdir subdirectory\ncp $1 subdirectory/backup-$1\n\n\n\n\n\nFirst, let\u2019s confirm that HTCondor does not bring back the output file in the subdirectory:\n\n\n\n\nSave the shell script in a file named \noutput.sh\n.\n\n\nWrite a submit file that transfers an input file and runs \noutput.sh\n on it (passing the filename as an argument).\n\n\nSubmit the job, wait for it to finish, and examine the contents of your submit directory.\n\n\n\n\nSuppose you decide that you want only the timestamp output file and all files in the subdirectory, but not the calendar output file. You can tell HTCondor to transfer these specific files:\n\n\ntransfer_output_files = output-timestamp.txt, subdirectory/\n\n\n\n\n\n\n\nNote\n\n\nSee the trailing slash (\n/\n) on the subdirectory?\nThat tells HTCondor to transfer back \nthe files\n contained in the subdirectory, but not the directory itself;\nthe files will be written directly into the submit directory.\nIf you want HTCondor to transfer back an entire directory, leave off the trailing slash.\n\n\n\n\n\n\nRemove all output files from the previous run, including \noutput-timestamp.txt\n and \noutput-calendar.txt\n.\n\n\nCopy the previous submit file that ran \noutput.sh\n and add the \ntransfer_output_files\n line from above.\n\n\nSubmit the job, wait for it to finish, and examine the contents of your submit directory.\n\n\n\n\nDid it work as you expected?\n\n\nThinking About Progress So Far\n\u00b6\n\n\nAt this point, you can do just about everything that you need in order to run jobs on a local HTC pool. You can identify the executable, arguments, and input files, and you can get output back from the job. This is a big achievement!\n\n\nIn some ways, everything after this exercise shows you how to submit multiple jobs at once and makes it easier to run certain kinds of jobs and deal with certain kinds of situations.\n\n\nReferences\n\u00b6\n\n\nThere are many more details about HTCondor\u2019s file transfer mechanism not covered here. For more information, read the \n\"Submitting Jobs Without a Shared Filesystem\"\n of the HTCondor Manual.",
            "title": "Exercise 2.1"
        },
        {
            "location": "/materials/day1/part2-ex1-files/#monday-exercise-21-work-with-input-and-output-files",
            "text": "The goal of this exercise is make input files available to your job on the execute machine, and return output files back. This small change significantly adds to the kinds of jobs that you can run.",
            "title": "Monday Exercise 2.1: Work With Input and Output Files"
        },
        {
            "location": "/materials/day1/part2-ex1-files/#viewing-a-job-sandbox",
            "text": "Before you learn to transfer files to and from your job, it is good to understand a bit more about the environment in which your job runs. When the HTCondor  starter  process prepares to run your job, it creates a new directory for your job and all of its files. We call this directory the  job sandbox , because it is your job\u2019s private space to play. Let\u2019s see what is in the job sandbox for a very simple job with no special input or output files.    Save the script below in a file named  sandbox.sh :  #!/bin/sh  echo   'Date: '   ` date `  echo   'Host: '   ` hostname `   echo   'Sandbox: '   ` pwd `  \nls -alF # END     Create a submit file for this script and submit it.   When the job finishes, look at the contents of the output file.   In the output file, note the  Sandbox:  line: That is the full path to your job sandbox for the run. It was created just for your job, and it was removed as soon as your job finished.  Next, look at the output that appears after the  Sandbox:  line; it is the output from the  ls  command in the script. It shows all of the files in your job sandbox, as they existed at the end of the execution of  sandbox.sh . The files are:           .chirp.config  Configuration for an advanced feature    .job.ad  The job ClassAd    .machine.ad  The machine ClassAd    _condor_stderr  Saved standard error from the job    _condor_stdout  Saved standard output from the job    condor_exec.exe  The executable, renamed from  sandbox.sh    tmp/  A directory in which to put temporary files     So, HTCondor wrote copies of the job and machine ads (for use by the job, if desired), transferred your executable ( sandbox.sh ), renamed it ( condor_exec.exe ), ran it, and saved its standard output and standard error into files. Notice that your submit file, which was in the same directory on the submit machine as your executable, was  not  transferred, nor were any other files that happened to be in directory with the submit file.  Now that we know something about the sandbox, we can transfer more files to and from it.",
            "title": "Viewing a Job Sandbox"
        },
        {
            "location": "/materials/day1/part2-ex1-files/#running-a-job-with-input-files",
            "text": "Next, you will run a job that requires an input file. Remember, the initial job sandbox will contain only the renamed job executable, unless you tell HTCondor explicitly about every other file that needs to be transferred. Fortunately, this is easy.  Here is a simple Python script that takes the name of an input file (containing one word per line) from the command line, counts the number of times each (lowercased) word occurs in the text, and prints out the final list of words and their counts.  #!/usr/bin/env python\n\nimport os\nimport sys\n\nif len(sys.argv) != 2:\n    print 'Usage: %s DATA' % (os.path.basename(sys.argv[0]))\n    sys.exit(1)\ninput_filename = sys.argv[1]\n\nwords = {}\n\nmy_file = open(input_filename, 'r')\nfor line in my_file:\n    word = line.strip().lower()\n    if word in words:\n        words[word] += 1\n    else:\n        words[word] = 1\nmy_file.close()\n\nfor word in sorted(words.keys()):\n    print '%8d %s' % (words[word], word)   Save the Python script in a file named  freq.py .   Download the input file for the script (263K lines, ~1.4 MB) and save it in your submit directory:  username@learn $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/mon-2.1-words.txt    Create a basic submit file for the  freq.py  executable.    Add a line to tell HTCondor to transfer the input file:  transfer_input_files = mon-2.1-words.txt  As with all submit file commands, it does not matter where this line goes, as long as it comes before the word  queue .    Do not forget to add a line to name the input file as the argument to the Python script.   Submit the job, wait for it to finish, and check the output!   If things do not work the first time, keep trying! At this point in the exercises, we are telling you less and less explicitly how to do steps that you have done before. If you get stuck, ask a neighbor or one of the instructors.   Note  If you want to transfer more than one input file, list all of them on a single  transfer_input_files  command,\nseparated by commas.\nFor example, if there are three input files:  transfer_input_files = a.txt, b.txt, c.txt",
            "title": "Running a Job With Input Files"
        },
        {
            "location": "/materials/day1/part2-ex1-files/#transferring-output-files",
            "text": "So far, we have relied on programs that send their output to the standard output and error streams, which HTCondor captures, saves, and returns back to the submit directory. But what if your program writes one or more files for its output? How do you tell HTCondor to bring them back?  Let\u2019s start by exploring what happens to files that a job creates in the sandbox. We will use a very simple method for creating a new file: We will copy an input file to another name.   Find or create a small input file (it is fine to use any small file from a previous exercise).  Create a submit file that transfers the input file and copies it to another name (as if doing  /bin/cp input.txt output.txt  on the command line)  Make the output filename different than any filenames that are in your submit directory  What is the  executable  line?  What is the  arguments  line?  How do you tell HTCondor to transfer the input file?  As always, use  output ,  error , and  log  filenames that are different from previous exercises    Submit the job and wait for it to finish.   What happened? Can you tell what HTCondor did with the output file that was created (did it end up back on the submit server?), after it was created in the job sandbox? Look carefully at the list of files in your submit directory now.",
            "title": "Transferring Output Files"
        },
        {
            "location": "/materials/day1/part2-ex1-files/#transferring-specific-output-files",
            "text": "As you saw in the last exercise, by default HTCondor transfers files that are created in the job sandbox back to the submit directory when the job finishes. In fact, HTCondor will also transfer back  changed  input files, too. But, this only works for files that are in the top-level sandbox directory, and  not  for ones contained in subdirectories.  What if you want to bring back only  some  output files, or output files contained in subdirectories?  Here is a simple shell script that creates several files, including a copy of an input file in a new subdirectory:  #!/bin/sh\nif [ $# -ne 1 ]; then echo \"Usage: $0 INPUT\"; exit 1; fi\ndate > output-timestamp.txt\ncal > output-calendar.txt\nmkdir subdirectory\ncp $1 subdirectory/backup-$1  First, let\u2019s confirm that HTCondor does not bring back the output file in the subdirectory:   Save the shell script in a file named  output.sh .  Write a submit file that transfers an input file and runs  output.sh  on it (passing the filename as an argument).  Submit the job, wait for it to finish, and examine the contents of your submit directory.   Suppose you decide that you want only the timestamp output file and all files in the subdirectory, but not the calendar output file. You can tell HTCondor to transfer these specific files:  transfer_output_files = output-timestamp.txt, subdirectory/   Note  See the trailing slash ( / ) on the subdirectory?\nThat tells HTCondor to transfer back  the files  contained in the subdirectory, but not the directory itself;\nthe files will be written directly into the submit directory.\nIf you want HTCondor to transfer back an entire directory, leave off the trailing slash.    Remove all output files from the previous run, including  output-timestamp.txt  and  output-calendar.txt .  Copy the previous submit file that ran  output.sh  and add the  transfer_output_files  line from above.  Submit the job, wait for it to finish, and examine the contents of your submit directory.   Did it work as you expected?",
            "title": "Transferring Specific Output Files"
        },
        {
            "location": "/materials/day1/part2-ex1-files/#thinking-about-progress-so-far",
            "text": "At this point, you can do just about everything that you need in order to run jobs on a local HTC pool. You can identify the executable, arguments, and input files, and you can get output back from the job. This is a big achievement!  In some ways, everything after this exercise shows you how to submit multiple jobs at once and makes it easier to run certain kinds of jobs and deal with certain kinds of situations.",
            "title": "Thinking About Progress So Far"
        },
        {
            "location": "/materials/day1/part2-ex1-files/#references",
            "text": "There are many more details about HTCondor\u2019s file transfer mechanism not covered here. For more information, read the  \"Submitting Jobs Without a Shared Filesystem\"  of the HTCondor Manual.",
            "title": "References"
        },
        {
            "location": "/materials/day1/part2-ex2-queue-n/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 2.2: Use queue \nN\n, $(Cluster), and $(Process)\n\u00b6\n\n\nThe goal of the next several exercises is to learn to submit many jobs from a single \nqueue\n statement, and then to control filenames and arguments per job.\n\n\nSuppose you have a program that you want to run many times with different arguments each time. With what you know so far, you have a couple of choices:\n\n\n\n\nWrite one submit file; submit one job, change the argument in the submit file, submit another job, change the submit file, \u2026\n\n\nWrite many submit files that are nearly identical except for the program argument\n\n\n\n\nNeither of these options seems very satisfying. Fortunately, we can do better with HTCondor.\n\n\nRunning Many Jobs With One queue Statement\n\u00b6\n\n\nHere is a C program that uses a simple stochastic (random) method to estimate the value of \u03c0\u00a0\u2014 feel free to try to figure out the method from the code, but it is not critical for this exercise. The single argument to the program is the number of samples to take. More samples should result in better estimates!\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/time.h>\n\nint main(int argc, char *argv[])\n{\n  struct timeval my_timeval;\n  int iterations = 0;\n  int inside_circle = 0;\n  int i;\n  double x, y, pi_estimate;\n\n  gettimeofday(&my_timeval, NULL);\n  srand48(my_timeval.tv_sec ^ my_timeval.tv_usec);\n\n  if (argc == 2) {\n    iterations = atoi(argv[1]);\n  } else {\n    printf(\"usage: circlepi ITERATIONS\\n\");\n    exit(1);\n  }\n\n  for (i = 0; i < iterations; i++) {\n    x = (drand48() - 0.5) * 2.0;\n    y = (drand48() - 0.5) * 2.0;\n    if (((x * x) + (y * y)) <= 1.0) {\n      inside_circle++;\n    }\n  }\n  pi_estimate = 4.0 * ((double) inside_circle / (double) iterations);\n  printf(\"%d iterations, %d inside; pi = %f\\n\", iterations, inside_circle, pi_estimate);\n  return 0;\n}\n\n\n\n\n\n\n\nIn a new directory for this exercise, save the code to a file named \ncirclepi.c\n\n\n\n\nCompile the code (we will cover this in more detail Wednesday):\n\n\nusername@learn $\n gcc -static -o circlepi circlepi.c\n\n\n\n\n\n\n\n\n\nTest the program with just 1000 samples:\n\n\nusername@learn $\n ./circlepi \n1000\n\n\n\n\n\n\n\n\n\n\nNow suppose that you want to run the program many times, to produce many estimates. This is exactly what a statement like \nqueue 3\n is useful for. Let\u2019s see how it works:\n\n\n\n\nWrite a normal submit file for this program\n\n\nPass 1 million (\n1000000\n) as the command line argument to \ncirclepi\n\n\nRemember to use \nqueue 3\n instead of just \nqueue\n\n\n\n\n\n\n\n\nSubmit the file. Note the slightly different message from \ncondor_submit\n:\n\n\n3 job(s) submitted to cluster *NNNN*.\n\n\n\n\n\n\n\n\n\n\nBefore the jobs execute, look at the job queue to see the multiple jobs\n\n\n\n\n\n\nHere is some sample \ncondor_q -nobatch\n output:\n\n\n ID       OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD\n\n\n10228.0   cat             7/25 11:57   0+00:00:00 I  0    0.7 circlepi 1000000000\n\n\n10228.1   cat             7/25 11:57   0+00:00:00 I  0    0.7 circlepi 1000000000\n\n\n10228.2   cat             7/25 11:57   0+00:00:00 I  0    0.7 circlepi 1000000000\n\n\n\n\n\n\nIn this sample, all three jobs are part of \ncluster\n \n10228\n, but the first job was assigned \nprocess\n \n0\n, the second job was assigned process \n1\n, and the third one was assigned process \n2\n. (Historical note: Programmers like to start counting from 0, hence the odd numbering scheme.)\n\n\nAt this time, it is worth reviewing the definition of a \njob ID\n. It is a job\u2019s cluster number, a dot (\n.\n), and the job\u2019s process number. So in the example above, the job ID of the second job is \n10228.1\n.\n\n\nPop Quiz:\n Do you remember how to ask HTCondor to list all of the jobs from one cluster? How about one specific job ID?\n\n\nUsing queue \nN\n With Output\n\u00b6\n\n\nWhen all three jobs in your single cluster are finished, examine the resulting files.\n\n\n\n\nWhat is in the output file?\n\n\nWhat is in the error file (hopefully nothing)?\n\n\nWhat is in the log file? Look carefully at the job IDs in each event.\n\n\nIs this what you expected? Is it what you wanted?\n\n\n\n\nUsing $(Process) to Distinguish Jobs\n\u00b6\n\n\nAs you saw with the experiment above, each job ended up overwriting the same output and error filenames in the submission directory.\nAfter all, we didn't tell it to behave any differently when it ran three jobs.\nWe need a way to separate output (and error) files \nper job that is queued\n, not just for the whole cluster of jobs. Fortunately, HTCondor has a way to separate the files easily.\n\n\nWhen processing a submit file, HTCondor will replace any instance of \n$(Process)\n with the process number of the job, for each job that is queued. \nFor example, you can use the \n$(Process)\n variable to define a separate output file name for each job:\n\n\noutput = my-output-file-$(Process).out\nqueue 10\n\n\n\n\n\nEven though the \noutput\n filename is defined only once, HTCondor will create separate output filenames for each job:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst job\n\n\nmy-output-file-0.out\n\n\n\n\n\n\nSecond job\n\n\nmy-output-file-1.out\n\n\n\n\n\n\nThird job\n\n\nmy-output-file-2.out\n\n\n\n\n\n\n...\n\n\n\n\n\n\n\n\nLast (tenth) job\n\n\nmy-output-file-9.out\n\n\n\n\n\n\n\n\nLet\u2019s see how this works for our program that estimates \u03c0.\n\n\n\n\nIn your submit file, change the definitions of \noutput\n and \nerror\n to use \n$(Process)\n in the filename, similar to the example above.\n\n\nDelete any output, error, and log files from previous runs.\n\n\nSubmit the updated file.\n\n\n\n\nWhen all three jobs are finished, examine the resulting files again.\n\n\n\n\nHow many files are there of each type? What are their names?\n\n\nIs this what you expected? Is it what you wanted from the \u03c0 estimation process?\n\n\n\n\nUsing $(Cluster) to Separate Files Across Runs\n\u00b6\n\n\nWith \n$(Process)\n, you can get separate output (and error) filenames for each job within a run. However, the next time you submit the same file, all of the output and error files are overwritten by new ones created by the new jobs. Maybe this is the behavior that you want. But sometimes, you may want to separate files by run, as well.\n\n\nIn addition to \n$(Process)\n, there is also a \n$(Cluster)\n variable that you can use in your submit files. It works just like \n$(Process)\n, except it is replaced with the cluster number of the entire submission. Because the cluster number is the same for all jobs within a single submission, it does not separate files by job within a submission. But when used \nwith\n \n$(Process)\n, it can be used to separate files by run. For example, consider this \noutput\n statement:\n\n\noutput = my-output-file-$(Cluster)-$(Process).out\n\n\n\n\n\nFor one particular run, it might result in output filenames like \nmy-output-file-2444-0.out\n, \nmyoutput-file-2444-1.out\n, \nmyoutput-file-2444-2.out\n, etc.\n\n\nHowever, the next run would have different filenames, replacing \n2444\n with the new Cluster number of that run.\n\n\nUsing $(Process) and $(Cluster) in Other Statements\n\u00b6\n\n\nThe \n$(Cluster)\n and \n$(Process)\n variables can be used in any submit file statement, although they are useful in some kinds of submit file statements and not really for others. For example, consider using $(Cluster) or $(Process) in each of the below:\n\n\n\n\nlog\n\n\ntransfer_input_files\n\n\ntransfer_output_files\n\n\narguments\n\n\n\n\nUnfortunately, HTCondor does not easily let you perform math on the \n$(Process)\n number when using it. So, for example, if you use \n$(Process)\n as a numeric argument to a command, it will always result in jobs getting the arguments 0, 1, 2, and so on. If you have control over your program and the way in which it uses command-line arguments, then you are fine. Otherwise, you might need a solution like those in the next exercises.\n\n\n(Optional) Defining JobBatchName for Tracking\n\u00b6\n\n\nDuring the lecture, it was mentioned that you can define arbitrary attributes in your submit file, and that one purpose of such attributes is to track or report on different jobs separately. In this optional exercise, you will see how this technique can be used.\n\n\nOnce again, we will use \nsleep\n jobs, so that your jobs remain in the queue long enough to experiment on.\n\n\n\n\nCreate a basic submit file that runs \nsleep 120\n (or some reasonable duration).\n\n\n\n\nInstead of a single \nqueue\n statement, write this:\n\n\njobbatchname = 1\nqueue 5\n\n\n\n\n\n\n\n\n\nSubmit the file.\n\n\n\n\n\n\nNow, quickly edit the submit file to instead say:\n\n\njobbatchname = 2\n\n\n\n\n\n\n\n\n\nSubmit the file again.\n\n\n\n\n\n\nCheck on the submissions using a normal \ncondor_q\n and \ncondor_q -nobatch\n. Of course, your special attribute does not appear in the \ncondor_q -nobatch\n output, but it is present in the \ncondor_q\n output and in each job\u2019s ClassAd. You can see the effect of the attribute by limiting your \ncondor_q\n output to one type of job or another. First, run this command:\n\n\nusername@learn $\n condor_q -constraint \n'JobBatchName == \"1\"'\n\n\n\n\n\n\nDo you get the output that you expected? Using the example command above, how would you list your other five jobs?\n(More on constraints in other exercises later today.)",
            "title": "Exercise 2.2"
        },
        {
            "location": "/materials/day1/part2-ex2-queue-n/#monday-exercise-22-use-queue-n-cluster-and-process",
            "text": "The goal of the next several exercises is to learn to submit many jobs from a single  queue  statement, and then to control filenames and arguments per job.  Suppose you have a program that you want to run many times with different arguments each time. With what you know so far, you have a couple of choices:   Write one submit file; submit one job, change the argument in the submit file, submit another job, change the submit file, \u2026  Write many submit files that are nearly identical except for the program argument   Neither of these options seems very satisfying. Fortunately, we can do better with HTCondor.",
            "title": "Monday Exercise 2.2: Use queue N, $(Cluster), and $(Process)"
        },
        {
            "location": "/materials/day1/part2-ex2-queue-n/#running-many-jobs-with-one-queue-statement",
            "text": "Here is a C program that uses a simple stochastic (random) method to estimate the value of \u03c0\u00a0\u2014 feel free to try to figure out the method from the code, but it is not critical for this exercise. The single argument to the program is the number of samples to take. More samples should result in better estimates!  #include <stdio.h>\n#include <stdlib.h>\n#include <sys/time.h>\n\nint main(int argc, char *argv[])\n{\n  struct timeval my_timeval;\n  int iterations = 0;\n  int inside_circle = 0;\n  int i;\n  double x, y, pi_estimate;\n\n  gettimeofday(&my_timeval, NULL);\n  srand48(my_timeval.tv_sec ^ my_timeval.tv_usec);\n\n  if (argc == 2) {\n    iterations = atoi(argv[1]);\n  } else {\n    printf(\"usage: circlepi ITERATIONS\\n\");\n    exit(1);\n  }\n\n  for (i = 0; i < iterations; i++) {\n    x = (drand48() - 0.5) * 2.0;\n    y = (drand48() - 0.5) * 2.0;\n    if (((x * x) + (y * y)) <= 1.0) {\n      inside_circle++;\n    }\n  }\n  pi_estimate = 4.0 * ((double) inside_circle / (double) iterations);\n  printf(\"%d iterations, %d inside; pi = %f\\n\", iterations, inside_circle, pi_estimate);\n  return 0;\n}   In a new directory for this exercise, save the code to a file named  circlepi.c   Compile the code (we will cover this in more detail Wednesday):  username@learn $  gcc -static -o circlepi circlepi.c    Test the program with just 1000 samples:  username@learn $  ./circlepi  1000     Now suppose that you want to run the program many times, to produce many estimates. This is exactly what a statement like  queue 3  is useful for. Let\u2019s see how it works:   Write a normal submit file for this program  Pass 1 million ( 1000000 ) as the command line argument to  circlepi  Remember to use  queue 3  instead of just  queue     Submit the file. Note the slightly different message from  condor_submit :  3 job(s) submitted to cluster *NNNN*.     Before the jobs execute, look at the job queue to see the multiple jobs    Here is some sample  condor_q -nobatch  output:   ID       OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD  10228.0   cat             7/25 11:57   0+00:00:00 I  0    0.7 circlepi 1000000000  10228.1   cat             7/25 11:57   0+00:00:00 I  0    0.7 circlepi 1000000000  10228.2   cat             7/25 11:57   0+00:00:00 I  0    0.7 circlepi 1000000000   In this sample, all three jobs are part of  cluster   10228 , but the first job was assigned  process   0 , the second job was assigned process  1 , and the third one was assigned process  2 . (Historical note: Programmers like to start counting from 0, hence the odd numbering scheme.)  At this time, it is worth reviewing the definition of a  job ID . It is a job\u2019s cluster number, a dot ( . ), and the job\u2019s process number. So in the example above, the job ID of the second job is  10228.1 .  Pop Quiz:  Do you remember how to ask HTCondor to list all of the jobs from one cluster? How about one specific job ID?",
            "title": "Running Many Jobs With One queue Statement"
        },
        {
            "location": "/materials/day1/part2-ex2-queue-n/#using-queue-n-with-output",
            "text": "When all three jobs in your single cluster are finished, examine the resulting files.   What is in the output file?  What is in the error file (hopefully nothing)?  What is in the log file? Look carefully at the job IDs in each event.  Is this what you expected? Is it what you wanted?",
            "title": "Using queue N With Output"
        },
        {
            "location": "/materials/day1/part2-ex2-queue-n/#using-process-to-distinguish-jobs",
            "text": "As you saw with the experiment above, each job ended up overwriting the same output and error filenames in the submission directory.\nAfter all, we didn't tell it to behave any differently when it ran three jobs.\nWe need a way to separate output (and error) files  per job that is queued , not just for the whole cluster of jobs. Fortunately, HTCondor has a way to separate the files easily.  When processing a submit file, HTCondor will replace any instance of  $(Process)  with the process number of the job, for each job that is queued. \nFor example, you can use the  $(Process)  variable to define a separate output file name for each job:  output = my-output-file-$(Process).out\nqueue 10  Even though the  output  filename is defined only once, HTCondor will create separate output filenames for each job:           First job  my-output-file-0.out    Second job  my-output-file-1.out    Third job  my-output-file-2.out    ...     Last (tenth) job  my-output-file-9.out     Let\u2019s see how this works for our program that estimates \u03c0.   In your submit file, change the definitions of  output  and  error  to use  $(Process)  in the filename, similar to the example above.  Delete any output, error, and log files from previous runs.  Submit the updated file.   When all three jobs are finished, examine the resulting files again.   How many files are there of each type? What are their names?  Is this what you expected? Is it what you wanted from the \u03c0 estimation process?",
            "title": "Using $(Process) to Distinguish Jobs"
        },
        {
            "location": "/materials/day1/part2-ex2-queue-n/#using-cluster-to-separate-files-across-runs",
            "text": "With  $(Process) , you can get separate output (and error) filenames for each job within a run. However, the next time you submit the same file, all of the output and error files are overwritten by new ones created by the new jobs. Maybe this is the behavior that you want. But sometimes, you may want to separate files by run, as well.  In addition to  $(Process) , there is also a  $(Cluster)  variable that you can use in your submit files. It works just like  $(Process) , except it is replaced with the cluster number of the entire submission. Because the cluster number is the same for all jobs within a single submission, it does not separate files by job within a submission. But when used  with   $(Process) , it can be used to separate files by run. For example, consider this  output  statement:  output = my-output-file-$(Cluster)-$(Process).out  For one particular run, it might result in output filenames like  my-output-file-2444-0.out ,  myoutput-file-2444-1.out ,  myoutput-file-2444-2.out , etc.  However, the next run would have different filenames, replacing  2444  with the new Cluster number of that run.",
            "title": "Using $(Cluster) to Separate Files Across Runs"
        },
        {
            "location": "/materials/day1/part2-ex2-queue-n/#using-process-and-cluster-in-other-statements",
            "text": "The  $(Cluster)  and  $(Process)  variables can be used in any submit file statement, although they are useful in some kinds of submit file statements and not really for others. For example, consider using $(Cluster) or $(Process) in each of the below:   log  transfer_input_files  transfer_output_files  arguments   Unfortunately, HTCondor does not easily let you perform math on the  $(Process)  number when using it. So, for example, if you use  $(Process)  as a numeric argument to a command, it will always result in jobs getting the arguments 0, 1, 2, and so on. If you have control over your program and the way in which it uses command-line arguments, then you are fine. Otherwise, you might need a solution like those in the next exercises.",
            "title": "Using $(Process) and $(Cluster) in Other Statements"
        },
        {
            "location": "/materials/day1/part2-ex2-queue-n/#optional-defining-jobbatchname-for-tracking",
            "text": "During the lecture, it was mentioned that you can define arbitrary attributes in your submit file, and that one purpose of such attributes is to track or report on different jobs separately. In this optional exercise, you will see how this technique can be used.  Once again, we will use  sleep  jobs, so that your jobs remain in the queue long enough to experiment on.   Create a basic submit file that runs  sleep 120  (or some reasonable duration).   Instead of a single  queue  statement, write this:  jobbatchname = 1\nqueue 5    Submit the file.    Now, quickly edit the submit file to instead say:  jobbatchname = 2    Submit the file again.    Check on the submissions using a normal  condor_q  and  condor_q -nobatch . Of course, your special attribute does not appear in the  condor_q -nobatch  output, but it is present in the  condor_q  output and in each job\u2019s ClassAd. You can see the effect of the attribute by limiting your  condor_q  output to one type of job or another. First, run this command:  username@learn $  condor_q -constraint  'JobBatchName == \"1\"'   Do you get the output that you expected? Using the example command above, how would you list your other five jobs?\n(More on constraints in other exercises later today.)",
            "title": "(Optional) Defining JobBatchName for Tracking"
        },
        {
            "location": "/materials/day1/part2-ex3-queue-matching/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 2.3: Submit With \u201cqueue matching\u201d\n\u00b6\n\n\nIn this exercise and the next one, you will explore more ways to use a single submit file to submit many jobs. The focus of this exercise is to submit one job per filename that matches a given pattern.\n\n\nIn all cases of submitting many jobs from a single submit file, the key questions are:\n\n\n\n\nWhat makes each job unique? In other words, there is one job per _____?\n\n\nSo, how should you tell HTCondor to distinguish each job?\n\n\n\n\nFor \nqueue *N*\n, jobs are distinguished simply by the built-in \"process\" variable. But with the remaining \nqueue\n forms, you help HTCondor distinguish jobs by other, more meaningful \ncustom\n variables.\n\n\nCounting Words in Files\n\u00b6\n\n\nSuppose you have a collection of books, and you want to analyze how words vary from book to book or author to author. As mentioned in the lecture, HTCondor provides many ways to do this task. You could create a separate submit file for each book, and submit all of the files manually, but you'd have a lot of file lines to modify each time (in particular, all five of the last lines before \nqueue\n below):\n\n\nexecutable              = freq.py\nrequest_memory          = 1GB\nrequest_disk            = 20MB\nshould_transfer_files   = YES\nwhen_to_transfer_output = ON_EXIT\n\ntransfer_input_files = AAiW.txt\narguments            = AAiW.txt\noutput               = AAiW.out\nerror                = AAiW.err\nlog                  = AAiW.log\nqueue\n\n\n\n\n\nQueue Jobs By Matching Filenames\n\u00b6\n\n\nFor our analysis, we will have a new version of the word-frequency counting script. It takes a single command-line argument, which is the name of the input file containing the text of a book, and it outputs the frequency of each word from least to most common. There will be several book files, and the filename for each book ends with \n.txt\n.\n\n\nThis is an example of a common scenario: We want to run one job per file, where the filenames match a certain consistent pattern. The \nqueue ... matching\n statement is made for this scenario.\n\n\nLet\u2019s see this in action. First, here is the new version of the script:\n\n\n#!/usr/bin/env python\n\nimport os\nimport sys\nimport operator\n\nif len(sys.argv) != 2:\n    print 'Usage: %s DATA' % (os.path.basename(sys.argv[0]))\n    sys.exit(1)\ninput_filename = sys.argv[1]\n\nwords = {}\n\nmy_file = open(input_filename, 'r')\nfor line in my_file:\n    line_words = line.split()\n    for word in line_words:\n        if word in words:\n            words[word] += 1\n        else:\n            words[word] = 1\nmy_file.close()\n\nsorted_words = sorted(words.items(), key=operator.itemgetter(1))\nfor word in sorted_words:\n    print '%s %8d' % (word[0], word[1])\n\n\n\n\n\nTo use the script:\n\n\n\n\nSave it as \nwordcount.py\n.\n\n\n\n\nDownload and unpack some books from Project Gutenberg:\n\n\nusername@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/books.zip\n\nusername@learn $\n unzip books.zip\n\n\n\n\n\n\n\n\n\nVerify the script by running it on one book manually.\n\n\n\n\nCreate a submit file to submit one job (pick a book file and model your submit file off of the one above), including memory and disk requests of 20\u00a0MB; submit it, if you like.\n\n\n\n\nModify the following submit file statements to work for all books:\n\n\ntransfer_input_files = $(BOOK) \narguments = $(book) \noutput = $(book).out \nerror = $(book).err \nqueue book matching *.txt\n\n\n\n\n\n\n\nNote\n\n\nAs always, the order of statements in a submit file does not matter, except that the \nqueue\n statement should be last. Also note that any submit file variable name (here, \nbook\n, but true for \nprocess\n and all others) may be used in any mixture of upper- and lowercase letters.\n\n\n\n\n\n\n\n\nSubmit the jobs.\n\n\n\n\n\n\nHTCondor uses the \nqueue ... matching\n statement to look for files in the submit directory that match the given pattern, then queues one job per match. For each job, the given variable (e.g., \nbook\n here) is assigned the name of the matching file, so that it can be used in \noutput\n, \nerror\n, and other statements.\n\n\nThe result is the same as if we had written out a much longer submit file:\n\n\n...\n\ntransfer_input_files = AAiW.txt\narguments = \"AAiW.txt\"\noutput = AAiW.txt.out\nerror = AAiW.txt.err\nqueue\n\ntransfer_input_files = PandP.txt\narguments = \"PandP.txt\"\noutput = PandP.txt.out\nerror = PandP.txt.err\nqueue\n\ntransfer_input_files = TAoSH.txt\narguments = \"TAoSH.txt\"\noutput = TAoSH.txt.out\nerror = TAoSH.txt.err\nqueue\n\n\n\n\n\nHere is some example \ncondor_q -nobatch\n output:\n\n\n ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD\n\n\n  89.0   iaross          7/17 11:41   0+00:00:00 I  0    0.0 wordcount.py AAiW.txt\n\n\n  89.1   iaross          7/17 11:41   0+00:00:00 I  0    0.0 wordcount.py PandP.txt\n\n\n  89.2   iaross          7/17 11:41   0+00:00:00 I  0    0.0 wordcount.py TAoSH.txt\n\n\n\n\n\n\nAll three jobs were part of cluster 89. The first filename that was matched in the queue statement resulted in a process ID of 0, the second match has a process ID of 1, and the third has a process ID of 2.\n\n\nWhen the three jobs finish, carefully look at the resulting files. Do they match your expectations? There should be a single log file, but three separate output files and three separate (and hopefully empty) error files, one for each job.\n\n\nExtra Challenge\n\u00b6\n\n\nIn the example above, you used a single log file for all three jobs. HTCondor handles this situation with no problem; each job writes its events into the log file without getting in the way of other events and other jobs. But as you may have seen, it may be difficult for a person to understand the events for any particular job in the combined log file.\n\n\nCreate a new submit file that works just like the one above, except that each job writes its own log file.",
            "title": "Exercise 2.3"
        },
        {
            "location": "/materials/day1/part2-ex3-queue-matching/#monday-exercise-23-submit-with-queue-matching",
            "text": "In this exercise and the next one, you will explore more ways to use a single submit file to submit many jobs. The focus of this exercise is to submit one job per filename that matches a given pattern.  In all cases of submitting many jobs from a single submit file, the key questions are:   What makes each job unique? In other words, there is one job per _____?  So, how should you tell HTCondor to distinguish each job?   For  queue *N* , jobs are distinguished simply by the built-in \"process\" variable. But with the remaining  queue  forms, you help HTCondor distinguish jobs by other, more meaningful  custom  variables.",
            "title": "Monday Exercise 2.3: Submit With \u201cqueue matching\u201d"
        },
        {
            "location": "/materials/day1/part2-ex3-queue-matching/#counting-words-in-files",
            "text": "Suppose you have a collection of books, and you want to analyze how words vary from book to book or author to author. As mentioned in the lecture, HTCondor provides many ways to do this task. You could create a separate submit file for each book, and submit all of the files manually, but you'd have a lot of file lines to modify each time (in particular, all five of the last lines before  queue  below):  executable              = freq.py\nrequest_memory          = 1GB\nrequest_disk            = 20MB\nshould_transfer_files   = YES\nwhen_to_transfer_output = ON_EXIT\n\ntransfer_input_files = AAiW.txt\narguments            = AAiW.txt\noutput               = AAiW.out\nerror                = AAiW.err\nlog                  = AAiW.log\nqueue",
            "title": "Counting Words in Files"
        },
        {
            "location": "/materials/day1/part2-ex3-queue-matching/#queue-jobs-by-matching-filenames",
            "text": "For our analysis, we will have a new version of the word-frequency counting script. It takes a single command-line argument, which is the name of the input file containing the text of a book, and it outputs the frequency of each word from least to most common. There will be several book files, and the filename for each book ends with  .txt .  This is an example of a common scenario: We want to run one job per file, where the filenames match a certain consistent pattern. The  queue ... matching  statement is made for this scenario.  Let\u2019s see this in action. First, here is the new version of the script:  #!/usr/bin/env python\n\nimport os\nimport sys\nimport operator\n\nif len(sys.argv) != 2:\n    print 'Usage: %s DATA' % (os.path.basename(sys.argv[0]))\n    sys.exit(1)\ninput_filename = sys.argv[1]\n\nwords = {}\n\nmy_file = open(input_filename, 'r')\nfor line in my_file:\n    line_words = line.split()\n    for word in line_words:\n        if word in words:\n            words[word] += 1\n        else:\n            words[word] = 1\nmy_file.close()\n\nsorted_words = sorted(words.items(), key=operator.itemgetter(1))\nfor word in sorted_words:\n    print '%s %8d' % (word[0], word[1])  To use the script:   Save it as  wordcount.py .   Download and unpack some books from Project Gutenberg:  username@learn $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/books.zip username@learn $  unzip books.zip    Verify the script by running it on one book manually.   Create a submit file to submit one job (pick a book file and model your submit file off of the one above), including memory and disk requests of 20\u00a0MB; submit it, if you like.   Modify the following submit file statements to work for all books:  transfer_input_files = $(BOOK) \narguments = $(book) \noutput = $(book).out \nerror = $(book).err \nqueue book matching *.txt   Note  As always, the order of statements in a submit file does not matter, except that the  queue  statement should be last. Also note that any submit file variable name (here,  book , but true for  process  and all others) may be used in any mixture of upper- and lowercase letters.     Submit the jobs.    HTCondor uses the  queue ... matching  statement to look for files in the submit directory that match the given pattern, then queues one job per match. For each job, the given variable (e.g.,  book  here) is assigned the name of the matching file, so that it can be used in  output ,  error , and other statements.  The result is the same as if we had written out a much longer submit file:  ...\n\ntransfer_input_files = AAiW.txt\narguments = \"AAiW.txt\"\noutput = AAiW.txt.out\nerror = AAiW.txt.err\nqueue\n\ntransfer_input_files = PandP.txt\narguments = \"PandP.txt\"\noutput = PandP.txt.out\nerror = PandP.txt.err\nqueue\n\ntransfer_input_files = TAoSH.txt\narguments = \"TAoSH.txt\"\noutput = TAoSH.txt.out\nerror = TAoSH.txt.err\nqueue  Here is some example  condor_q -nobatch  output:   ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD    89.0   iaross          7/17 11:41   0+00:00:00 I  0    0.0 wordcount.py AAiW.txt    89.1   iaross          7/17 11:41   0+00:00:00 I  0    0.0 wordcount.py PandP.txt    89.2   iaross          7/17 11:41   0+00:00:00 I  0    0.0 wordcount.py TAoSH.txt   All three jobs were part of cluster 89. The first filename that was matched in the queue statement resulted in a process ID of 0, the second match has a process ID of 1, and the third has a process ID of 2.  When the three jobs finish, carefully look at the resulting files. Do they match your expectations? There should be a single log file, but three separate output files and three separate (and hopefully empty) error files, one for each job.",
            "title": "Queue Jobs By Matching Filenames"
        },
        {
            "location": "/materials/day1/part2-ex3-queue-matching/#extra-challenge",
            "text": "In the example above, you used a single log file for all three jobs. HTCondor handles this situation with no problem; each job writes its events into the log file without getting in the way of other events and other jobs. But as you may have seen, it may be difficult for a person to understand the events for any particular job in the combined log file.  Create a new submit file that works just like the one above, except that each job writes its own log file.",
            "title": "Extra Challenge"
        },
        {
            "location": "/materials/day1/part2-ex4-queue-from/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 2.4: Submit with \u201cqueue from\u201d\n\u00b6\n\n\nThe goal of this exercise is to submit many jobs from a single submit file by using the \nqueue ... from\n syntax to read variable values from a file.\n\n\nMore Job Submission Alternatives\n\u00b6\n\n\nIn the previous exercise, we used the \nqueue ... matching\n syntax, which is primarily useful for collections of files with similar names. But that method has its weaknesses, too. \nIt is less useful when the values for different job conditions are \nnot\n filenames, or if you have other filenames with a similar naming pattern that should \nnot\n be used for jobs.\n\n\nQueue Jobs From a List of Values\n\u00b6\n\n\nSuppose we want to modify our word-frequency analysis a little bit so that it outputs only the most common \nN\n words of a document. Further, we want to experiment with different values of \nN\n. And finally, even though we downloaded it, we no longer want to analyze the Sherlock Holmes stories in the \nTAoSH.txt\n file. Clearly, \nqueue ... matching\n will not help us in this case.\n\n\nFirst, we need a new version of the word counting program so that it accepts an extra number as a command line argument and outputs only that many of the most common words. Here is the new code (it's still not important that you understand this code):\n\n\n#!/usr/bin/env python\n\nimport os\nimport sys\nimport operator\n\nif len(sys.argv) != 3:\n    print 'Usage: %s DATA NUM_WORDS' % (os.path.basename(sys.argv[0]))\n    sys.exit(1)\ninput_filename = sys.argv[1]\nnum_words = int(sys.argv[2])\n\nwords = {}\n\nmy_file = open(input_filename, 'r')\nfor line in my_file:\n    line_words = line.split()\n    for word in line_words:\n        if word in words:\n            words[word] += 1\n        else:\n            words[word] = 1\nmy_file.close()\n\nsorted_words = sorted(words.items(), key=operator.itemgetter(1))\nfor word in sorted_words[-num_words:]:\n    print '%s %8d' % (word[0], word[1])\n\n\n\n\n\nTo submit this program with a collection of two variable values for each run, one for the number of top words and one for the filename:\n\n\n\n\nIn the same directory as the last exercise, save the script as \nwordcount-top-n.py\n.\n\n\nCopy your submit file from the last exercise to a new name (maybe \nwordcount-top.sub\n).\n\n\nUpdate the \nexecutable\n and \nlog\n statements as appropriate.\n\n\n\n\nUpdate other statements to work with two variables, \nbook\n and \nn\n:\n\n\noutput = $(book)_top_$(n).out \nerror = $(book)_top_$(n).err \ntransfer_input_files = $(book) \narguments = \"$(book) $(n)\" \nqueue book,n from books_n.txt\n\n\n\n\n\nNote especially the changes to the \nqueue\n statement; it now tells HTCondor to read a separate text file of \npairs\n of values, which will be assigned to \nbook\n and \nn\n respectively.\n\n\n\n\n\n\nCreate the separate text file of job variable values and save it as \nbooks_n.txt\n:\n\n\nAAiW.txt, 10 \nAAiW.txt, 25 \nAAiW.txt, 50 \nPandP.txt, 10 \nPandP.txt, 25 \nPandP.txt, 50\n\n\n\n\n\nNote that we used 3 different values for \nn\n for each book, and that we dropped the Sherlock Holmes book, \nTAoSH.txt\n.\n\n\n\n\n\n\nSubmit the file\n\n\n\n\nDo a quick sanity check: How many jobs were submitted? How many log, output, and error files were created?\n\n\n\n\nExtra Challenge 1\n\u00b6\n\n\nBetween this exercise and the previous one, you have explored two of the three primary \nqueue\n statements. How would you use the \nqueue in ... list\n statement to accomplish the same thing(s) as one or both of the exercises?\n\n\nExtra Challenge 2\n\u00b6\n\n\nYou may have noticed that the output of these jobs has a messy naming convention. Because our macros resolve to the filenames, including their extension (e.g., \nAAiW.txt\n), the output filenames contain with multiple extensions (e.g., \nAAiW.txt.err\n). Although the extra extension is acceptable, it makes the filenames harder to read and possibly organize. \nChange your submit file and variable file for this exercise so that the output filenames do not include the \n.txt\n extension.",
            "title": "Exercise 2.4"
        },
        {
            "location": "/materials/day1/part2-ex4-queue-from/#monday-exercise-24-submit-with-queue-from",
            "text": "The goal of this exercise is to submit many jobs from a single submit file by using the  queue ... from  syntax to read variable values from a file.",
            "title": "Monday Exercise 2.4: Submit with \u201cqueue from\u201d"
        },
        {
            "location": "/materials/day1/part2-ex4-queue-from/#more-job-submission-alternatives",
            "text": "In the previous exercise, we used the  queue ... matching  syntax, which is primarily useful for collections of files with similar names. But that method has its weaknesses, too. \nIt is less useful when the values for different job conditions are  not  filenames, or if you have other filenames with a similar naming pattern that should  not  be used for jobs.",
            "title": "More Job Submission Alternatives"
        },
        {
            "location": "/materials/day1/part2-ex4-queue-from/#queue-jobs-from-a-list-of-values",
            "text": "Suppose we want to modify our word-frequency analysis a little bit so that it outputs only the most common  N  words of a document. Further, we want to experiment with different values of  N . And finally, even though we downloaded it, we no longer want to analyze the Sherlock Holmes stories in the  TAoSH.txt  file. Clearly,  queue ... matching  will not help us in this case.  First, we need a new version of the word counting program so that it accepts an extra number as a command line argument and outputs only that many of the most common words. Here is the new code (it's still not important that you understand this code):  #!/usr/bin/env python\n\nimport os\nimport sys\nimport operator\n\nif len(sys.argv) != 3:\n    print 'Usage: %s DATA NUM_WORDS' % (os.path.basename(sys.argv[0]))\n    sys.exit(1)\ninput_filename = sys.argv[1]\nnum_words = int(sys.argv[2])\n\nwords = {}\n\nmy_file = open(input_filename, 'r')\nfor line in my_file:\n    line_words = line.split()\n    for word in line_words:\n        if word in words:\n            words[word] += 1\n        else:\n            words[word] = 1\nmy_file.close()\n\nsorted_words = sorted(words.items(), key=operator.itemgetter(1))\nfor word in sorted_words[-num_words:]:\n    print '%s %8d' % (word[0], word[1])  To submit this program with a collection of two variable values for each run, one for the number of top words and one for the filename:   In the same directory as the last exercise, save the script as  wordcount-top-n.py .  Copy your submit file from the last exercise to a new name (maybe  wordcount-top.sub ).  Update the  executable  and  log  statements as appropriate.   Update other statements to work with two variables,  book  and  n :  output = $(book)_top_$(n).out \nerror = $(book)_top_$(n).err \ntransfer_input_files = $(book) \narguments = \"$(book) $(n)\" \nqueue book,n from books_n.txt  Note especially the changes to the  queue  statement; it now tells HTCondor to read a separate text file of  pairs  of values, which will be assigned to  book  and  n  respectively.    Create the separate text file of job variable values and save it as  books_n.txt :  AAiW.txt, 10 \nAAiW.txt, 25 \nAAiW.txt, 50 \nPandP.txt, 10 \nPandP.txt, 25 \nPandP.txt, 50  Note that we used 3 different values for  n  for each book, and that we dropped the Sherlock Holmes book,  TAoSH.txt .    Submit the file   Do a quick sanity check: How many jobs were submitted? How many log, output, and error files were created?",
            "title": "Queue Jobs From a List of Values"
        },
        {
            "location": "/materials/day1/part2-ex4-queue-from/#extra-challenge-1",
            "text": "Between this exercise and the previous one, you have explored two of the three primary  queue  statements. How would you use the  queue in ... list  statement to accomplish the same thing(s) as one or both of the exercises?",
            "title": "Extra Challenge 1"
        },
        {
            "location": "/materials/day1/part2-ex4-queue-from/#extra-challenge-2",
            "text": "You may have noticed that the output of these jobs has a messy naming convention. Because our macros resolve to the filenames, including their extension (e.g.,  AAiW.txt ), the output filenames contain with multiple extensions (e.g.,  AAiW.txt.err ). Although the extra extension is acceptable, it makes the filenames harder to read and possibly organize.  Change your submit file and variable file for this exercise so that the output filenames do not include the  .txt  extension.",
            "title": "Extra Challenge 2"
        },
        {
            "location": "/materials/day1/part3-ex1-queue/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 3.1: Explore condor_q\n\u00b6\n\n\nThe goal of this exercise is try out some of the most common options to the \ncondor_q\n command, so that you can view jobs effectively.\n\n\nThe main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a \ncondor_q\n expert!\n\n\nSelecting Jobs\n\u00b6\n\n\nThe \ncondor_q\n program has many options for selecting which jobs are listed. You have already seen that the default mode (as of version 8.5) is to show only your jobs in \"batch\" mode:\n\n\nusername@learn $\n condor_q\n\n\n\n\n\nYou've seen that you can view all jobs (all users) in the submit node's queue by using the \n-all\n argument:\n\n\nusername@learn $\n condor_q -all\n\n\n\n\n\nAnd you've seen that you can view more details about queued jobs, with each separate job on a single line using the \n-nobatch\n option:\n\n\nusername@learn $\n condor_q -nobatch\n\nusername@learn $\n condor_q -all -nobatch\n\n\n\n\n\nDid you know you can also name one or more user IDs on the command line, in which case jobs for all of the named users are listed at once?\n\n\nusername@learn $\n condor_q \n<username1> <username2> <username3>\n\n\n\n\n\n\nThere are two other, simple selection criteria that you can use. To list just the jobs associated with a single cluster number:\n\n\nusername@learn $\n condor_q \n<CLUSTER>\n\n\n\n\n\n\nFor example, if you want to see the jobs in cluster 5678 (i.e., \n5678.0\n, \n5678.1\n, etc.), you use \ncondor_q 5678\n.\n\n\nTo list a specific job (i.e., cluster.process, as in 5678.0):\n\n\nusername@learn $\n condor_q \n<JOB.ID>\n\n\n\n\n\n\nFor example, to see job ID 5678.1, you use \ncondor_q 5678.1\n.\n\n\n\n\nNote\n\n\nYou can name more than one cluster, job ID, or combination thereof on the command line, in which case jobs for\n\nall\n of the named clusters and/or job IDs are listed.\n\n\n\n\nLet\u2019s get some practice using \ncondor_q\n selections!\n\n\n\n\nUsing a previous exercise, submit several \nsleep\n jobs.\n\n\nList all jobs in the queue \u2014 are there others besides your own?\n\n\nPractice using all forms of \ncondor_q\n that you have learned:\n\n\nList just your jobs, with and without batching.\n\n\nList a specific cluster.\n\n\nList a specific job ID.\n\n\nTry listing several users at once.\n\n\nTry listing several clusters and job IDs at once.\n\n\n\n\n\n\nWhen there are a variety of jobs in the queue, try combining a username and a different user's cluster or job ID in the same command \u2014 what happens?\n\n\n\n\nViewing a Job ClassAd\n\u00b6\n\n\nYou may have wondered why it is useful to be able to list a single job ID using \ncondor_q\n. By itself, it may not be that useful. But, in combination with another option, it is very useful!\n\n\nIf you add the \n-long\n option to \ncondor_q\n (or its short form, \n-l\n), it will show the complete ClassAd for each selected job, instead of the one-line summary that you have seen so far. Because job ClassAds may have 80\u201390 attributes (or more), it probably makes the most sense to show the ClassAd for a single job at a time. And you know how to show just one job! Here is what the command looks like:\n\n\nusername@learn $\n condor_q -long \n<JOB.ID>\n\n\n\n\n\n\nThe output from this command is long and complex. Most of the attributes that HTCondor adds to a job are arcane and uninteresting for us now. But here are some examples of common, interesting attributes taken directly from \ncondor_q\n output (except with some line breaks added to the \nRequirements\n attribute):\n\n\nMyType = \"Job\"\nErr = \"sleep.err\"\nUserLog = \"/home/cat/1-monday-2.1-queue/sleep.log\"\nRequirements = ( IsOSGSchoolSlot =?= true ) &&\n        ( TARGET.Arch == \"X86_64\" ) &&\n        ( TARGET.OpSys == \"LINUX\" ) &&\n        ( TARGET.Disk >= RequestDisk ) &&\n        ( TARGET.Memory >= RequestMemory ) &&\n        ( TARGET.HasFileTransfer )\nClusterId = 2420\nWhenToTransferOutput = \"ON_EXIT\"\nOwner = \"cat\"\nCondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\"\nOut = \"sleep.out\"\nCmd = \"/bin/sleep\"\nArguments = \"120\"\n\n\n\n\n\n\n\nNote\n\n\nAttributes are listed in no particular order and may change from time to time.\nDo not assume anything about the order of attributes in \ncondor_q\n output.\n\n\n\n\nSee what you can find in a job ClassAd from your own job.\n\n\n\n\nUsing a previous exercise, submit a \nsleep\n job that sleeps for at least 3 minutes (180 seconds).\n\n\n\n\nBefore the job executes, capture its ClassAd and save to a file:\n\n\ncondor_q -l \n<JOB.ID>\n > classad-1.txt\n\n\n\n\n\n\n\n\n\n\nAfter the job starts execution but before it finishes, capture its ClassAd again and save to a file\n\n\ncondor_q -l \n<JOB.ID>\n > classad-2.txt\n\n\n\n\n\n\n\n\n\n\nNow examine each saved ClassAd file. Here are a few things to look for:\n\n\n\n\nCan you find attributes that came from your submit file? (E.g., Cmd, Arguments, Out, Err, UserLog, and so forth)\n\n\nCan you find attributes that could have come from your submit file, but that HTCondor added for you? (E.g., Requirements)\n\n\nHow many of the following attributes can you guess the meaning of?\n\n\nDiskUsage\n\n\nImageSize\n\n\nBytesSent\n\n\nJobStatus\n\n\n\n\n\n\n\n\nWhy Is My Job Not Running?\n\u00b6\n\n\nSometimes, you submit a job and it just sits in the queue in Idle state, never running. It can be difficult to figure out why a job never matches and runs. Fortunately, HTCondor can give you some help.\n\n\nTo ask HTCondor why your job is not running, add the \n-better-analyze\n option to \ncondor_q\n for the specific job. For example, for job ID 2423.0, the command is:\n\n\nusername@learn $\n condor_q -better-analyze \n2423\n.0\n\n\n\n\n\nOf course, replace the job ID with your own.\n\n\nLet\u2019s submit a job that will never run and see what happens. Here is the submit file to use:\n\n\nexecutable = /bin/hostname\noutput = norun.out\nerror = norun.err\nlog = norun.log\nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\nrequest_memory = 8TB\nqueue\n\n\n\n\n\n(Do you see what I did?)\n\n\n\n\nSave and submit this file.\n\n\nRun \ncondor_q -analyze\n on the job ID.\n\n\n\n\nThere is a lot of output, but a few items are worth highlighting. Here is a sample from my own job (with many lines left out):\n\n\n-- Submitter: learn.chtc.wisc.edu : ....\n...\n---\n2423.000:  Run analysis summary.  Of 12388 machines,\n   12388 are rejected by your job's requirements \n...\nWARNING:  Be advised:\n   No resources matched request's constraints\n\nThe Requirements expression for your job is:\n...\n\nSuggestions:\n\n    Condition                         Machines Matched    Suggestion\n    ---------                         ----------------    ----------\n1   ( TARGET.Memory >= 8388608 )      0                   MODIFY TO 1000064\n2   ( ... )\n                                      12145                \n3   ( TARGET.Arch == \"X86_64\" )       12388                \n4   ( TARGET.OpSys == \"LINUX\" )       12386                \n5   ( TARGET.Disk >= 20 )             12387                \n6   ( TARGET.HasFileTransfer )        12388                \n\n\n\n\n\nToward the top, \ncondor_q\n said that it considered 12388 \u201cmachines\u201d (really, slots) and \nall\n 12388 of them were rejected by \nmy job\u2019s requirements\n. In other words, I am asking for something that is not available. But what?\n\n\nThe real clue comes from the breakdown of the Requirements expression, at the end of the output. \nNote the highlighted line: My job asked for \n8 terabytes\n of memory (8,388,608 MB) and \nno\n machines matched that part of the expression. \nWell, of course! 8 TB is a lot of memory on today\u2019s machines. \nAnd finally, note the suggestion: If I reduce my memory request to 1,000,064 MB (about 1 TB), there will be at least one slot in the pool that will match that expression.\n\n\nThe output from \ncondor_q -analyze\n (and \ncondor_q -better-analyze\n) may be helpful or it may not be, depending on your exact case. The example above was constructed so that it would be obvious what the problem was. But in many cases, this is a good place to start looking if you are having problems matching.\n\n\nBonus: Automatic Formatting Output\n\u00b6\n\n\nDo this exercise only if you have time, though it's pretty awesome!\n\n\nThere is a way to select the specific job attributes you want \ncondor_q\n to tell you about with the \n-autoformat\n or \n-af\n option. In this case, HTCondor decides for you how to format the data you ask for from job ClassAd(s). \n(To tell HTCondor how to specially format this information, yourself, you could use the \n-format\n option, which we're not covering.)\n\n\nTo use autoformatting, use the \n-af\n option followed by the attribute name, for each attribute that you want to output:\n\n\nusername@learn $\n condor_q -af Owner ClusterId Cmd\n\nmoate 2418 /share/test.sh\n\n\ncat 2421 /bin/sleep\n\n\ncat 2422 /bin/sleep\n\n\n\n\n\n\nBonus Question\n: If you wanted to print out the \nRequirements\n expression of a job, how would you do that with \n-af\n? Is the output what you expected? (HINT: for ClassAd attributes like \"Requirements\" that are long expressions, instead of simple values, you can use \n-af:r\n to view the expressions, instead of what it's current evaluation.)\n\n\nReferences\n\u00b6\n\n\nAs suggested above, if you want to learn more about \ncondor_q\n, you can do some reading:\n\n\n\n\nRead the \ncondor_q\n man page or HTCondor Manual section (same text) to learn about more options\n\n\nRead about ClassAd attributes in Appendix A of the HTCondor Manual",
            "title": "Exercise 3.1"
        },
        {
            "location": "/materials/day1/part3-ex1-queue/#monday-exercise-31-explore-condor_q",
            "text": "The goal of this exercise is try out some of the most common options to the  condor_q  command, so that you can view jobs effectively.  The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a  condor_q  expert!",
            "title": "Monday Exercise 3.1: Explore condor_q"
        },
        {
            "location": "/materials/day1/part3-ex1-queue/#selecting-jobs",
            "text": "The  condor_q  program has many options for selecting which jobs are listed. You have already seen that the default mode (as of version 8.5) is to show only your jobs in \"batch\" mode:  username@learn $  condor_q  You've seen that you can view all jobs (all users) in the submit node's queue by using the  -all  argument:  username@learn $  condor_q -all  And you've seen that you can view more details about queued jobs, with each separate job on a single line using the  -nobatch  option:  username@learn $  condor_q -nobatch username@learn $  condor_q -all -nobatch  Did you know you can also name one or more user IDs on the command line, in which case jobs for all of the named users are listed at once?  username@learn $  condor_q  <username1> <username2> <username3>   There are two other, simple selection criteria that you can use. To list just the jobs associated with a single cluster number:  username@learn $  condor_q  <CLUSTER>   For example, if you want to see the jobs in cluster 5678 (i.e.,  5678.0 ,  5678.1 , etc.), you use  condor_q 5678 .  To list a specific job (i.e., cluster.process, as in 5678.0):  username@learn $  condor_q  <JOB.ID>   For example, to see job ID 5678.1, you use  condor_q 5678.1 .   Note  You can name more than one cluster, job ID, or combination thereof on the command line, in which case jobs for all  of the named clusters and/or job IDs are listed.   Let\u2019s get some practice using  condor_q  selections!   Using a previous exercise, submit several  sleep  jobs.  List all jobs in the queue \u2014 are there others besides your own?  Practice using all forms of  condor_q  that you have learned:  List just your jobs, with and without batching.  List a specific cluster.  List a specific job ID.  Try listing several users at once.  Try listing several clusters and job IDs at once.    When there are a variety of jobs in the queue, try combining a username and a different user's cluster or job ID in the same command \u2014 what happens?",
            "title": "Selecting Jobs"
        },
        {
            "location": "/materials/day1/part3-ex1-queue/#viewing-a-job-classad",
            "text": "You may have wondered why it is useful to be able to list a single job ID using  condor_q . By itself, it may not be that useful. But, in combination with another option, it is very useful!  If you add the  -long  option to  condor_q  (or its short form,  -l ), it will show the complete ClassAd for each selected job, instead of the one-line summary that you have seen so far. Because job ClassAds may have 80\u201390 attributes (or more), it probably makes the most sense to show the ClassAd for a single job at a time. And you know how to show just one job! Here is what the command looks like:  username@learn $  condor_q -long  <JOB.ID>   The output from this command is long and complex. Most of the attributes that HTCondor adds to a job are arcane and uninteresting for us now. But here are some examples of common, interesting attributes taken directly from  condor_q  output (except with some line breaks added to the  Requirements  attribute):  MyType = \"Job\"\nErr = \"sleep.err\"\nUserLog = \"/home/cat/1-monday-2.1-queue/sleep.log\"\nRequirements = ( IsOSGSchoolSlot =?= true ) &&\n        ( TARGET.Arch == \"X86_64\" ) &&\n        ( TARGET.OpSys == \"LINUX\" ) &&\n        ( TARGET.Disk >= RequestDisk ) &&\n        ( TARGET.Memory >= RequestMemory ) &&\n        ( TARGET.HasFileTransfer )\nClusterId = 2420\nWhenToTransferOutput = \"ON_EXIT\"\nOwner = \"cat\"\nCondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\"\nOut = \"sleep.out\"\nCmd = \"/bin/sleep\"\nArguments = \"120\"   Note  Attributes are listed in no particular order and may change from time to time.\nDo not assume anything about the order of attributes in  condor_q  output.   See what you can find in a job ClassAd from your own job.   Using a previous exercise, submit a  sleep  job that sleeps for at least 3 minutes (180 seconds).   Before the job executes, capture its ClassAd and save to a file:  condor_q -l  <JOB.ID>  > classad-1.txt     After the job starts execution but before it finishes, capture its ClassAd again and save to a file  condor_q -l  <JOB.ID>  > classad-2.txt     Now examine each saved ClassAd file. Here are a few things to look for:   Can you find attributes that came from your submit file? (E.g., Cmd, Arguments, Out, Err, UserLog, and so forth)  Can you find attributes that could have come from your submit file, but that HTCondor added for you? (E.g., Requirements)  How many of the following attributes can you guess the meaning of?  DiskUsage  ImageSize  BytesSent  JobStatus",
            "title": "Viewing a Job ClassAd"
        },
        {
            "location": "/materials/day1/part3-ex1-queue/#why-is-my-job-not-running",
            "text": "Sometimes, you submit a job and it just sits in the queue in Idle state, never running. It can be difficult to figure out why a job never matches and runs. Fortunately, HTCondor can give you some help.  To ask HTCondor why your job is not running, add the  -better-analyze  option to  condor_q  for the specific job. For example, for job ID 2423.0, the command is:  username@learn $  condor_q -better-analyze  2423 .0  Of course, replace the job ID with your own.  Let\u2019s submit a job that will never run and see what happens. Here is the submit file to use:  executable = /bin/hostname\noutput = norun.out\nerror = norun.err\nlog = norun.log\nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\nrequest_memory = 8TB\nqueue  (Do you see what I did?)   Save and submit this file.  Run  condor_q -analyze  on the job ID.   There is a lot of output, but a few items are worth highlighting. Here is a sample from my own job (with many lines left out):  -- Submitter: learn.chtc.wisc.edu : ....\n...\n---\n2423.000:  Run analysis summary.  Of 12388 machines,\n   12388 are rejected by your job's requirements \n...\nWARNING:  Be advised:\n   No resources matched request's constraints\n\nThe Requirements expression for your job is:\n...\n\nSuggestions:\n\n    Condition                         Machines Matched    Suggestion\n    ---------                         ----------------    ----------\n1   ( TARGET.Memory >= 8388608 )      0                   MODIFY TO 1000064\n2   ( ... )\n                                      12145                \n3   ( TARGET.Arch == \"X86_64\" )       12388                \n4   ( TARGET.OpSys == \"LINUX\" )       12386                \n5   ( TARGET.Disk >= 20 )             12387                \n6   ( TARGET.HasFileTransfer )        12388                  Toward the top,  condor_q  said that it considered 12388 \u201cmachines\u201d (really, slots) and  all  12388 of them were rejected by  my job\u2019s requirements . In other words, I am asking for something that is not available. But what?  The real clue comes from the breakdown of the Requirements expression, at the end of the output. \nNote the highlighted line: My job asked for  8 terabytes  of memory (8,388,608 MB) and  no  machines matched that part of the expression. \nWell, of course! 8 TB is a lot of memory on today\u2019s machines. \nAnd finally, note the suggestion: If I reduce my memory request to 1,000,064 MB (about 1 TB), there will be at least one slot in the pool that will match that expression.  The output from  condor_q -analyze  (and  condor_q -better-analyze ) may be helpful or it may not be, depending on your exact case. The example above was constructed so that it would be obvious what the problem was. But in many cases, this is a good place to start looking if you are having problems matching.",
            "title": "Why Is My Job Not Running?"
        },
        {
            "location": "/materials/day1/part3-ex1-queue/#bonus-automatic-formatting-output",
            "text": "Do this exercise only if you have time, though it's pretty awesome!  There is a way to select the specific job attributes you want  condor_q  to tell you about with the  -autoformat  or  -af  option. In this case, HTCondor decides for you how to format the data you ask for from job ClassAd(s). \n(To tell HTCondor how to specially format this information, yourself, you could use the  -format  option, which we're not covering.)  To use autoformatting, use the  -af  option followed by the attribute name, for each attribute that you want to output:  username@learn $  condor_q -af Owner ClusterId Cmd moate 2418 /share/test.sh  cat 2421 /bin/sleep  cat 2422 /bin/sleep   Bonus Question : If you wanted to print out the  Requirements  expression of a job, how would you do that with  -af ? Is the output what you expected? (HINT: for ClassAd attributes like \"Requirements\" that are long expressions, instead of simple values, you can use  -af:r  to view the expressions, instead of what it's current evaluation.)",
            "title": "Bonus: Automatic Formatting Output"
        },
        {
            "location": "/materials/day1/part3-ex1-queue/#references",
            "text": "As suggested above, if you want to learn more about  condor_q , you can do some reading:   Read the  condor_q  man page or HTCondor Manual section (same text) to learn about more options  Read about ClassAd attributes in Appendix A of the HTCondor Manual",
            "title": "References"
        },
        {
            "location": "/materials/day1/part3-ex2-status/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 3.2: Explore condor_status\n\u00b6\n\n\nThe goal of this exercise is try out some of the most common options to the \ncondor_status\n command, so that you can view slots effectively.\n\n\nThe main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a \ncondor_status\n expert!\n\n\nSelecting Slots\n\u00b6\n\n\nThe \ncondor_status\n program has many options for selecting which slots are listed. You've already learned the basic \ncondor_status\n and the \ncondor_status -compact\n variation (which you may wish to retry now, before proceeding).\n\n\nAnother convenient option is to list only those slots that are available now:\n\n\nusername@learn $\n condor_status -avail\n\n\n\n\n\nOf course, the individual execute machines only report their slots to the collector at certain time intervals, so this list will not reflect the up-to-the-second reality of all slots. But this limitation is true of all \ncondor_status\n output, not just with the \n-avail\n option.\n\n\nSimilar to \ncondor_q\n, you can limit the slots that are listed in two easy ways. To list just the slots on a specific machine:\n\n\nusername@learn $\n condor_status \n<hostname>\n\n\n\n\n\n\nFor example, if you want to see the slots on \ne242.chtc.wisc.edu\n (in the CHTC pool):\n\n\nusername@learn $\n condor_status e242.chtc.wisc.edu\n\n\n\n\n\nTo list a specific slot on a machine:\n\n\nusername@learn $\n condor_status \n<slot>\n@\n<hostname>\n\n\n\n\n\n\nFor example, to see the \u201cfirst\u201d slot on the machine above:\n\n\nusername@learn $\n condor_status slot1@e242.chtc.wisc.edu\n\n\n\n\n\n\n\nNote\n\n\nYou can name more than one hostname, slot, or combination thereof on the command line, in which case slots for\n\nall\n of the named hostnames and/or slots are listed.\n\n\n\n\nLet\u2019s get some practice using \ncondor_status\n selections!\n\n\n\n\nList all slots in the pool \u2014 how many are there total?\n\n\nPractice using all forms of \ncondor_status\n that you have learned:\n\n\nList the available slots.\n\n\nList the slots on a specific machine (e.g., \ne242.chtc.wisc.edu\n).\n\n\nList a specific slot from that machine.\n\n\nTry listing the slots from a few (but not all) machines at once.\n\n\nTry using a mix of hostnames and slot IDs at once.\n\n\n\n\n\n\n\n\nViewing a Slot ClassAd\n\u00b6\n\n\nJust as with \ncondor_q\n, you can use \ncondor_status\n to view the complete ClassAd for a given slot (often confusingly called the \u201cmachine\u201d ad):\n\n\nusername@learn $\n condor_status -long \n<slot>\n@\n<hostname>\n\n\n\n\n\n\nBecause slot ClassAds may have 150\u2013200 attributes (or more), it probably makes the most sense to show the ClassAd for a single slot at a time, as shown above.\n\n\nHere are some examples of common, interesting attributes taken directly from \ncondor_status\n output:\n\n\nOpSys = \"LINUX\"\nDetectedCpus = 24\nOpSysAndVer = \"SL6\"\nMyType = \"Machine\"\nLoadAvg = 0.99\nTotalDisk = 798098404\nOSIssue = \"Scientific Linux release 6.6 (Carbon)\"\nTotalMemory = 24016\nMachine = \"e242.chtc.wisc.edu\"\nCondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\"\nMemory = 1024\n\n\n\n\n\nAs you may be able to tell, there is a mix of attributes about the machine as a whole (hence the name \u201cmachine ad\u201d) and about the slot in particular.\n\n\nGo ahead and examine a machine ClassAd now. I suggest looking at one of the slots on, say, \nc010.chtc.wisc.edu\n because of its relatively simple configuration.\n\n\nViewing Slots by ClassAd Expression\n\u00b6\n\n\nOften, it is helpful to view slots that meet some particular criteria. For example, if you know that your job needs a lot of memory to run, you may want to see how many high-memory slots there are and whether they are busy. You can filter the list of slots like this using the \n-constraint\n option and a ClassAd expression.\n\n\nFor example, suppose we want to list all slots that are running Scientific Linux 6 (operating system) and have at least 16 GB memory available. Note that memory is reported in units of Megabytes. The command is:\n\n\nusername@learn $\n condor_status -constraint \n'OpSysAndVer == \"CentOS7\" && Memory >= 200000'\n\n\n\n\n\n\n\n\nNote\n\n\nBe very careful with using quote characters appropriately in these commands.\nIn the example above, the single quotes (\n'\n) are for the shell, so that the entire expression is passed to\n\ncondor_status\n untouched, and the double quotes (\n\"\n) surround a string value within the expression itself.\n\n\n\n\nCurrently on CHTC, there are only a few slots that meet these criteria (our high-memory servers, mainly used for metagenomics assemblies).\n\n\nIf you are interested in learning more about writing ClassAd expressions, look at section 4.1 and especially 4.1.4 of the HTCondor Manual. This is definitely advanced material, so if you do not want to read it, that is fine. But if you do, take some time to practice writing expressions for the \ncondor_status -constraint\n command.\n\n\n\n\nNote\n\n\nThe \ncondor_q\n command accepts the \n-constraint\n option as well!\nAs you might expect, the option allows you to limit the jobs that are listed based on a ClassAd expression.\n\n\n\n\nBonus: Formatting Output\n\u00b6\n\n\nThe \ncondor_status\n command accepts the same \n-autoformat\n (\n-af\n) options that \ncondor_q\n accepts, and the options have the same meanings in both commands. Of course, the attributes available in machine ads may differ from the ones that are available in job ads. Use the HTCondor Manual or look at individual slot ClassAds to get a better idea of what attributes are available.\n\n\nFor example, I was curious about the Windows slots listed in the \ncondor_status\n summary output. Here are two commands that show the full hostnames and major version information for the Windows slots:\n\n\nusername@learn $\n condor_status -af Machine -af OpSysAndVer -constraint \n'OpSys == \"WINDOWS\"'\n\n\n\n\n\n\nIf you like, spend a few minutes now or later experimenting with \ncondor_status\n formatting.\n\n\nReferences\n\u00b6\n\n\nAs suggested above, if you want to learn more about \ncondor_q\n, you can do some reading:\n\n\n\n\nRead the \ncondor_status\n man page or HTCondor Manual section (same text) to learn about more options\n\n\nRead about ClassAd attributes in Appendix A of the HTCondor Manual\n\n\nRead about ClassAd expressions in section 4.1.4 of the HTCondor Manual",
            "title": "Exercise 3.2"
        },
        {
            "location": "/materials/day1/part3-ex2-status/#monday-exercise-32-explore-condor_status",
            "text": "The goal of this exercise is try out some of the most common options to the  condor_status  command, so that you can view slots effectively.  The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a  condor_status  expert!",
            "title": "Monday Exercise 3.2: Explore condor_status"
        },
        {
            "location": "/materials/day1/part3-ex2-status/#selecting-slots",
            "text": "The  condor_status  program has many options for selecting which slots are listed. You've already learned the basic  condor_status  and the  condor_status -compact  variation (which you may wish to retry now, before proceeding).  Another convenient option is to list only those slots that are available now:  username@learn $  condor_status -avail  Of course, the individual execute machines only report their slots to the collector at certain time intervals, so this list will not reflect the up-to-the-second reality of all slots. But this limitation is true of all  condor_status  output, not just with the  -avail  option.  Similar to  condor_q , you can limit the slots that are listed in two easy ways. To list just the slots on a specific machine:  username@learn $  condor_status  <hostname>   For example, if you want to see the slots on  e242.chtc.wisc.edu  (in the CHTC pool):  username@learn $  condor_status e242.chtc.wisc.edu  To list a specific slot on a machine:  username@learn $  condor_status  <slot> @ <hostname>   For example, to see the \u201cfirst\u201d slot on the machine above:  username@learn $  condor_status slot1@e242.chtc.wisc.edu   Note  You can name more than one hostname, slot, or combination thereof on the command line, in which case slots for all  of the named hostnames and/or slots are listed.   Let\u2019s get some practice using  condor_status  selections!   List all slots in the pool \u2014 how many are there total?  Practice using all forms of  condor_status  that you have learned:  List the available slots.  List the slots on a specific machine (e.g.,  e242.chtc.wisc.edu ).  List a specific slot from that machine.  Try listing the slots from a few (but not all) machines at once.  Try using a mix of hostnames and slot IDs at once.",
            "title": "Selecting Slots"
        },
        {
            "location": "/materials/day1/part3-ex2-status/#viewing-a-slot-classad",
            "text": "Just as with  condor_q , you can use  condor_status  to view the complete ClassAd for a given slot (often confusingly called the \u201cmachine\u201d ad):  username@learn $  condor_status -long  <slot> @ <hostname>   Because slot ClassAds may have 150\u2013200 attributes (or more), it probably makes the most sense to show the ClassAd for a single slot at a time, as shown above.  Here are some examples of common, interesting attributes taken directly from  condor_status  output:  OpSys = \"LINUX\"\nDetectedCpus = 24\nOpSysAndVer = \"SL6\"\nMyType = \"Machine\"\nLoadAvg = 0.99\nTotalDisk = 798098404\nOSIssue = \"Scientific Linux release 6.6 (Carbon)\"\nTotalMemory = 24016\nMachine = \"e242.chtc.wisc.edu\"\nCondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\"\nMemory = 1024  As you may be able to tell, there is a mix of attributes about the machine as a whole (hence the name \u201cmachine ad\u201d) and about the slot in particular.  Go ahead and examine a machine ClassAd now. I suggest looking at one of the slots on, say,  c010.chtc.wisc.edu  because of its relatively simple configuration.",
            "title": "Viewing a Slot ClassAd"
        },
        {
            "location": "/materials/day1/part3-ex2-status/#viewing-slots-by-classad-expression",
            "text": "Often, it is helpful to view slots that meet some particular criteria. For example, if you know that your job needs a lot of memory to run, you may want to see how many high-memory slots there are and whether they are busy. You can filter the list of slots like this using the  -constraint  option and a ClassAd expression.  For example, suppose we want to list all slots that are running Scientific Linux 6 (operating system) and have at least 16 GB memory available. Note that memory is reported in units of Megabytes. The command is:  username@learn $  condor_status -constraint  'OpSysAndVer == \"CentOS7\" && Memory >= 200000'    Note  Be very careful with using quote characters appropriately in these commands.\nIn the example above, the single quotes ( ' ) are for the shell, so that the entire expression is passed to condor_status  untouched, and the double quotes ( \" ) surround a string value within the expression itself.   Currently on CHTC, there are only a few slots that meet these criteria (our high-memory servers, mainly used for metagenomics assemblies).  If you are interested in learning more about writing ClassAd expressions, look at section 4.1 and especially 4.1.4 of the HTCondor Manual. This is definitely advanced material, so if you do not want to read it, that is fine. But if you do, take some time to practice writing expressions for the  condor_status -constraint  command.   Note  The  condor_q  command accepts the  -constraint  option as well!\nAs you might expect, the option allows you to limit the jobs that are listed based on a ClassAd expression.",
            "title": "Viewing Slots by ClassAd Expression"
        },
        {
            "location": "/materials/day1/part3-ex2-status/#bonus-formatting-output",
            "text": "The  condor_status  command accepts the same  -autoformat  ( -af ) options that  condor_q  accepts, and the options have the same meanings in both commands. Of course, the attributes available in machine ads may differ from the ones that are available in job ads. Use the HTCondor Manual or look at individual slot ClassAds to get a better idea of what attributes are available.  For example, I was curious about the Windows slots listed in the  condor_status  summary output. Here are two commands that show the full hostnames and major version information for the Windows slots:  username@learn $  condor_status -af Machine -af OpSysAndVer -constraint  'OpSys == \"WINDOWS\"'   If you like, spend a few minutes now or later experimenting with  condor_status  formatting.",
            "title": "Bonus: Formatting Output"
        },
        {
            "location": "/materials/day1/part3-ex2-status/#references",
            "text": "As suggested above, if you want to learn more about  condor_q , you can do some reading:   Read the  condor_status  man page or HTCondor Manual section (same text) to learn about more options  Read about ClassAd attributes in Appendix A of the HTCondor Manual  Read about ClassAd expressions in section 4.1.4 of the HTCondor Manual",
            "title": "References"
        },
        {
            "location": "/materials/day1/part3-ex3-job-retry/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 3.3: Retries\n\u00b6\n\n\nThe goal of this exercise is to demonstrate running a job that intermittently fails and thus could benefit from having HTCondor automatically retry it.\n\n\nThis first part of the exercise should take only a few minutes, and is designed to setup the next exercises.\n\n\nBad Job\n\u00b6\n\n\nLet\u2019s assume that a colleague has shared with you a program, and it fails once in a while. In the real world, we would probably just fix the program, but what if you cannot change the software? Unfortunately, this situation happens more often than we would like.\n\n\nBelow is a simple Python script that fails once in a while. We will not fix it, but use it to simulate a program that can fail and that we \ncannot\n fix.\n\n\n#!/usr/bin/env python\n\n# murphy.py simulates a real program with real problems\nimport random\nimport sys\nimport time\n\n# Create a random number seeded by system entropy\nr = random.SystemRandom()\n\n# One time in three, simulate a runtime error\nif (r.randint(0,2) == 0):\n    # intentionally print no output\n    sys.exit(15)\nelse:\n    time.sleep(3)\n    print \"All work done correctly\"\n\n# By convention, zero exit code means success\nsys.exit(0)\n\n\n\n\n\nEven if you are not a Python expert, you may be able to figure out what this program does.\n\n\nLet\u2019s see what happens when a program like this one is run in HTCondor.\n\n\n\n\nIn a new directory for this exercise, save the script above as \nmurphy.py\n.\n\n\nWrite a submit file for the script; \nqueue 20\n instances of the job and be sure to ask for 20\u00a0MB of memory and disk.\n\n\nSubmit the file and wait for the jobs to finish.\n\n\n\n\nWhat output do you expect? What output did you get? If you are curious about the exit code from the job, it is saved in completed jobs in \ncondor_history\n in the \nExitCode\n attribute. The following command will show the \nExitCode\n for a given cluster of jobs:\n\n\nusername@learn $\n condor_history \n<CLUSTER>\n -af ProcId ExitCode\n\n\n\n\n\n(Be sure to replace \n<cluster>\n with your actual cluster ID)\n\n\nHow many of the jobs succeeded? How many failed?\n\n\nRetrying Failed Jobs\n\u00b6\n\n\nNow let\u2019s see if we can solve the problem of jobs that fail once in a while. In this particular case, if HTCondor runs a failed job again, it has a good chance of succeeding. Not all failing jobs are like this, but in this case it is a reasonable assumption.\n\n\nFrom the lecture materials, implement the \nmax_retries\n feature to retry any job with a non-zero exit code up to 5 times, then resubmit the jobs. Did your change work?\n\n\nAfter the jobs have finished, examine the log file(s) to see what happened in detail. Did any jobs need to be restarted? Another way to see how many restarts there were is to look at the \nNumJobStarts\n attribute of a completed job with the \ncondor_history\n command, in the same way you looked at the \nExitCode\n attribute earlier. Does the number of retries seem correct? For those jobs which did need to be retried, what is their \nExitCode\n; and what about the \nExitCode\n from earlier execution attempts?\n\n\nA (Too) Long Running Job\n\u00b6\n\n\nSometimes, an ill-behaved job will get stuck in a loop and run forever, instead of exiting with a failure code, and it may just need to be re-run (or run on a different execute server) to complete without getting stuck. We can modify our Python program to simulate this kind of bad job with the following file:\n\n\n#!/usr/bin/env python\n\n# murphy.py simulate a real program with real problems\nimport random\nimport sys\nimport time\n\n# Create a random number seeded by system entropy\nr = random.SystemRandom()\n\n# One time in three, simulate an infinite loop\nif (r.randint(0,2) == 0):\n        # intentionally print no output\n        time.sleep(3600)\n        sys.exit(15)\nelse:\n        time.sleep(3)\n        print \"All work done correctly\"\n\n# By convention, zero exit code means success\nsys.exit(0)\n\n\n\n\n\nAgain, you may be able to figure out what this new program does.\n\n\n\n\nSave the script to a new file named \nmurphy2.py\n.\n\n\nCopy your previous submit file to a new name and change the \nexecutable\n to \nmurphy2.py\n.\n\n\nIf you like, submit the new file\u00a0\u2014 but after a while be sure to remove the whole cluster to clear out the \u201chung\u201d jobs.\n\n\n\n\nNow try to change the submit file to automatically remove any jobs that \nrun\n for more than one minute. You can make this change with just a single line in your submit file\n\n\nperiodic_remove = (JobStatus == 2) && ( (CurrentTime - EnteredCurrentStatus) > 60 )\n\n\n\n\n\n\n\n\n\nSubmit the new file. Do the long running jobs get removed? What does \ncondor_history\n show for the cluster after all jobs are done? Which job status (i.e. idle, held, running) do you think \nJobStatus == 2\n corresponds to?\n\n\n\n\n\n\nBonus Exercise\n\u00b6\n\n\nIf you have time, edit your submit file so that instead of removing long running jobs, have HTCondor automatically put the long-running job on hold, and then automatically release it.",
            "title": "Exercise 3.3"
        },
        {
            "location": "/materials/day1/part3-ex3-job-retry/#monday-exercise-33-retries",
            "text": "The goal of this exercise is to demonstrate running a job that intermittently fails and thus could benefit from having HTCondor automatically retry it.  This first part of the exercise should take only a few minutes, and is designed to setup the next exercises.",
            "title": "Monday Exercise 3.3: Retries"
        },
        {
            "location": "/materials/day1/part3-ex3-job-retry/#bad-job",
            "text": "Let\u2019s assume that a colleague has shared with you a program, and it fails once in a while. In the real world, we would probably just fix the program, but what if you cannot change the software? Unfortunately, this situation happens more often than we would like.  Below is a simple Python script that fails once in a while. We will not fix it, but use it to simulate a program that can fail and that we  cannot  fix.  #!/usr/bin/env python\n\n# murphy.py simulates a real program with real problems\nimport random\nimport sys\nimport time\n\n# Create a random number seeded by system entropy\nr = random.SystemRandom()\n\n# One time in three, simulate a runtime error\nif (r.randint(0,2) == 0):\n    # intentionally print no output\n    sys.exit(15)\nelse:\n    time.sleep(3)\n    print \"All work done correctly\"\n\n# By convention, zero exit code means success\nsys.exit(0)  Even if you are not a Python expert, you may be able to figure out what this program does.  Let\u2019s see what happens when a program like this one is run in HTCondor.   In a new directory for this exercise, save the script above as  murphy.py .  Write a submit file for the script;  queue 20  instances of the job and be sure to ask for 20\u00a0MB of memory and disk.  Submit the file and wait for the jobs to finish.   What output do you expect? What output did you get? If you are curious about the exit code from the job, it is saved in completed jobs in  condor_history  in the  ExitCode  attribute. The following command will show the  ExitCode  for a given cluster of jobs:  username@learn $  condor_history  <CLUSTER>  -af ProcId ExitCode  (Be sure to replace  <cluster>  with your actual cluster ID)  How many of the jobs succeeded? How many failed?",
            "title": "Bad Job"
        },
        {
            "location": "/materials/day1/part3-ex3-job-retry/#retrying-failed-jobs",
            "text": "Now let\u2019s see if we can solve the problem of jobs that fail once in a while. In this particular case, if HTCondor runs a failed job again, it has a good chance of succeeding. Not all failing jobs are like this, but in this case it is a reasonable assumption.  From the lecture materials, implement the  max_retries  feature to retry any job with a non-zero exit code up to 5 times, then resubmit the jobs. Did your change work?  After the jobs have finished, examine the log file(s) to see what happened in detail. Did any jobs need to be restarted? Another way to see how many restarts there were is to look at the  NumJobStarts  attribute of a completed job with the  condor_history  command, in the same way you looked at the  ExitCode  attribute earlier. Does the number of retries seem correct? For those jobs which did need to be retried, what is their  ExitCode ; and what about the  ExitCode  from earlier execution attempts?",
            "title": "Retrying Failed Jobs"
        },
        {
            "location": "/materials/day1/part3-ex3-job-retry/#a-too-long-running-job",
            "text": "Sometimes, an ill-behaved job will get stuck in a loop and run forever, instead of exiting with a failure code, and it may just need to be re-run (or run on a different execute server) to complete without getting stuck. We can modify our Python program to simulate this kind of bad job with the following file:  #!/usr/bin/env python\n\n# murphy.py simulate a real program with real problems\nimport random\nimport sys\nimport time\n\n# Create a random number seeded by system entropy\nr = random.SystemRandom()\n\n# One time in three, simulate an infinite loop\nif (r.randint(0,2) == 0):\n        # intentionally print no output\n        time.sleep(3600)\n        sys.exit(15)\nelse:\n        time.sleep(3)\n        print \"All work done correctly\"\n\n# By convention, zero exit code means success\nsys.exit(0)  Again, you may be able to figure out what this new program does.   Save the script to a new file named  murphy2.py .  Copy your previous submit file to a new name and change the  executable  to  murphy2.py .  If you like, submit the new file\u00a0\u2014 but after a while be sure to remove the whole cluster to clear out the \u201chung\u201d jobs.   Now try to change the submit file to automatically remove any jobs that  run  for more than one minute. You can make this change with just a single line in your submit file  periodic_remove = (JobStatus == 2) && ( (CurrentTime - EnteredCurrentStatus) > 60 )    Submit the new file. Do the long running jobs get removed? What does  condor_history  show for the cluster after all jobs are done? Which job status (i.e. idle, held, running) do you think  JobStatus == 2  corresponds to?",
            "title": "A (Too) Long Running Job"
        },
        {
            "location": "/materials/day1/part3-ex3-job-retry/#bonus-exercise",
            "text": "If you have time, edit your submit file so that instead of removing long running jobs, have HTCondor automatically put the long-running job on hold, and then automatically release it.",
            "title": "Bonus Exercise"
        },
        {
            "location": "/materials/day1/part4-ex1-submit-refresher/",
            "text": "Monday Exercise 4.1: Refresher \u2013 Submitting Multiple Jobs\n\u00b6\n\n\nThe goal of this exercise is to map the physical locations of some worker nodes in our local cluster.\nTo do this, you will write a simple submit file that will queue multiple jobs and then manually collate the results.\n\n\nWhere in the world are my jobs?\n\u00b6\n\n\nTo find the physical location of the computers your jobs our running on, you will use a method called \ngeolocation\n.\nGeolocation uses a registry to match a computer\u2019s network address to an approximate latitude and longitude.\n\n\nGeolocating several machines\n\u00b6\n\n\nNow, let\u2019s try to remember some basic HTCondor ideas from earlier today:\n\n\n\n\nLog in to \nlearn.chtc.wisc.edu\n\n\nCreate and change into a new folder for this exercise, for example \nmonday-4.1\n\n\n\n\nDownload the geolocation code:\n\n\nuser@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/location-wrapper.sh \n\\\n\n             http://proxy.chtc.wisc.edu/SQUID/osgschool19/wn-geoip.tar.gz\n\n\n\n\n\nYou will be using \nlocation-wrapper.sh\n as your executable and \nwn-geoip.tar.gz\n as an input file.\n\n\n\n\n\n\nCreate a submit file that generates \nfifty\n jobs that run \nlocation-wrapper.sh\n, transfers \nwn-geoip.tar.gz\n as an\n    input file, and uses the \n$(Process)\n macro to write different \noutput\n and \nerror\n files.\n    Also, add the following requirement to the submit file (it's not important to know what it does):\n\n\nRequirements = (HAS_CVMFS_oasis_opensciencegrid_org =?= TRUE)\n\n\n\n\n\nTry to do this step without looking at materials from earlier today.\nBut if you are stuck, see \ntoday\u2019s exercise 2.2\n.\n\n\n\n\n\n\nSubmit your jobs and wait for the results\n\n\n\n\n\n\nCollating your results\n\u00b6\n\n\nNow that you have your results, it's time to summarize them.\nRather than inspecting each output file individually, you can use the \ncat\n command to print the results from all of\nyour output files at once.\nIf all of your output files have the format \nlocation-#.out\n (e.g., \nlocation-10.out\n), your command will look something\nlike this:\n\n\nuser@learn $\n cat location-*.out\n\n\n\n\n\nThe \n*\n is a wildcard so the above cat command runs on all files that start with \nlocation-\n and end in \n.out\n.\nAdditionally, you can use \ncat\n in combination with the \nsort\n and \nuniq\n commands using \"pipes\" (\n|\n) to print only\nthe unique results:\n\n\nuser@learn $\n cat location-*.out \n|\n sort \n|\n uniq\n\n\n\n\n\nMapping your results\n\u00b6\n\n\nTo visualize the locations of the machines that your jobs ran on, you will be using \nhttp://www.mapcustomizer.com/\n.\nCopy and paste the collated results into the text box that pops up when clicking on the 'Bulk Entry' button on the\nright-hand side.\nWhere did your jobs run?\n\n\nNext exercise\n\u00b6\n\n\nOnce completed, move onto the next exercise: \nLogging in to the OSG Submit Machine",
            "title": "Exercise 4.1"
        },
        {
            "location": "/materials/day1/part4-ex1-submit-refresher/#monday-exercise-41-refresher-submitting-multiple-jobs",
            "text": "The goal of this exercise is to map the physical locations of some worker nodes in our local cluster.\nTo do this, you will write a simple submit file that will queue multiple jobs and then manually collate the results.",
            "title": "Monday Exercise 4.1: Refresher \u2013 Submitting Multiple Jobs"
        },
        {
            "location": "/materials/day1/part4-ex1-submit-refresher/#where-in-the-world-are-my-jobs",
            "text": "To find the physical location of the computers your jobs our running on, you will use a method called  geolocation .\nGeolocation uses a registry to match a computer\u2019s network address to an approximate latitude and longitude.",
            "title": "Where in the world are my jobs?"
        },
        {
            "location": "/materials/day1/part4-ex1-submit-refresher/#geolocating-several-machines",
            "text": "Now, let\u2019s try to remember some basic HTCondor ideas from earlier today:   Log in to  learn.chtc.wisc.edu  Create and change into a new folder for this exercise, for example  monday-4.1   Download the geolocation code:  user@learn $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/location-wrapper.sh  \\ \n             http://proxy.chtc.wisc.edu/SQUID/osgschool19/wn-geoip.tar.gz  You will be using  location-wrapper.sh  as your executable and  wn-geoip.tar.gz  as an input file.    Create a submit file that generates  fifty  jobs that run  location-wrapper.sh , transfers  wn-geoip.tar.gz  as an\n    input file, and uses the  $(Process)  macro to write different  output  and  error  files.\n    Also, add the following requirement to the submit file (it's not important to know what it does):  Requirements = (HAS_CVMFS_oasis_opensciencegrid_org =?= TRUE)  Try to do this step without looking at materials from earlier today.\nBut if you are stuck, see  today\u2019s exercise 2.2 .    Submit your jobs and wait for the results",
            "title": "Geolocating several machines"
        },
        {
            "location": "/materials/day1/part4-ex1-submit-refresher/#collating-your-results",
            "text": "Now that you have your results, it's time to summarize them.\nRather than inspecting each output file individually, you can use the  cat  command to print the results from all of\nyour output files at once.\nIf all of your output files have the format  location-#.out  (e.g.,  location-10.out ), your command will look something\nlike this:  user@learn $  cat location-*.out  The  *  is a wildcard so the above cat command runs on all files that start with  location-  and end in  .out .\nAdditionally, you can use  cat  in combination with the  sort  and  uniq  commands using \"pipes\" ( | ) to print only\nthe unique results:  user@learn $  cat location-*.out  |  sort  |  uniq",
            "title": "Collating your results"
        },
        {
            "location": "/materials/day1/part4-ex1-submit-refresher/#mapping-your-results",
            "text": "To visualize the locations of the machines that your jobs ran on, you will be using  http://www.mapcustomizer.com/ .\nCopy and paste the collated results into the text box that pops up when clicking on the 'Bulk Entry' button on the\nright-hand side.\nWhere did your jobs run?",
            "title": "Mapping your results"
        },
        {
            "location": "/materials/day1/part4-ex1-submit-refresher/#next-exercise",
            "text": "Once completed, move onto the next exercise:  Logging in to the OSG Submit Machine",
            "title": "Next exercise"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/",
            "text": "Monday Exercise 4.2: Log in to the OSG Submit Machine\n\u00b6\n\n\nThe goal of this exercise is to log in to a different submit host so that you can start submitting jobs into the OSG\ninstead of the local cluster here at UW-Madison.\nAdditionally, you will learn about the \ntar\n and \nscp\n commands, which will allow you to efficiently copy files between\nthe two submit nodes.\n\n\nIf you have trouble getting \nssh\n access to the submit machine, ask the instructors right away! Gaining access is\ncritical for all remaining exercises.\n\n\nLog in to the OSG submit machine\n\u00b6\n\n\nFor some of the remaining exercises today, you will be using a machine named \ntraining.osgconnect.net\n.\nThe username and password are listed on your 'Accounts' paper that you received yesterday.\nIf you no longer have it, please ask the instructors for help.\n\n\nOnce you have your account details, \nssh\n in to the machine and take a look around.\n\n\nPreparing files for transfer\n\u00b6\n\n\nWhen transferring files between computers, it's best to limit the number of files as well as their size.\nSmaller files transfer more quickly and if your network connection drops, restarting the transfer is less painful than\nit would be if you were transferring large files.\n\n\nArchiving tools (WinZip, 7zip, Archive Utility, etc.) can compress the size of your files and place them into a single,\nsmaller archive file.\nThe \ntar\n command is a one-stop shop for creating, extracting, and viewing the contents of \ntar\n archives (called\ntarballs) whose usage is as follows:\n\n\n\n\n\n\nTo \ncreate\n a tarball named \n<archive filename>\n containing \n<archive contents>\n, use the following command:\n\n\nuser@training $\n tar -czvf <archive filename> <archive contents>\n\n\n\n\n\nWhere \n<archive filename>\n should end in \n.tar.gz\n and \n<archive contents>\n can be a list of any number of files\nand/or folders, separated by spaces.\n\n\n\n\n\n\nTo \nextract\n the files from a tarball into the current directory:\n\n\nuser@training $\n tar -xzvf <archive filename>\n\n\n\n\n\n\n\n\n\nTo \nlist\n the files within a tarball:\n\n\nuser@training $\n tar -tzvf <archive filename>\n\n\n\n\n\n\n\n\n\nUsing the above knowledge, log into \nlearn.chtc.wisc.edu\n, create a tarball that contains Monday's exercise 2.4\ndirectory, and verify that it contains all the proper files.\n\n\nComparing compressed sizes\n\u00b6\n\n\nYou can adjust the level of compression of \ntar\n by prepending your command with \nGZIP=--<COMPRESSION>\n, where\n\n<COMPRESSION>\n can be either \nfast\n for the least compression, or \nbest\n for the most compression (the default\ncompression is between \nbest\n and \nfast\n).\n\n\n\n\nUse \nwget\n to download the following files from our web server:\n\n\nText file: \nhttp://proxy.chtc.wisc.edu/SQUID/osgschool19/random_text\n\n\nArchive: \nhttp://proxy.chtc.wisc.edu/SQUID/osgschool19/pdbaa.tar.gz\n\n\nImage: \nhttp://proxy.chtc.wisc.edu/SQUID/osgschool19/obligatory_cat.jpg\n\n\n\n\n\n\nUse \ntar\n on each file and use \nls -l\n to compare the sizes of the original file and the compressed version.\n\n\n\n\nWhich files were compressed the least? Why?\n\n\nTransferring files\n\u00b6\n\n\nUsing secure copy\n\u00b6\n\n\nSecure copy\n (\nscp\n) is a command based on \nSSH\n that lets you securely copy\nfiles between two different hosts.\nIt takes similar arguments to the \ncp\n command that you are familiar with but also takes additional host information:\n\n\nuser@learn $\n scp <\nsource\n \n1\n> <\nsource\n \n2\n>...<\nsource\n N> <remote host>:<remote path>\n\n\n\n\n\nFor example, if I were logged in to \nlearn.chtc.wisc.edu\n and wanted to copy the file \nfoo\n from my current directory to\nmy home directory on \ntraining.osgconnect.net\n, the command would look like this:\n\n\nuser@learn $\n scp foo training.osgconnect.net:~\n\n\n\n\n\nAdditionally, I could also pull files from \ntraining.osgconnect.net\n to \nlearn.chtc.wisc.edu\n.\nThe following command copies \nbar\n from my home directory on \ntraining.osgconnect.net\n to my current directory on\n\nlearn.chtc.wisc.edu\n:\n\n\nuser@learn $\n scp training.osgconnect.net:~/bar .\n\n\n\n\n\nYou can also copy folders between hosts using the \n-r\n option.\nIf I kept all my files from Monday's exercise 1.3 in a folder named \nmonday-1.3\n on \nlearn.chtc.wisc.edu\n, I could use\nthe following command to copy them to my home directory on \ntraining.osgconnect.net\n:\n\n\nuser@learn $\n scp -r monday-1.3 training.osgconnect.net:~\n\n\n\n\n\nTry copying the tarball you created earlier in this exercise on \nlearn.chtc.wisc.edu\n to \ntraining.osgconnect.net\n.\n\n\nSecure copy from your laptop\n\u00b6\n\n\nDuring your research, you may need to retrieve output files from your submit host to inspect them on your personal\nmachine, which can also be done with \nscp\n! To use \nscp\n on your laptop, follow the instructions relevant to your\nmachine's operating system:\n\n\nMac and Linux users\n\u00b6\n\n\nscp\n should be included by default and available via the terminal on both Mac and Linux operating systems.\nOpen a terminal window on your laptop and try copying the tarball containing Monday's 2.4 exercise from\n\ntraining.osgconnect.net\n to your laptop.\n\n\nWindows users\n\u00b6\n\n\nWinSCP is an \nscp\n client for Windows operating systems.\n\n\n\n\nInstall WinSCP from \nhttps://winscp.net/eng/index.php\n\n\nStart WinSCP and enter your SSH credentials for \ntraining.osgconnect.net\n\n\nCopy the tarball containing Monday's 2.4 exercise exercise to your laptop\n\n\n\n\nExtra challenge: Using rsync\n\u00b6\n\n\nscp\n is a great, ubiquitous tool for one-time transfers but there are better tools if you find yourself transferring\nthe same set of files to the same location repeatedly.\nAnother common tool available on many Linux machines is \nrsync\n, which is like a beefed-up version of \nscp\n.\nThe invocation is similar to \nscp\n: you can transfer files and/or folders, but the options are different and when\ntransferring folders, make sure they don't have a trailing slash (\n/\n, this means to copy all the files within the\nfolder instead of the folder itself):\n\n\nuser@learn $\n rsync -Pavz <\nsource\n \n1\n> <\nsource\n \n2\n>...<\nsource\n N> <remote host>:<remote path>\n\n\n\n\n\nrsync\n has many benefits over \nscp\n but two of its biggest features are built-in compression (so you don't have to\ncreate a tarball) and the ability to only transfer files that have changed.\nBoth of these feature are helpful when you're having connectivity issues so that you don't have to restart the transfer\nfrom scratch every time your connection fails.\n\n\n\n\nUse \nrsync\n to transfer the folder containing today's exercise 1.1 to \ntraining.osgconnect.net\n\n\n\n\nCreate a new file in your exercise 1.1 folder on \nlearn.chtc.wisc.edu\n with the \ntouch\n command:\n\n\nuser@learn $\n touch <filename>\n\n\n\n\n\n\n\n\n\nUse the same \nrsync\n command to transfer the folder with the new file you just created.\n   How many files were transferred the first time? How many files were transferred if you run the same rsync command\n   again?\n\n\n\n\n\n\nNext exercise\n\u00b6\n\n\nOnce completed, move onto the next exercise: \nRunning jobs in the OSG",
            "title": "Exercise 4.2"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/#monday-exercise-42-log-in-to-the-osg-submit-machine",
            "text": "The goal of this exercise is to log in to a different submit host so that you can start submitting jobs into the OSG\ninstead of the local cluster here at UW-Madison.\nAdditionally, you will learn about the  tar  and  scp  commands, which will allow you to efficiently copy files between\nthe two submit nodes.  If you have trouble getting  ssh  access to the submit machine, ask the instructors right away! Gaining access is\ncritical for all remaining exercises.",
            "title": "Monday Exercise 4.2: Log in to the OSG Submit Machine"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/#log-in-to-the-osg-submit-machine",
            "text": "For some of the remaining exercises today, you will be using a machine named  training.osgconnect.net .\nThe username and password are listed on your 'Accounts' paper that you received yesterday.\nIf you no longer have it, please ask the instructors for help.  Once you have your account details,  ssh  in to the machine and take a look around.",
            "title": "Log in to the OSG submit machine"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/#preparing-files-for-transfer",
            "text": "When transferring files between computers, it's best to limit the number of files as well as their size.\nSmaller files transfer more quickly and if your network connection drops, restarting the transfer is less painful than\nit would be if you were transferring large files.  Archiving tools (WinZip, 7zip, Archive Utility, etc.) can compress the size of your files and place them into a single,\nsmaller archive file.\nThe  tar  command is a one-stop shop for creating, extracting, and viewing the contents of  tar  archives (called\ntarballs) whose usage is as follows:    To  create  a tarball named  <archive filename>  containing  <archive contents> , use the following command:  user@training $  tar -czvf <archive filename> <archive contents>  Where  <archive filename>  should end in  .tar.gz  and  <archive contents>  can be a list of any number of files\nand/or folders, separated by spaces.    To  extract  the files from a tarball into the current directory:  user@training $  tar -xzvf <archive filename>    To  list  the files within a tarball:  user@training $  tar -tzvf <archive filename>    Using the above knowledge, log into  learn.chtc.wisc.edu , create a tarball that contains Monday's exercise 2.4\ndirectory, and verify that it contains all the proper files.",
            "title": "Preparing files for transfer"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/#comparing-compressed-sizes",
            "text": "You can adjust the level of compression of  tar  by prepending your command with  GZIP=--<COMPRESSION> , where <COMPRESSION>  can be either  fast  for the least compression, or  best  for the most compression (the default\ncompression is between  best  and  fast ).   Use  wget  to download the following files from our web server:  Text file:  http://proxy.chtc.wisc.edu/SQUID/osgschool19/random_text  Archive:  http://proxy.chtc.wisc.edu/SQUID/osgschool19/pdbaa.tar.gz  Image:  http://proxy.chtc.wisc.edu/SQUID/osgschool19/obligatory_cat.jpg    Use  tar  on each file and use  ls -l  to compare the sizes of the original file and the compressed version.   Which files were compressed the least? Why?",
            "title": "Comparing compressed sizes"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/#transferring-files",
            "text": "",
            "title": "Transferring files"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/#using-secure-copy",
            "text": "Secure copy  ( scp ) is a command based on  SSH  that lets you securely copy\nfiles between two different hosts.\nIt takes similar arguments to the  cp  command that you are familiar with but also takes additional host information:  user@learn $  scp < source   1 > < source   2 >...< source  N> <remote host>:<remote path>  For example, if I were logged in to  learn.chtc.wisc.edu  and wanted to copy the file  foo  from my current directory to\nmy home directory on  training.osgconnect.net , the command would look like this:  user@learn $  scp foo training.osgconnect.net:~  Additionally, I could also pull files from  training.osgconnect.net  to  learn.chtc.wisc.edu .\nThe following command copies  bar  from my home directory on  training.osgconnect.net  to my current directory on learn.chtc.wisc.edu :  user@learn $  scp training.osgconnect.net:~/bar .  You can also copy folders between hosts using the  -r  option.\nIf I kept all my files from Monday's exercise 1.3 in a folder named  monday-1.3  on  learn.chtc.wisc.edu , I could use\nthe following command to copy them to my home directory on  training.osgconnect.net :  user@learn $  scp -r monday-1.3 training.osgconnect.net:~  Try copying the tarball you created earlier in this exercise on  learn.chtc.wisc.edu  to  training.osgconnect.net .",
            "title": "Using secure copy"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/#secure-copy-from-your-laptop",
            "text": "During your research, you may need to retrieve output files from your submit host to inspect them on your personal\nmachine, which can also be done with  scp ! To use  scp  on your laptop, follow the instructions relevant to your\nmachine's operating system:",
            "title": "Secure copy from your laptop"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/#mac-and-linux-users",
            "text": "scp  should be included by default and available via the terminal on both Mac and Linux operating systems.\nOpen a terminal window on your laptop and try copying the tarball containing Monday's 2.4 exercise from training.osgconnect.net  to your laptop.",
            "title": "Mac and Linux users"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/#windows-users",
            "text": "WinSCP is an  scp  client for Windows operating systems.   Install WinSCP from  https://winscp.net/eng/index.php  Start WinSCP and enter your SSH credentials for  training.osgconnect.net  Copy the tarball containing Monday's 2.4 exercise exercise to your laptop",
            "title": "Windows users"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/#extra-challenge-using-rsync",
            "text": "scp  is a great, ubiquitous tool for one-time transfers but there are better tools if you find yourself transferring\nthe same set of files to the same location repeatedly.\nAnother common tool available on many Linux machines is  rsync , which is like a beefed-up version of  scp .\nThe invocation is similar to  scp : you can transfer files and/or folders, but the options are different and when\ntransferring folders, make sure they don't have a trailing slash ( / , this means to copy all the files within the\nfolder instead of the folder itself):  user@learn $  rsync -Pavz < source   1 > < source   2 >...< source  N> <remote host>:<remote path>  rsync  has many benefits over  scp  but two of its biggest features are built-in compression (so you don't have to\ncreate a tarball) and the ability to only transfer files that have changed.\nBoth of these feature are helpful when you're having connectivity issues so that you don't have to restart the transfer\nfrom scratch every time your connection fails.   Use  rsync  to transfer the folder containing today's exercise 1.1 to  training.osgconnect.net   Create a new file in your exercise 1.1 folder on  learn.chtc.wisc.edu  with the  touch  command:  user@learn $  touch <filename>    Use the same  rsync  command to transfer the folder with the new file you just created.\n   How many files were transferred the first time? How many files were transferred if you run the same rsync command\n   again?",
            "title": "Extra challenge: Using rsync"
        },
        {
            "location": "/materials/day1/part4-ex2-login-scp/#next-exercise",
            "text": "Once completed, move onto the next exercise:  Running jobs in the OSG",
            "title": "Next exercise"
        },
        {
            "location": "/materials/day1/part4-ex3-submit-osg/",
            "text": "Monday Exercise 4.3: Running jobs in the OSG\n\u00b6\n\n\nThe goal of this exercise is to have your jobs running on the OSG and map their geographical locations.\n\n\nWhere in the world are my jobs? (Part 2)\n\u00b6\n\n\nIn this version of the geolocating exercise, you will submit jobs to the OSG from \ntraining.osgconnect.net\n and\nhopefully getting back much more interesting results!\nYou will be using the same exact payload as you did in \nexercise 4.1\n.\n\n\nGathering network information from the OSG\n\u00b6\n\n\nNow to create submit files that will run in the OSG!\n\n\n\n\nIf not already logged in, \nssh\n into \ntraining.osgconnect.net\n\n\nMake a new directory for this exercise, \ntuesday-4.3\n and change into it\n\n\nUse \nscp\n or \nrsync\n from \nexercise 4.2\n to copy over the executable and input\n   file from the \nmonday-4.1\n directory from \nlearn\n.\n\n\nRe-create the submit file from exercise 4.1 except this time around change your submit file so that it submits \nfive\n   hundred\n jobs!\n\n\nSubmit your file and wait for the results\n\n\n\n\nMapping your jobs\n\u00b6\n\n\nAs before, you will be using \nhttp://www.mapcustomizer.com/\n to visualize where your jobs have landed in the OSG.\nCopy and paste the collated results from your job output into the bulk creation text box at the bottom of the screen.\nWhere did your jobs end up?\n\n\nNext exercise\n\u00b6\n\n\nOnce completed, move onto the next exercise: \nHardware Differences in the OSG\n\n\nExtra Challenge: Cleaning up your submit directory\n\u00b6\n\n\nIf you run \nls\n in the directory from which you submitted your job, you may see that you now have thousands of files!\nProper data management starts to become a requirement as you start to develop truly HTC workflows;\nyou'll want organize your submit files, code, and input data separate from your output data.\n\n\n\n\n\n\nTry editing your submit file so that all your output and error files are saved to separate directories within your\n   submit directory.\n\n\n\n\nTip\n\n\nExperiment with fewer job submissions until you're confident you have it right, then go back to submitting 500\njobs!\n\n\n\n\n\n\n\n\nSubmit your file and track the status of your jobs.\n\n\n\n\n\n\nDid your jobs complete successfully with output and error files saved in separate directories?\nIf not, can you find any useful information in the job logs or hold messages?\nIf you get stuck, review the \nslides for submitting many jobs\n.",
            "title": "Exercise 4.3"
        },
        {
            "location": "/materials/day1/part4-ex3-submit-osg/#monday-exercise-43-running-jobs-in-the-osg",
            "text": "The goal of this exercise is to have your jobs running on the OSG and map their geographical locations.",
            "title": "Monday Exercise 4.3: Running jobs in the OSG"
        },
        {
            "location": "/materials/day1/part4-ex3-submit-osg/#where-in-the-world-are-my-jobs-part-2",
            "text": "In this version of the geolocating exercise, you will submit jobs to the OSG from  training.osgconnect.net  and\nhopefully getting back much more interesting results!\nYou will be using the same exact payload as you did in  exercise 4.1 .",
            "title": "Where in the world are my jobs? (Part 2)"
        },
        {
            "location": "/materials/day1/part4-ex3-submit-osg/#gathering-network-information-from-the-osg",
            "text": "Now to create submit files that will run in the OSG!   If not already logged in,  ssh  into  training.osgconnect.net  Make a new directory for this exercise,  tuesday-4.3  and change into it  Use  scp  or  rsync  from  exercise 4.2  to copy over the executable and input\n   file from the  monday-4.1  directory from  learn .  Re-create the submit file from exercise 4.1 except this time around change your submit file so that it submits  five\n   hundred  jobs!  Submit your file and wait for the results",
            "title": "Gathering network information from the OSG"
        },
        {
            "location": "/materials/day1/part4-ex3-submit-osg/#mapping-your-jobs",
            "text": "As before, you will be using  http://www.mapcustomizer.com/  to visualize where your jobs have landed in the OSG.\nCopy and paste the collated results from your job output into the bulk creation text box at the bottom of the screen.\nWhere did your jobs end up?",
            "title": "Mapping your jobs"
        },
        {
            "location": "/materials/day1/part4-ex3-submit-osg/#next-exercise",
            "text": "Once completed, move onto the next exercise:  Hardware Differences in the OSG",
            "title": "Next exercise"
        },
        {
            "location": "/materials/day1/part4-ex3-submit-osg/#extra-challenge-cleaning-up-your-submit-directory",
            "text": "If you run  ls  in the directory from which you submitted your job, you may see that you now have thousands of files!\nProper data management starts to become a requirement as you start to develop truly HTC workflows;\nyou'll want organize your submit files, code, and input data separate from your output data.    Try editing your submit file so that all your output and error files are saved to separate directories within your\n   submit directory.   Tip  Experiment with fewer job submissions until you're confident you have it right, then go back to submitting 500\njobs!     Submit your file and track the status of your jobs.    Did your jobs complete successfully with output and error files saved in separate directories?\nIf not, can you find any useful information in the job logs or hold messages?\nIf you get stuck, review the  slides for submitting many jobs .",
            "title": "Extra Challenge: Cleaning up your submit directory"
        },
        {
            "location": "/materials/day1/part4-ex4-hardware-diffs/",
            "text": "Monday Exercise 4.4: Hardware Differences in the OSG\n\u00b6\n\n\nThe goal of this exercise is to compare hardware differences between our local cluster (CHTC here at UW\u2013Madison) and an\nOSG glidein pool.\nSpecifically, we will look at how easy it is to get access to resources in terms of the amount of memory that is\nrequested.\nThis will not be a very careful study, but should give you some idea of one way in which the pools are different.\n\n\nIn the first two parts of the exercise, you will submit a bunch of jobs that differ only in how much memory each one\nrequests;\nwe call this a \nparameter sweep\n, in that we are testing many possible values of a parameter.\nWe will request memory from 8\u201364GB, doubling the memory each time.\nOne set of jobs will be submitted locally, and the other, identical set of jobs will be submitted to OSG.\nYou will check the queue periodically to see how many jobs have completed and how many are still waiting to run.\n\n\nChecking CHTC memory availability\n\u00b6\n\n\nIn this first part, you will create the submit file for both the local and OSG jobs, then submit the local set.\n\n\nYet another queue syntax\n\u00b6\n\n\nEarlier today, you learned about the \nqueue\n statement and some of the different ways it can be invoked to submit\nmultiple jobs.\nSimilar to the \nqueue from\n statement to submit jobs based on lines from a specific file, you can use \nqueue in\n to\nsubmit jobs based on a list directly from your submit file:\n\n\nqueue <# of jobs> <variable> in (\n<item 1>\n<item 2>\n<item 3>\n...\n)\n\n\n\n\n\nFor example, to submit 6 total jobs that sleep for \n5\n, \n5\n, \n10\n, \n10\n, \n15\n, and \n15\n seconds, you could write the\nfollowing submit file:\n\n\nexecutable = /bin/sleep\n\nqueue 2 arguments in (\n5\n10\n15\n)\n\n\n\n\n\nTry submitting this yourself and check the jobs that end up in the queue with \ncondor_q -nobatch\n.\n\n\nCreate the submit files\n\u00b6\n\n\nTo create our parameter sweep, we will create a \nnew\n submit file with multiple queue statements and change the value\nof our parameter (\nrequest_memory\n) for each batch of jobs.\n\n\n\n\nIf not already, log in to \nlearn.chtc.wisc.edu\n\n\nCreate and change into a new subdirectory called \nmonday-4.4\n\n\n\n\nCreate a submit file that is named \nsleep.sub\n that executes the command \n/bin/sleep 300\n.\n\n\n\n\nNote\n\n\nIf you do not remember all of the submit statements to write this file, or just to go faster, find a similar\nsubmit file from yesterday.\nCopy the file and rename it here, and make sure the argument to \nsleep\n is \n300\n.\n\n\n\n\n\n\n\n\nUse the \nqueue in\n syntax to submit 10 jobs each for the following memory requests: 8, 16, 32, and 64GB.\n    You should have 10 jobs requesting 8GB, 10 jobs requesting 16GB, etc.\n\n\n\n\nSave the submit file and exit your editor\n\n\nSubmit your jobs\n\n\n\n\nMonitoring the local jobs\n\u00b6\n\n\nEvery few minutes, run \ncondor_q\n and see how your sleep jobs are doing.\nTo easily see how many jobs of each type you have left, run the following command:\n\n\nuser@learn $\n condor_q <Cluster ID> -af RequestMemory \n|\n sort -n \n|\n uniq -c\n\n\n\n\n\nThe numbers in the left column are the number of jobs left of that type and the number on the right is the amount of\nmemory you requested in MB.\nConsider making a little table like the one below to track progress.\n\n\n\n\n\n\n\n\nMemory\n\n\nRemaining #1\n\n\nRemaining #2\n\n\nRemaining #3\n\n\n\n\n\n\n\n\n\n\n8 GB\n\n\n10\n\n\n6\n\n\n\n\n\n\n\n\n16 GB\n\n\n10\n\n\n7\n\n\n\n\n\n\n\n\n32 GB\n\n\n10\n\n\n8\n\n\n\n\n\n\n\n\n64 GB\n\n\n10\n\n\n9\n\n\n\n\n\n\n\n\n\n\nIn the meantime, between checking on your local jobs, start the next section \u2013 taking a break every few minutes to\nrecord progress on your local jobs.\n\n\nChecking OSG memory availability\n\u00b6\n\n\nFor the second part of the exercise, you will just copy over the directory from the \nabove section\n\non \nlearn.chtc.wisc.edu\n to \ntraining.osgconnect.net\n and resubmit your jobs to the OSG.\nIf you get stuck during the copying process, refer to \nexercise 4.2\n.\n\n\nMonitoring the remote jobs\n\u00b6\n\n\nAs you did in the first part, use \ncondor_q\n to track how your sleep jobs are doing.\nYou can move onto the next exercise but keep tracking the status of your jobs.\nAfter you are done with the \nnext exercise\n, come back to this exercise,\nand move onto analyzing the results.\n\n\nAnalyzing the results\n\u00b6\n\n\nNow that you've finished the other exercise, how many jobs have completed locally? How many have completed remotely?\n\n\nDue to the dynamic nature of the remote pool, the OSG may have noticed the demand for higher memory jobs and leased more\nhigh memory slots for our pool.\nThat being said, 64GB+ slots are a high-demand, low-availability resource in the OSG so it's unlikely that all of your\n64GB+ jobs matched and ran to completion, if any.\nOn the other hand, the local cluster has a fair number of 64GB+ slots so all your jobs have a high chance of running.",
            "title": "Exercise 4.4"
        },
        {
            "location": "/materials/day1/part4-ex4-hardware-diffs/#monday-exercise-44-hardware-differences-in-the-osg",
            "text": "The goal of this exercise is to compare hardware differences between our local cluster (CHTC here at UW\u2013Madison) and an\nOSG glidein pool.\nSpecifically, we will look at how easy it is to get access to resources in terms of the amount of memory that is\nrequested.\nThis will not be a very careful study, but should give you some idea of one way in which the pools are different.  In the first two parts of the exercise, you will submit a bunch of jobs that differ only in how much memory each one\nrequests;\nwe call this a  parameter sweep , in that we are testing many possible values of a parameter.\nWe will request memory from 8\u201364GB, doubling the memory each time.\nOne set of jobs will be submitted locally, and the other, identical set of jobs will be submitted to OSG.\nYou will check the queue periodically to see how many jobs have completed and how many are still waiting to run.",
            "title": "Monday Exercise 4.4: Hardware Differences in the OSG"
        },
        {
            "location": "/materials/day1/part4-ex4-hardware-diffs/#checking-chtc-memory-availability",
            "text": "In this first part, you will create the submit file for both the local and OSG jobs, then submit the local set.",
            "title": "Checking CHTC memory availability"
        },
        {
            "location": "/materials/day1/part4-ex4-hardware-diffs/#yet-another-queue-syntax",
            "text": "Earlier today, you learned about the  queue  statement and some of the different ways it can be invoked to submit\nmultiple jobs.\nSimilar to the  queue from  statement to submit jobs based on lines from a specific file, you can use  queue in  to\nsubmit jobs based on a list directly from your submit file:  queue <# of jobs> <variable> in (\n<item 1>\n<item 2>\n<item 3>\n...\n)  For example, to submit 6 total jobs that sleep for  5 ,  5 ,  10 ,  10 ,  15 , and  15  seconds, you could write the\nfollowing submit file:  executable = /bin/sleep\n\nqueue 2 arguments in (\n5\n10\n15\n)  Try submitting this yourself and check the jobs that end up in the queue with  condor_q -nobatch .",
            "title": "Yet another queue syntax"
        },
        {
            "location": "/materials/day1/part4-ex4-hardware-diffs/#create-the-submit-files",
            "text": "To create our parameter sweep, we will create a  new  submit file with multiple queue statements and change the value\nof our parameter ( request_memory ) for each batch of jobs.   If not already, log in to  learn.chtc.wisc.edu  Create and change into a new subdirectory called  monday-4.4   Create a submit file that is named  sleep.sub  that executes the command  /bin/sleep 300 .   Note  If you do not remember all of the submit statements to write this file, or just to go faster, find a similar\nsubmit file from yesterday.\nCopy the file and rename it here, and make sure the argument to  sleep  is  300 .     Use the  queue in  syntax to submit 10 jobs each for the following memory requests: 8, 16, 32, and 64GB.\n    You should have 10 jobs requesting 8GB, 10 jobs requesting 16GB, etc.   Save the submit file and exit your editor  Submit your jobs",
            "title": "Create the submit files"
        },
        {
            "location": "/materials/day1/part4-ex4-hardware-diffs/#monitoring-the-local-jobs",
            "text": "Every few minutes, run  condor_q  and see how your sleep jobs are doing.\nTo easily see how many jobs of each type you have left, run the following command:  user@learn $  condor_q <Cluster ID> -af RequestMemory  |  sort -n  |  uniq -c  The numbers in the left column are the number of jobs left of that type and the number on the right is the amount of\nmemory you requested in MB.\nConsider making a little table like the one below to track progress.     Memory  Remaining #1  Remaining #2  Remaining #3      8 GB  10  6     16 GB  10  7     32 GB  10  8     64 GB  10  9      In the meantime, between checking on your local jobs, start the next section \u2013 taking a break every few minutes to\nrecord progress on your local jobs.",
            "title": "Monitoring the local jobs"
        },
        {
            "location": "/materials/day1/part4-ex4-hardware-diffs/#checking-osg-memory-availability",
            "text": "For the second part of the exercise, you will just copy over the directory from the  above section \non  learn.chtc.wisc.edu  to  training.osgconnect.net  and resubmit your jobs to the OSG.\nIf you get stuck during the copying process, refer to  exercise 4.2 .",
            "title": "Checking OSG memory availability"
        },
        {
            "location": "/materials/day1/part4-ex4-hardware-diffs/#monitoring-the-remote-jobs",
            "text": "As you did in the first part, use  condor_q  to track how your sleep jobs are doing.\nYou can move onto the next exercise but keep tracking the status of your jobs.\nAfter you are done with the  next exercise , come back to this exercise,\nand move onto analyzing the results.",
            "title": "Monitoring the remote jobs"
        },
        {
            "location": "/materials/day1/part4-ex4-hardware-diffs/#analyzing-the-results",
            "text": "Now that you've finished the other exercise, how many jobs have completed locally? How many have completed remotely?  Due to the dynamic nature of the remote pool, the OSG may have noticed the demand for higher memory jobs and leased more\nhigh memory slots for our pool.\nThat being said, 64GB+ slots are a high-demand, low-availability resource in the OSG so it's unlikely that all of your\n64GB+ jobs matched and ran to completion, if any.\nOn the other hand, the local cluster has a fair number of 64GB+ slots so all your jobs have a high chance of running.",
            "title": "Analyzing the results"
        },
        {
            "location": "/materials/day1/part4-ex5-software-diffs/",
            "text": "Monday Exercise 4.5: Software Differences in the OSG\n\u00b6\n\n\nThe goal of this exercise is to see the differences in availability of software in the OSG.\nAt your local cluster, you may be used to having certain versions of software but out on the OSG, it's possible that the\nsoftware you need won't even be installed.\n\n\nRefresher - condor_status\n\u00b6\n\n\nThe OSG pool, like the local pool you used earlier today, is just another HTCondor pool.\nThis means that the commands you use will be the same and the jobs you submit can have similar payloads but there is one\nmajor difference: the slots are different!\nYou can use the \ncondor_status\n command just as you did yesterday to inspect these differences.\n\n\n\n\nOpen two terminal windows side-by-side\n\n\nLog in to \nlearn.chtc.wisc.edu\n in one window and \ntraining.osgconnect.net\n in the other\n\n\nRun \ncondor_status\n in both windows\n\n\n\n\n\n\nNote\n\n\nFor \ntraining.osgconnect.net\n you will need to add \n-pool flock.opensciencegrid.org\n to your \ncondor_status\n command.\n\n\n\n\nNotice any differences?\n\n\nComparing operating systems\n\u00b6\n\n\nTo really see differences between slots in the local cluster vs the OSG, you will want to compare the slot ClassAds\nbetween the two pools.\nRather than inspecting the very long ClassAd for each slot, you will look at a specific attribute called \nOpSysAndVer\n,\nwhich tells us the operating system version of the machine where a slot resides.\nAn easy way to show this attribute for all slots is by using \ncondor_status\n in conjunction with the \n-autoformat\n (or\n\n-af\n for short) option.\n\n-autoformat\n like the \n-format\n option you learned about earlier today will print out the attributes you're interested\nin for each slot but as you probably guessed, it does some automatic formatting for you.\nSo to show the operating system and version of each slot, run the following command in both of your terminal windows:\n\n\nuser@submit-host $\n condor_status -autoformat OpSysAndVer\n\n\n\n\n\nYou will see many values with the type of operating system at the front and the version number at the end (i.e. SL6\nstands for Scientific Linux 6).\nThe only problem is that with hundreds or thousands of slots, it's difficult to get a feel for the composition of each\npool from this output.\nYou can find a count for each operating system by passing the \ncondor_status\n output into the \nsort\n and \nuniq\n\ncommands.\nYour command line should look something like this:\n\n\nuser@learn $\n condor_status -autoformat OpSysAndVer \n|\n sort \n|\n uniq -c\n\n\n\n\n\nCan you spot the differences between the two pools now?\n\n\nSubmitting probe jobs\n\u00b6\n\n\nKnowing the type and version of the operating systems is a step in the right direction to knowing what kind of software\nwill be available on the machines that your jobs land on.\nHowever it still only serves as a proxy to the information that you really want: does the machine have the software that\nyou want?\nDoes it have the correct version?\n\n\nSoftware probe code\n\u00b6\n\n\nThe following shell script probes for software and returns the version if it is installed:\n\n\n#!/bin/sh\n\n\nget_version\n(){\n\n    \nprogram\n=\n$1\n\n    \n$program\n --version > /dev/null \n2\n>\n&\n1\n\n    \ndouble_dash_rc\n=\n$?\n\n    \n$program\n -version > /dev/null \n2\n>\n&\n1\n\n    \nsingle_dash_rc\n=\n$?\n\n    which \n$program\n > /dev/null \n2\n>\n&\n1\n\n    \nwhich_rc\n=\n$?\n\n    \nif\n \n[\n \n$double_dash_rc\n -eq \n0\n \n]\n;\n \nthen\n\n        \n$program\n --version \n2\n>\n&\n1\n\n    \nelif\n \n[\n \n$single_dash_rc\n -eq \n0\n \n]\n;\n \nthen\n\n        \n$program\n -version \n2\n>\n&\n1\n\n    \nelif\n \n[\n \n$which_rc\n -eq \n0\n \n]\n;\n \nthen\n\n        \necho\n \n\"\n$program\n installed but could not find version information\"\n\n    \nelse\n\n        \necho\n \n\"\n$program\n not installed\"\n\n    \nfi\n\n\n}\n\n\nget_version \n'R'\n\nget_version \n'cmake'\n\nget_version \n'python'\n\n\n\n\n\n\nIf there's a specific command line program that your research requires, feel free to add it to the script!\nFor example, if you wanted to test for the existence and version of \nnslookup\n, you would add the following to the end\nof the script:\n\n\nget_version 'nslookup'\n\n\n\n\n\nProbing several machines\n\u00b6\n\n\nFor this part of the exercise, try creating a submit file without referring to previous exercises!\n\n\n\n\nLog in to \ntraining.osgconnect.net\n\n\nCreate and change into a new folder for this exercise, e.g. \nmonday-4.5\n\n\nSave the above script as a file named \nsw_probe.sh\n\n\nCreate a submit file that runs \nsw_probe.sh\n 100 times and uses macros to write different \noutput\n, \nerror\n, and\n    \nlog\n files\n\n\nSubmit your job and wait for the results\n\n\n\n\nWill you be able to do your research on the OSG with what's available?\nDon't fret if it doesn't look like you can: over the next few days, you'll learn how to make your jobs portable enough\nso that they can run anywhere!",
            "title": "Exercise 4.5"
        },
        {
            "location": "/materials/day1/part4-ex5-software-diffs/#monday-exercise-45-software-differences-in-the-osg",
            "text": "The goal of this exercise is to see the differences in availability of software in the OSG.\nAt your local cluster, you may be used to having certain versions of software but out on the OSG, it's possible that the\nsoftware you need won't even be installed.",
            "title": "Monday Exercise 4.5: Software Differences in the OSG"
        },
        {
            "location": "/materials/day1/part4-ex5-software-diffs/#refresher-condor95status",
            "text": "The OSG pool, like the local pool you used earlier today, is just another HTCondor pool.\nThis means that the commands you use will be the same and the jobs you submit can have similar payloads but there is one\nmajor difference: the slots are different!\nYou can use the  condor_status  command just as you did yesterday to inspect these differences.   Open two terminal windows side-by-side  Log in to  learn.chtc.wisc.edu  in one window and  training.osgconnect.net  in the other  Run  condor_status  in both windows    Note  For  training.osgconnect.net  you will need to add  -pool flock.opensciencegrid.org  to your  condor_status  command.   Notice any differences?",
            "title": "Refresher - condor_status"
        },
        {
            "location": "/materials/day1/part4-ex5-software-diffs/#comparing-operating-systems",
            "text": "To really see differences between slots in the local cluster vs the OSG, you will want to compare the slot ClassAds\nbetween the two pools.\nRather than inspecting the very long ClassAd for each slot, you will look at a specific attribute called  OpSysAndVer ,\nwhich tells us the operating system version of the machine where a slot resides.\nAn easy way to show this attribute for all slots is by using  condor_status  in conjunction with the  -autoformat  (or -af  for short) option. -autoformat  like the  -format  option you learned about earlier today will print out the attributes you're interested\nin for each slot but as you probably guessed, it does some automatic formatting for you.\nSo to show the operating system and version of each slot, run the following command in both of your terminal windows:  user@submit-host $  condor_status -autoformat OpSysAndVer  You will see many values with the type of operating system at the front and the version number at the end (i.e. SL6\nstands for Scientific Linux 6).\nThe only problem is that with hundreds or thousands of slots, it's difficult to get a feel for the composition of each\npool from this output.\nYou can find a count for each operating system by passing the  condor_status  output into the  sort  and  uniq \ncommands.\nYour command line should look something like this:  user@learn $  condor_status -autoformat OpSysAndVer  |  sort  |  uniq -c  Can you spot the differences between the two pools now?",
            "title": "Comparing operating systems"
        },
        {
            "location": "/materials/day1/part4-ex5-software-diffs/#submitting-probe-jobs",
            "text": "Knowing the type and version of the operating systems is a step in the right direction to knowing what kind of software\nwill be available on the machines that your jobs land on.\nHowever it still only serves as a proxy to the information that you really want: does the machine have the software that\nyou want?\nDoes it have the correct version?",
            "title": "Submitting probe jobs"
        },
        {
            "location": "/materials/day1/part4-ex5-software-diffs/#software-probe-code",
            "text": "The following shell script probes for software and returns the version if it is installed:  #!/bin/sh \n\nget_version (){ \n     program = $1 \n     $program  --version > /dev/null  2 > & 1 \n     double_dash_rc = $? \n     $program  -version > /dev/null  2 > & 1 \n     single_dash_rc = $? \n    which  $program  > /dev/null  2 > & 1 \n     which_rc = $? \n     if   [   $double_dash_rc  -eq  0   ] ;   then \n         $program  --version  2 > & 1 \n     elif   [   $single_dash_rc  -eq  0   ] ;   then \n         $program  -version  2 > & 1 \n     elif   [   $which_rc  -eq  0   ] ;   then \n         echo   \" $program  installed but could not find version information\" \n     else \n         echo   \" $program  not installed\" \n     fi  } \n\nget_version  'R' \nget_version  'cmake' \nget_version  'python'   If there's a specific command line program that your research requires, feel free to add it to the script!\nFor example, if you wanted to test for the existence and version of  nslookup , you would add the following to the end\nof the script:  get_version 'nslookup'",
            "title": "Software probe code"
        },
        {
            "location": "/materials/day1/part4-ex5-software-diffs/#probing-several-machines",
            "text": "For this part of the exercise, try creating a submit file without referring to previous exercises!   Log in to  training.osgconnect.net  Create and change into a new folder for this exercise, e.g.  monday-4.5  Save the above script as a file named  sw_probe.sh  Create a submit file that runs  sw_probe.sh  100 times and uses macros to write different  output ,  error , and\n     log  files  Submit your job and wait for the results   Will you be able to do your research on the OSG with what's available?\nDon't fret if it doesn't look like you can: over the next few days, you'll learn how to make your jobs portable enough\nso that they can run anywhere!",
            "title": "Probing several machines"
        },
        {
            "location": "/materials/day2/part1-ex1-troubleshooting/",
            "text": "Tuesday Exercise 1.1: Troubleshooting Jobs\n\u00b6\n\n\nThe goal of this exercise is to troubleshoot some common problems that you may encounter when submitting jobs using HTCondor.\nThis exercise will likely take you longer than the allotted time;\ndon't fret, an answer key is available and office hours the rest of the week, so work at your own pace.\n\n\nAcquiring the Materials\n\u00b6\n\n\nThe materials for this exercise are located on our web server.\n\n\n\n\nLog into \nlearn.chtc.wisc.edu\n\n\n\n\nUse \nwget\n to retrieve the materials from the web server:\n\n\nuser@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/tues-part1-ex1.tar.gz\n\n\n\n\n\n\n\n\n\nExtract the tarball using the commands that you learned yesterday\n\n\n\n\nChange into the directory extracted from the tarball and explore its contents\n\n\n\n\nSolving a Project Euler Problem\n\u00b6\n\n\nThe contents of the tarball that you've extracted contain a series of submit files, Python scripts, and an input file \nthat are designed to solve \nProject Euler problem 98\n:\n\n\n\n\nBy replacing each of the letters in the word CARE with 1, 2, 9, and 6 respectively, we form a square number: 1296 =\n36^2. What is remarkable is that, by using the same digital substitutions, the anagram, RACE, also forms a square\nnumber: 9216 = 96^2. We shall call CARE (and RACE) a square anagram word pair and specify further that leading zeroes\nare not permitted, neither may a different letter have the same digital value as another letter.\n\n\nUsing p098_words.txt, a 16K text file containing nearly two-thousand common English words, find all the square\nanagram word pairs (a palindromic word is NOT considered to be an anagram of itself).\n\n\nWhat is the largest square number formed by any member of such a pair?\n\n\nNOTE:\n All anagrams formed must be contained in the given text file.\n\n\n\n\nUnfortunately, there are many issues with the submit files that you will have to work through before you can you can\nobtain the solution to the problem!\nThe code in the Python scripts themselves should be bug-free.\n\n\nFinding anagrams\n\u00b6\n\n\nThe first step in our workflow takes an input file with a list of words (\np098_words.txt\n) and extracts all of the\nanagrams using the \nfind_anagrams.py\n script.\nNaturally, we want to run this as an HTCondor job, so \n\n\n\n\nSubmit the accompanying \nfind-anagrams.sub\n file from the tarball.\n   Try to do this step without looking at materials from yesterday.\n   But if you are stuck, see \nyesterday\u2019s exercise 2.2\n.\n\n\nResolve any issues that you encounter until the job returns pairs of anagrams as its output.\n\n\n\n\nOnce you have satisfactory output, move onto the next section.\n\n\n\n\nPlease be polite\n\n\nSubmit hosts are shared resources, so you should clean up after yourself.\nAfter you're done troubleshooting held jobs, remove them with the following command:\n\n\nuser@learn $\n condor_rm -const \n'JobStatus =?= 5'\n <JOB FILTER>\n\n\n\n\n\n\n\n\n\n\n\nWhere replacing \n<JOB FILTER>\n with...\n\n\nWill remove...\n\n\n\n\n\n\n\n\n\n\nYour username (e.g. \nblin\n)\n\n\nAll of your held jobs\n\n\n\n\n\n\nA cluster ID (e.g. \n74078\n)\n\n\nAll held jobs matching the given cluster ID\n\n\n\n\n\n\nA job ID (e.g. \n97932.30\n)\n\n\nThat specific held job\n\n\n\n\n\n\n\n\n\n\nFinding the largest square\n\u00b6\n\n\nThe next step of the workflow uses the \nmax_square.py\n script to find the largest square number, if any, for a given\nanagram word pair.\nLet's submit jobs that runs \nmax_square.py\n for all of the anagram word pairs (i.e. one job per word pair) that you\nfound in the previous section:\n\n\n\n\nSubmit the accompanying \nsquares.sub\n file from the tarball\n\n\nResolve any issues that you encounter until you receive output for each job.\n   Note that some jobs may have empty output since not all anagram word pairs are \nsquare\n anagram word pairs.\n\n\n\n\nNext, you can find the largest square among your output by directly using the command line.\nFor example, if all of your job output has been placed in the \nsquares\n directory and are named \nsquare-1.out\n,\n\nsquare-2.out\n, etc. then you could run the following command to find the largest square:\n\n\nuser@learn $\n cat squares/square-*.out \n|\n sort -n \n|\n tail - \n1\n\n\n\n\n\n\nYou can check if you have the right answer with any of the OSG staff or by submitting the answer to Project Euler\n(requires an account).\n\n\nAnswer Key\n\u00b6\n\n\nThere is also a working solution on our web server that can be retrieved with\n\n\nuser@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/tues-part1-ex1-key.tar.gz\n\n\n\n\n\nIt contains comments labeled \nSOLUTION\n that you can consult in case you get stuck.\nLike any answer key, it's mainly useful as a verification tool, so try to only use it as a last resort or for detailed\nexplanations to improve your understanding.",
            "title": "Exercise 1.1"
        },
        {
            "location": "/materials/day2/part1-ex1-troubleshooting/#tuesday-exercise-11-troubleshooting-jobs",
            "text": "The goal of this exercise is to troubleshoot some common problems that you may encounter when submitting jobs using HTCondor.\nThis exercise will likely take you longer than the allotted time;\ndon't fret, an answer key is available and office hours the rest of the week, so work at your own pace.",
            "title": "Tuesday Exercise 1.1: Troubleshooting Jobs"
        },
        {
            "location": "/materials/day2/part1-ex1-troubleshooting/#acquiring-the-materials",
            "text": "The materials for this exercise are located on our web server.   Log into  learn.chtc.wisc.edu   Use  wget  to retrieve the materials from the web server:  user@learn $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/tues-part1-ex1.tar.gz    Extract the tarball using the commands that you learned yesterday   Change into the directory extracted from the tarball and explore its contents",
            "title": "Acquiring the Materials"
        },
        {
            "location": "/materials/day2/part1-ex1-troubleshooting/#solving-a-project-euler-problem",
            "text": "The contents of the tarball that you've extracted contain a series of submit files, Python scripts, and an input file \nthat are designed to solve  Project Euler problem 98 :   By replacing each of the letters in the word CARE with 1, 2, 9, and 6 respectively, we form a square number: 1296 =\n36^2. What is remarkable is that, by using the same digital substitutions, the anagram, RACE, also forms a square\nnumber: 9216 = 96^2. We shall call CARE (and RACE) a square anagram word pair and specify further that leading zeroes\nare not permitted, neither may a different letter have the same digital value as another letter.  Using p098_words.txt, a 16K text file containing nearly two-thousand common English words, find all the square\nanagram word pairs (a palindromic word is NOT considered to be an anagram of itself).  What is the largest square number formed by any member of such a pair?  NOTE:  All anagrams formed must be contained in the given text file.   Unfortunately, there are many issues with the submit files that you will have to work through before you can you can\nobtain the solution to the problem!\nThe code in the Python scripts themselves should be bug-free.",
            "title": "Solving a Project Euler Problem"
        },
        {
            "location": "/materials/day2/part1-ex1-troubleshooting/#finding-anagrams",
            "text": "The first step in our workflow takes an input file with a list of words ( p098_words.txt ) and extracts all of the\nanagrams using the  find_anagrams.py  script.\nNaturally, we want to run this as an HTCondor job, so    Submit the accompanying  find-anagrams.sub  file from the tarball.\n   Try to do this step without looking at materials from yesterday.\n   But if you are stuck, see  yesterday\u2019s exercise 2.2 .  Resolve any issues that you encounter until the job returns pairs of anagrams as its output.   Once you have satisfactory output, move onto the next section.   Please be polite  Submit hosts are shared resources, so you should clean up after yourself.\nAfter you're done troubleshooting held jobs, remove them with the following command:  user@learn $  condor_rm -const  'JobStatus =?= 5'  <JOB FILTER>     Where replacing  <JOB FILTER>  with...  Will remove...      Your username (e.g.  blin )  All of your held jobs    A cluster ID (e.g.  74078 )  All held jobs matching the given cluster ID    A job ID (e.g.  97932.30 )  That specific held job",
            "title": "Finding anagrams"
        },
        {
            "location": "/materials/day2/part1-ex1-troubleshooting/#finding-the-largest-square",
            "text": "The next step of the workflow uses the  max_square.py  script to find the largest square number, if any, for a given\nanagram word pair.\nLet's submit jobs that runs  max_square.py  for all of the anagram word pairs (i.e. one job per word pair) that you\nfound in the previous section:   Submit the accompanying  squares.sub  file from the tarball  Resolve any issues that you encounter until you receive output for each job.\n   Note that some jobs may have empty output since not all anagram word pairs are  square  anagram word pairs.   Next, you can find the largest square among your output by directly using the command line.\nFor example, if all of your job output has been placed in the  squares  directory and are named  square-1.out , square-2.out , etc. then you could run the following command to find the largest square:  user@learn $  cat squares/square-*.out  |  sort -n  |  tail -  1   You can check if you have the right answer with any of the OSG staff or by submitting the answer to Project Euler\n(requires an account).",
            "title": "Finding the largest square"
        },
        {
            "location": "/materials/day2/part1-ex1-troubleshooting/#answer-key",
            "text": "There is also a working solution on our web server that can be retrieved with  user@learn $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/tues-part1-ex1-key.tar.gz  It contains comments labeled  SOLUTION  that you can consult in case you get stuck.\nLike any answer key, it's mainly useful as a verification tool, so try to only use it as a last resort or for detailed\nexplanations to improve your understanding.",
            "title": "Answer Key"
        },
        {
            "location": "/materials/day2/part3-ex1-compiling/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nTuesday Exercise 3.1: Compiling Programs for Portability\n\u00b6\n\n\nThe goal of this exercise is to compile and statically link a piece of code and then submit it as a job. This exercise should take 5-10 minutes.\n\n\nBackground\n\u00b6\n\n\nThere is a large amount of scientific software that is available as source code. Source code is usually a group of text files (code) meant to be downloaded and then compiled into a \nbinary file\n which a computer can understand. Sometimes the source code depends on other pieces of code called libraries. If the source code is linked \nstatically\n, these libraries are bundled into the compilation with the source code, creating a \nstatic binary\n which can be run on any computer with the same operating system.\n\n\nOur Software Example\n\u00b6\n\n\nFor this compiling example, we will use a script written in C. C code depends on libraries and therefore will benefit from being statically linked.\n\n\nOur C code prints 7 rows of Pascal's triangle.\n\n\n\n\nLog into the OSG submit node \ntraining.osgconnect.net\n. Create a directory for this exercise and \ncd\n into it.\n\n\nCopy and paste the following code into a file named \npascal.c\n.\n#include\n \n\"stdio.h\"\n\n\n\nlong\n \nfactorial\n(\nint\n);\n\n\n\nint\n \nmain\n()\n\n\n{\n\n\nint\n \ni\n,\n \nn\n,\n \nc\n;\n\n\nn\n=\n7\n;\n\n\nfor\n \n(\ni\n \n=\n \n0\n;\n \ni\n \n<\n \nn\n;\n \ni\n++\n){\n\n  \nfor\n \n(\nc\n \n=\n \n0\n;\n \nc\n \n<=\n \n(\nn\n \n-\n \ni\n \n-\n \n2\n);\n \nc\n++\n)\n\n  \nprintf\n(\n\" \"\n);\n\n      \nfor\n \n(\nc\n \n=\n \n0\n \n;\n \nc\n \n<=\n \ni\n;\n \nc\n++\n)\n\n         \nprintf\n(\n\"%ld \"\n,\nfactorial\n(\ni\n)\n/\n(\nfactorial\n(\nc\n)\n*\nfactorial\n(\ni\n-\nc\n)));\n\n      \nprintf\n(\n\"\n\\n\n\"\n);\n\n   \n}\n\n   \nreturn\n \n0\n;\n\n\n}\n\n\n\nlong\n \nfactorial\n(\nint\n \nn\n)\n\n\n{\n\n   \nint\n \nc\n;\n\n   \nlong\n \nresult\n \n=\n \n1\n;\n\n   \nfor\n \n(\nc\n \n=\n \n1\n;\n \nc\n \n<=\n \nn\n;\n \nc\n++\n)\n\n         \nresult\n \n=\n \nresult\n*\nc\n;\n\n   \nreturn\n \nresult\n;\n\n\n}\n\n\n\n\n\n\n\n\n\n\nCompiling\n\u00b6\n\n\nIn order to use this code in a job, we will first need to statically compile the code. Recall the slide from the lecture - where \ncan\n we compile and where \nshould\n we compile? In particular:\n\n\n\n\nWhere is the compiler available?\n\n\nHow computationally intensive will this compilation be?    \n\n\n\n\n\n\n\n\n\n\n\nThink about these questions before moving on. Where do you think we should compile?\n\n\n\n\n\n\nMost linux servers (including our submit node) have the \ngcc\n (GNU compiler collection) installed, so we already have a compiler on the submit node. Furthermore, this is a simple piece of C code, so the compilation will not be computationally intensive. Thus, we should be able to compile directly on the submit node. \n\n\n\n\n\n\nCompile the code, using the command: \n\n\nusername@training $\n gcc -static pascal.c -o pascal\n\n\n\n\n\nNote that we have added the \n-static\n option to make sure that the compiled binary includes the necessary libraries. This will allow the code to run on any Linux machine, no matter where those libraries are located. \n\n\n\n\n\n\nVerify that the compiled binary was statically linked:\n\n\nusername@training $\n file pascal\n\n\n\n\n\n\n\n\n\nThe Linux \nfile\n command provides information about the \ntype\n or \nkind\n of file that is given as an argument. In this case, you should get output like this:\n\n\nusername@host $\n file pascal\n\npascal: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), \nstatically linked\n,\n\n\nfor GNU/Linux 2.6.18, not stripped\n\n\n\n\n\n\nNote the blue text, which clearly states that this executable (software) is statically linked. The same command run on a non-statically linked executable file would include the text \ndynamically linked (uses shared libs)\n instead. So with this simple verification step, which could even be run on files that you did not compile yourself, you have some further reassurance that it is safe to use on other Linux machines. (Bonus exercise: Try the \nfile\n command on lots of other files)\n\n\nSubmit the Job\n\u00b6\n\n\nNow that our code is compiled, we can use it to submit a job.\n\n\n\n\n\n\nThink about what submit file lines we need to use to run this job:\n\n\n\n\nAre there input files?\n\n\nAre there command line arguments?\n\n\nWhere is its output written?\n\n\n\n\n\n\n\n\nBased on what you thought about in 1., find a submit file from earlier in the week that you can modify to run our compiled \npascal\n code.\n\n\n\n\n\n\nCopy it to the directory with the \npascal\n binary and make those changes. \n\n\n\n\n\n\nSubmit the job using \ncondor_submit\n. \n\n\n\n\n\n\nOnce the job has run and left the queue, you should be able to see the results (seven rows of Pascal's triangle) in the \n.out\n file created by the job.",
            "title": "Exercise 3.1"
        },
        {
            "location": "/materials/day2/part3-ex1-compiling/#tuesday-exercise-31-compiling-programs-for-portability",
            "text": "The goal of this exercise is to compile and statically link a piece of code and then submit it as a job. This exercise should take 5-10 minutes.",
            "title": "Tuesday Exercise 3.1: Compiling Programs for Portability"
        },
        {
            "location": "/materials/day2/part3-ex1-compiling/#background",
            "text": "There is a large amount of scientific software that is available as source code. Source code is usually a group of text files (code) meant to be downloaded and then compiled into a  binary file  which a computer can understand. Sometimes the source code depends on other pieces of code called libraries. If the source code is linked  statically , these libraries are bundled into the compilation with the source code, creating a  static binary  which can be run on any computer with the same operating system.",
            "title": "Background"
        },
        {
            "location": "/materials/day2/part3-ex1-compiling/#our-software-example",
            "text": "For this compiling example, we will use a script written in C. C code depends on libraries and therefore will benefit from being statically linked.  Our C code prints 7 rows of Pascal's triangle.   Log into the OSG submit node  training.osgconnect.net . Create a directory for this exercise and  cd  into it.  Copy and paste the following code into a file named  pascal.c . #include   \"stdio.h\"  long   factorial ( int );  int   main ()  {  int   i ,   n ,   c ;  n = 7 ;  for   ( i   =   0 ;   i   <   n ;   i ++ ){ \n   for   ( c   =   0 ;   c   <=   ( n   -   i   -   2 );   c ++ ) \n   printf ( \" \" ); \n       for   ( c   =   0   ;   c   <=   i ;   c ++ ) \n          printf ( \"%ld \" , factorial ( i ) / ( factorial ( c ) * factorial ( i - c ))); \n       printf ( \" \\n \" ); \n    } \n    return   0 ;  }  long   factorial ( int   n )  { \n    int   c ; \n    long   result   =   1 ; \n    for   ( c   =   1 ;   c   <=   n ;   c ++ ) \n          result   =   result * c ; \n    return   result ;  }",
            "title": "Our Software Example"
        },
        {
            "location": "/materials/day2/part3-ex1-compiling/#compiling",
            "text": "In order to use this code in a job, we will first need to statically compile the code. Recall the slide from the lecture - where  can  we compile and where  should  we compile? In particular:   Where is the compiler available?  How computationally intensive will this compilation be?          Think about these questions before moving on. Where do you think we should compile?    Most linux servers (including our submit node) have the  gcc  (GNU compiler collection) installed, so we already have a compiler on the submit node. Furthermore, this is a simple piece of C code, so the compilation will not be computationally intensive. Thus, we should be able to compile directly on the submit node.     Compile the code, using the command:   username@training $  gcc -static pascal.c -o pascal  Note that we have added the  -static  option to make sure that the compiled binary includes the necessary libraries. This will allow the code to run on any Linux machine, no matter where those libraries are located.     Verify that the compiled binary was statically linked:  username@training $  file pascal    The Linux  file  command provides information about the  type  or  kind  of file that is given as an argument. In this case, you should get output like this:  username@host $  file pascal pascal: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux),  statically linked ,  for GNU/Linux 2.6.18, not stripped   Note the blue text, which clearly states that this executable (software) is statically linked. The same command run on a non-statically linked executable file would include the text  dynamically linked (uses shared libs)  instead. So with this simple verification step, which could even be run on files that you did not compile yourself, you have some further reassurance that it is safe to use on other Linux machines. (Bonus exercise: Try the  file  command on lots of other files)",
            "title": "Compiling"
        },
        {
            "location": "/materials/day2/part3-ex1-compiling/#submit-the-job",
            "text": "Now that our code is compiled, we can use it to submit a job.    Think about what submit file lines we need to use to run this job:   Are there input files?  Are there command line arguments?  Where is its output written?     Based on what you thought about in 1., find a submit file from earlier in the week that you can modify to run our compiled  pascal  code.    Copy it to the directory with the  pascal  binary and make those changes.     Submit the job using  condor_submit .     Once the job has run and left the queue, you should be able to see the results (seven rows of Pascal's triangle) in the  .out  file created by the job.",
            "title": "Submit the Job"
        },
        {
            "location": "/materials/day2/part3-ex2-precompiled/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nTuesday Exercise 3.2: Using a Pre-compiled Binary\n\u00b6\n\n\nIn this exercise, we will run a job using a downloaded, pre-compiled binary. This exercise should take 10-15 minutes.\n\n\nBackground\n\u00b6\n\n\nIn the previous exercise, we used a piece of software that was available as source code, and could be compiled to a static binary. However, software is available in many different forms. You may also encounter scientific software provided as a static binary. Here, the software provider has compiled the software for you (typically on several operating systems). Using a pre-compiled binary means you can avoid compiling the code yourself; accessing the software and getting it ready to run is as simple as downloading the binary.\n\n\nOur Software Example\n\u00b6\n\n\nThe software we will be using for this example is a common tool for aligning genome and protein sequences against a\nreference database, the BLAST program.\n\n\n\n\n\n\nSearch the internet for the BLAST software.  Searches might include \"blast executable or \"download blast software\".  Hopefully these searches will lead you to a BLAST website page that looks like this:\n\n\n\n\n\n\n\n\nClick on the title that says \"Download BLAST\" and then look for the link that has the latest installation and source code.  You should end up on a page with a list of each version of BLAST that is available for different operating systems.\n\n\n\n\n\n\nWe could download the source and compile it ourselves, but instead, we're going to use one of the pre-built binaries.  Before proceeding, look at the list of downloads and try to determine which one you want. \n\n\n\n\n\n\nBased on our operating system, we want to use the Linux binary, which is labelled with the \nx64-linux\n suffix. \n\n\n\n\nAll the other links are either for source code or other operating systems. \n\n\n\n\n\n\nWhile logged into \ntraining.osgconnect.net\n, create a directory for this exercise. Then download the appropriate \ntar.gz\n file and un-tar it. You can download the file directly from the BLAST website using \nwget\n or download our local copy with the command below: \n\n\nuser@training $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/ncbi-blast-2.9.0+-x64-linux.tar.gz\n\nuser@training $\n tar -xzf ncbi-blast-2.9.0+-x64-linux.tar.gz\n\n\n\n\n\n\n\n\n\nWe're going to be using the \nblastx\n binary in our job. Where is it in the directory you just downloaded?\n\n\n\n\n\n\nCopy the Input Files\n\u00b6\n\n\nTo run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information.\n\n\n\n\n\n\nDownload these files to your current directory: \n\n\nusername@training $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/pdbaa.tar.gz\n\nusername@training $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/mouse.fa\n\n\n\n\n\n\n\n\n\nUntar the \npdbaa\n database: \n\n\nusername@training $\n tar -xzf pdbaa.tar.gz\n\n\n\n\n\n\n\n\n\nSubmitting the Job\n\u00b6\n\n\nWe now have our program (the pre-compiled \nblastx\n binary) and our input files, so all that remains is to create the submit file. A typical \nblastx\n command looks something like this:\n\n\nblastx -db database -query input_file -out results.txt\n\n\n\n\n\n\n\n\n\nCopy the submit file from the last exercise into your current directory. \n\n\n\n\n\n\nThink about which lines you will need to change or add to your submit file in order to submit the job successfully. In particular:    \n\n\n\n\nWhat is the executable?\n\n\nHow can you indicate the entire command line sequence above?\n\n\nWhich files need to be transferred in addition to the executable?\n\n\nDoes this job require a certain type of operating system?\n\n\n\n\n\n\n\n\nTry to answer these questions and modify your submit file appropriately.\n\n\n\n\n\n\nOnce you have done all you can, check your submit file against the lines below, which contain the necessary changes to run this particular job.\n\n\n\n\n\n\nThe executable is \nblastx\n, which is located in the \nbin\n directory of our downloaded BLAST directory. We need to use the \narguments\n line in the submit file to express the rest of the command. \n\n\nexecutable = ncbi-blast-2.9.0+/bin/blastx\narguments = -db pdbaa/pdbaa -query mouse.fa -out results.txt\n\n\n\n\n\n\n\n\n\nThe BLAST program requires our input file and database, so they must be transferred with \ntransfer_input_files\n. \n\n\ntransfer_input_files = pdbaa, mouse.fa\n\n\n\n\n\n\n\n\n\nBecause we downloaded a Linux-specific binary, we need to request machines that are running Linux. \n\n\nrequirements = (OpSys == \"LINUX\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit the blast job using \ncondor_submit\n. Once the job starts, it should run in just a few minutes and produce a file called \nresults.txt\n.",
            "title": "Exercise 3.2"
        },
        {
            "location": "/materials/day2/part3-ex2-precompiled/#tuesday-exercise-32-using-a-pre-compiled-binary",
            "text": "In this exercise, we will run a job using a downloaded, pre-compiled binary. This exercise should take 10-15 minutes.",
            "title": "Tuesday Exercise 3.2: Using a Pre-compiled Binary"
        },
        {
            "location": "/materials/day2/part3-ex2-precompiled/#background",
            "text": "In the previous exercise, we used a piece of software that was available as source code, and could be compiled to a static binary. However, software is available in many different forms. You may also encounter scientific software provided as a static binary. Here, the software provider has compiled the software for you (typically on several operating systems). Using a pre-compiled binary means you can avoid compiling the code yourself; accessing the software and getting it ready to run is as simple as downloading the binary.",
            "title": "Background"
        },
        {
            "location": "/materials/day2/part3-ex2-precompiled/#our-software-example",
            "text": "The software we will be using for this example is a common tool for aligning genome and protein sequences against a\nreference database, the BLAST program.    Search the internet for the BLAST software.  Searches might include \"blast executable or \"download blast software\".  Hopefully these searches will lead you to a BLAST website page that looks like this:     Click on the title that says \"Download BLAST\" and then look for the link that has the latest installation and source code.  You should end up on a page with a list of each version of BLAST that is available for different operating systems.    We could download the source and compile it ourselves, but instead, we're going to use one of the pre-built binaries.  Before proceeding, look at the list of downloads and try to determine which one you want.     Based on our operating system, we want to use the Linux binary, which is labelled with the  x64-linux  suffix.    All the other links are either for source code or other operating systems.     While logged into  training.osgconnect.net , create a directory for this exercise. Then download the appropriate  tar.gz  file and un-tar it. You can download the file directly from the BLAST website using  wget  or download our local copy with the command below:   user@training $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/ncbi-blast-2.9.0+-x64-linux.tar.gz user@training $  tar -xzf ncbi-blast-2.9.0+-x64-linux.tar.gz    We're going to be using the  blastx  binary in our job. Where is it in the directory you just downloaded?",
            "title": "Our Software Example"
        },
        {
            "location": "/materials/day2/part3-ex2-precompiled/#copy-the-input-files",
            "text": "To run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information.    Download these files to your current directory:   username@training $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/pdbaa.tar.gz username@training $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/mouse.fa    Untar the  pdbaa  database:   username@training $  tar -xzf pdbaa.tar.gz",
            "title": "Copy the Input Files"
        },
        {
            "location": "/materials/day2/part3-ex2-precompiled/#submitting-the-job",
            "text": "We now have our program (the pre-compiled  blastx  binary) and our input files, so all that remains is to create the submit file. A typical  blastx  command looks something like this:  blastx -db database -query input_file -out results.txt    Copy the submit file from the last exercise into your current directory.     Think about which lines you will need to change or add to your submit file in order to submit the job successfully. In particular:       What is the executable?  How can you indicate the entire command line sequence above?  Which files need to be transferred in addition to the executable?  Does this job require a certain type of operating system?     Try to answer these questions and modify your submit file appropriately.    Once you have done all you can, check your submit file against the lines below, which contain the necessary changes to run this particular job.    The executable is  blastx , which is located in the  bin  directory of our downloaded BLAST directory. We need to use the  arguments  line in the submit file to express the rest of the command.   executable = ncbi-blast-2.9.0+/bin/blastx\narguments = -db pdbaa/pdbaa -query mouse.fa -out results.txt    The BLAST program requires our input file and database, so they must be transferred with  transfer_input_files .   transfer_input_files = pdbaa, mouse.fa    Because we downloaded a Linux-specific binary, we need to request machines that are running Linux.   requirements = (OpSys == \"LINUX\")      Submit the blast job using  condor_submit . Once the job starts, it should run in just a few minutes and produce a file called  results.txt .",
            "title": "Submitting the Job"
        },
        {
            "location": "/materials/day2/part3-ex3-wrapper/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } \n\n\n\nTuesday Exercise 3.3: Using Wrapper Scripts to Submit Jobs\n\u00b6\n\n\nIn this exercise, you will create a wrapper script to run the same program (\nblastx\n) as the \nprevious exercise\n.\n\n\nBackground\n\u00b6\n\n\nWrapper scripts are a useful tool for running software that can't be compiled into one piece, needs to be installed with every job, or just for running extra steps.  A wrapper script can either install the software from the source code, or use an already existing software (as in this exercise). Not only does this portability technique work with almost any kind of software that can be locally installed, it also allows for a great deal of control and flexibility for what happens within your job. Once you can write a script to handle your software (and often your data as well), you can submit a large variety of workflows to a distributed computing system like the Open Science Grid.\n\n\nWrapper Script, part 1\n\u00b6\n\n\nOur wrapper script will be a bash script that runs several commands.\n\n\n\n\n\n\nIn the same directory as the last exercise (still logged into \ntraining.osgconnect.net.chtc.wisc.edu\n) make a file called \nrun_blast.sh\n. \n\n\n\n\n\n\nThe first line we'll place in the script is the basic command for running blast. Based on our previous submit file, what command needs to go into the script? Once you have an idea, check against the example below:  \n\n\n#!/bin/bash\n\n\nncbi-blast-2.9.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results.txt\n\n\n\n\n\n\n\nNote\n\n\nThe \"header\" of \n#!/bin/bash\n will tell the computer that this is a bash shell script and can be run in the same way that  you would run individual commands on the command line.\n\n\n\n\n\n\n\n\nSubmit File Changes\n\u00b6\n\n\nWe now need to make some changes to our submit file.\n\n\n\n\n\n\nMake a copy of your previous submit file and open it. \n\n\n\n\n\n\nSince we are now using a wrapper script, that will be our job's executable. Replace the original \nblastx\n exeuctable with the name of our wrapper script and comment out the arguments line.  \n\n\nexecutable = run_blast.sh \n#arguments =\n\n\n\n\n\n\n\n\n\nNote that since the \nblastx\n program is no longer listed as the executable, it will be need to be included in \ntransfer_input_files\n. Instead of transferring just that program, we will transfer the original downloaded \ntar.gz\n file.  \n\n\ntransfer_input_files = pdbaa, mouse.fa, \nncbi-blast-2.9.0+-x64-linux.tar.gz\n\n\n\n\n\n\n\n\n\n\nTo achieve efficiency, we'll also transfer the pdbaa database as the original \ntar.gz\n file instead of as the unzipped folder: \n\n\ntransfer_input_files = \npdbaa.tar.gz\n, mouse.fa, ncbi-blast-2.9.0+-x64-linux.tar.gz\n\n\n\n\n\n\n\n\n\nWrapper Script, part 2\n\u00b6\n\n\nNow that our database and BLAST software are being transferred to the job as \ntar.gz\n files, our script needs to accommodate.\n\n\n\n\n\n\nOpening your \nrun_blast.sh\n script, add two commands at the start to un-tar the BLAST and pdbaa \ntar.gz\n files. See the \nprevious exercise\n if you're not sure what this command looks like. \n\n\n\n\n\n\nIn order to distinguish this job from our previous job, change the output file name to something besides \nresults.txt\n. \n\n\n\n\n\n\nThe completed script \nrun_blast.sh\n should look like this: \n\n\n#/bin/bash\n\n\ntar -xzf ncbi-blast-2.9.0+-x64-linux.tar.gz \ntar -xzf pdbaa.tar.gz\n\nncbi-blast-2.9.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt\n\n\n\n\n\n\n\n\n\nWhile not strictly necessary, it's a good idea to enable executable permissions on the wrapper script, like so: \n\n\nusername@training.osgconnect.net $\n chmod u+x run_blast.sh\n\n\n\n\n\n\n\n\n\nYour job is now ready to submit. Submit it using \ncondor_submit\n and monitor using \ncondor_q\n.",
            "title": "Exercise 3.3"
        },
        {
            "location": "/materials/day2/part3-ex3-wrapper/#tuesday-exercise-33-using-wrapper-scripts-to-submit-jobs",
            "text": "In this exercise, you will create a wrapper script to run the same program ( blastx ) as the  previous exercise .",
            "title": "Tuesday Exercise 3.3: Using Wrapper Scripts to Submit Jobs"
        },
        {
            "location": "/materials/day2/part3-ex3-wrapper/#background",
            "text": "Wrapper scripts are a useful tool for running software that can't be compiled into one piece, needs to be installed with every job, or just for running extra steps.  A wrapper script can either install the software from the source code, or use an already existing software (as in this exercise). Not only does this portability technique work with almost any kind of software that can be locally installed, it also allows for a great deal of control and flexibility for what happens within your job. Once you can write a script to handle your software (and often your data as well), you can submit a large variety of workflows to a distributed computing system like the Open Science Grid.",
            "title": "Background"
        },
        {
            "location": "/materials/day2/part3-ex3-wrapper/#wrapper-script-part-1",
            "text": "Our wrapper script will be a bash script that runs several commands.    In the same directory as the last exercise (still logged into  training.osgconnect.net.chtc.wisc.edu ) make a file called  run_blast.sh .     The first line we'll place in the script is the basic command for running blast. Based on our previous submit file, what command needs to go into the script? Once you have an idea, check against the example below:    #!/bin/bash \n\nncbi-blast-2.9.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results.txt   Note  The \"header\" of  #!/bin/bash  will tell the computer that this is a bash shell script and can be run in the same way that  you would run individual commands on the command line.",
            "title": "Wrapper Script, part 1"
        },
        {
            "location": "/materials/day2/part3-ex3-wrapper/#submit-file-changes",
            "text": "We now need to make some changes to our submit file.    Make a copy of your previous submit file and open it.     Since we are now using a wrapper script, that will be our job's executable. Replace the original  blastx  exeuctable with the name of our wrapper script and comment out the arguments line.    executable = run_blast.sh \n#arguments =    Note that since the  blastx  program is no longer listed as the executable, it will be need to be included in  transfer_input_files . Instead of transferring just that program, we will transfer the original downloaded  tar.gz  file.    transfer_input_files = pdbaa, mouse.fa,  ncbi-blast-2.9.0+-x64-linux.tar.gz     To achieve efficiency, we'll also transfer the pdbaa database as the original  tar.gz  file instead of as the unzipped folder:   transfer_input_files =  pdbaa.tar.gz , mouse.fa, ncbi-blast-2.9.0+-x64-linux.tar.gz",
            "title": "Submit File Changes"
        },
        {
            "location": "/materials/day2/part3-ex3-wrapper/#wrapper-script-part-2",
            "text": "Now that our database and BLAST software are being transferred to the job as  tar.gz  files, our script needs to accommodate.    Opening your  run_blast.sh  script, add two commands at the start to un-tar the BLAST and pdbaa  tar.gz  files. See the  previous exercise  if you're not sure what this command looks like.     In order to distinguish this job from our previous job, change the output file name to something besides  results.txt .     The completed script  run_blast.sh  should look like this:   #/bin/bash \n\ntar -xzf ncbi-blast-2.9.0+-x64-linux.tar.gz \ntar -xzf pdbaa.tar.gz\n\nncbi-blast-2.9.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt    While not strictly necessary, it's a good idea to enable executable permissions on the wrapper script, like so:   username@training.osgconnect.net $  chmod u+x run_blast.sh    Your job is now ready to submit. Submit it using  condor_submit  and monitor using  condor_q .",
            "title": "Wrapper Script, part 2"
        },
        {
            "location": "/materials/day2/part3-ex4-prepackaged/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } \n\n\n\nWednesday Exercise 1.4: Pre-packaging Code\n\u00b6\n\n\nIn this exercise, you will create an installation of a Bayesian inference package (OpenBUGS) and then create a wrapper script to unpack that installation to run jobs. It should take 30-35 minutes.\n\n\nBackground\n\u00b6\n\n\nSome software cannot be compiled into a single executable, whether you compile it yourself (as in \nExercise 3.1\n) or download it already compiled (as in \nExercise 3.2\n). In this case, it is necessary to download or create a portable copy of the software and then use a wrapper script (as in the \nprevious exercise\n) to \"install\" the software on a per job basis. This script can either install the software from the source code, or (as in this exercise), unpack a portable software package that you've pre-built yourself.\n\n\nOur Software Example\n\u00b6\n\n\nFor this exercise, we will be using the Bayseian inference package OpenBUGS. OpenBUGS is a good example of software that is not compiled to a single executable; it has multiple executables as well as a helper library.\n\n\n\n\n\n\nDo an internet search to find the Open BUGS software downloads page.\n\n\n\n\n\n\nCreate a directory for this exercise on the CHTC submit server \nlearn.chtc.wisc.edu\n (\nnot\n \ntraining.osgconnect.net\n), \n\n\n\n\n\n\nBecause you can't download the OpenBUGS source tarball directly, download it from our \"squid\" webserver: \n\n\nusername@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/OpenBUGS-3.2.3.tar.gz\n\n\n\n\n\n\n\n\n\nWhere to Prepare\n\u00b6\n\n\nOur goal is to pre-build an OpenBUGS installation, and then write a script that will unpack that installation and run a simulation.\n\n\n\n\n\n\nWhere can we create this pre-built installation? Based on the end of the lecture, what are our options and which would be most appropriate? Make a guess before moving on.\n\n\n\n\n\n\nBecause we're on the CHTC-based submit node (\nlearn.chtc.wisc.edu\n), we have the option of using an interactive job to build the OpenBUGS installation. This is a good option because the submit server is already busy with lots of users and we don't know how long the OpenBUGS install will take. We'll also target specific build servers with extra tools by adding some special requirements to our interactive job. \n\n\n\n\n\n\nCopy the following lines into a file named \nbuild.submit\n\n\nlog = build.log\n\nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\ntransfer_input_files =\n\n+IsBuildJob = true\nrequirements = (IsBuildSlot == true)\n\nrequest_cpus = 1\nrequest_disk = 2GB\nrequest_memory = 2GB\n\nqueue\n\n\n\n\n\n\n\n\n\nNote the lack of executable. Condor doesn't need an executable for this job because it will be interactive, meaning \nyou\n are running the commands instead of Condor.\n\n\n\n\n\n\nIn order to create the installation, we will need the source code to come with us. The \ntransfer_input_files\n line is blank - \nfill it in with the name of our Open BUGS source tarball\n.\n\n\n\n\n\n\nTo request an interactive job, we will add a \n-i\n flag to the \ncondor_submit\n command. The whole command you enter should look like this: \n\n\nusername@learn $\n condor_submit -i build.submit\n\n\n\n\n\n\n\n\n\n\n\n\n\nRead Through Installation Documentation\n\u00b6\n\n\nWhile you're waiting for the interactive job to start, you can start reading the Open BUGS installation documentation online.\n\n\n\n\nFind the installation instructions for Open BUGS.\n\n\nOn the downloads page, there are short instructions for how to install Open BUGS. There are two options shown for installation -- which should we use?\n\n\nThe first installation option given uses \nsudo\n -- which is an administrative permission that you won't have as a normal user. Luckily, as described in the instructions, you can use the \n--prefix\n option to set where Open BUGS will be installed, which will allow us to install it without administrative permissions.\n\n\n\n\nInstallation\n\u00b6\n\n\nYour interactive job should have started by now and we've learned about installing our program. Let's test it out.\n\n\n\n\n\n\nBefore we follow the installation instructions, we should create a directory to hold our installation. You can create this in the current directory. \n\n\nusername@host $\n mkdir openbugs\n\n\n\n\n\n\n\n\n\nNow run the commands to unpack the source code: \n\n\nusername@host $\n tar -zxf OpenBUGS-3.2.3.tar.gz\n\nusername@host $\n \ncd\n OpenBUGS-3.2.3\n\n\n\n\n\n\n\n\n\nNow we can follow the second set of installation instructions. For the prefix, we'll use the command \n$(pwd)\n to capture the name of our current working directory and then a relative path to the \nopenbugs\n directory we created in step 1: \n\n\nusername@host $\n ./configure --prefix\n=\n$(\npwd\n)\n/../openbugs\n\nusername@host $\n make\n\nusername@host $\n make install\n\n\n\n\n\n\n\n\n\nGo back to the job's main working directory\n: \n\n\nusername@host $\n \ncd\n ..\n\n\n\n\n\nand confirm that our installation procedure created \nbin\n,  \nlib\n, and \nshare\n directories. \n\n\nusername@host $\n ls openbugs\n\nbin lib share\n\n\n\n\n\n\n\n\n\n\nNow we want to package up our installation, so we can use it in other jobs. We can do this by compressing any necessary directories into a single gzipped tarball. \n\n\nusername@host $\n tar -czf openbugs.tar.gz openbugs/\n\n\n\n\n\n\n\n\n\nOnce everything is complete, type \nexit\n to leave the interactive job. Make sure that your tarball is in the main working directory - it will be transferred back to the submit server automatically. \n\n\nusername@learn $\n \nexit\n\n\n\n\n\n\n\n\n\n\nNote that we now have two tarballs in our directory -- the \nsource\n tarball (\nOpenBUGS-3.2.3.tar.gz\n), which we will no longer need and our newly built installation (\nopenbugs.tar.gz\n) which is what we will actually be using to run jobs.\n\n\nWrapper Script\n\u00b6\n\n\nNow that we've created our portable installation, we need to write a script that opens and uses the installation, similar to the process we used in the  \nprevious exercise\n. These steps should be performed back on the submit server (\nlearn.chtc.wisc.edu\n).\n\n\n\n\n\n\nCreate a script called \nrun_openbugs.sh\n. The script will first need to untar our installation, so the script should start out like this:  \n\n\n#!/bin/bash\n\n\ntar -xzf openbugs.tar.gz\n\n\n\n\n\n\n\n\n\nWe're going to use the same \n$(pwd)\n trick from the installation in order to tell the computer how to find Open BUGS. We will do this by setting the \nPATH\n environment variable, to include the directory where Open BUGS is installed: \n\n\nexport\n \nPATH\n=\n$(\npwd\n)\n/openbugs/bin:\n$PATH\n\n\n\n\n\n\n\n\n\n\nFinally, the wrapper script needs to not only setup Open BUGS, but actually run the program. Add the following lines to your \nrun_openbugs.sh\n wrapper script. \n\n\nOpenBUGS < input.txt > results.txt\n\n\n\n\n\n\n\n\n\nMake sure the wrapper script has executable permissions: \n\n\nusername@learn $\n chmod u+x run_openbugs.sh\n\n\n\n\n\n\n\n\n\nRun a Open BUGS job\n\u00b6\n\n\nWe're almost ready! We need two more pieces to run a OpenBUGS job.\n\n\n\n\n\n\nDownload the necessary input files to your directory on the submit server and then untar them. \n\n\nusername@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool18/openbugs_files.tar.gz\n\nusername@learn $\n tar -xzf openbugs_files.tar.gz\n\n\n\n\n\n\n\n\n\nOur last step is to create a submit file for our Open BUGS job. Think about which lines this submit file will need. Make a copy of a previous submit file (you could use the blast submit file from the \nprevious exercise\n as a base) and modify it as you think necessary.\n\n\n\n\n\n\nThe two most important lines to modify for this job are listed below; check them against your own submit file: \n\n\nexecutable = run_openbugs.sh\ntransfer_input_files = openbugs.tar.gz, openbugs_files/\n\n\n\n\n\nA wrapper script will always be a job's \nexecutable\n.\nWhen using a wrapper script, you must also always remember to transfer the software/source code using\n\ntransfer_input_files\n.\n\n\n\n\nNote\n\n\nThe \n/\n in the \ntransfer_input_files\n line indicates that we are transferring the \ncontents\n of that directory (which in this case, is what we want), rather than the directory itself.\n\n\n\n\n\n\n\n\nWe also need to add a requirement to ensure that the job will run on a Linux system. This can be done with the line:\n\n\nrequirements = (OpSys == \"LINUX\")\n\n\n\n\n\n\n\n\n\nSubmit the job with \ncondor_submit\n.\n\n\n\n\n\n\nOnce the job completes, it should produce a \nresults.txt\n file.",
            "title": "Exercise 3.4"
        },
        {
            "location": "/materials/day2/part3-ex4-prepackaged/#wednesday-exercise-14-pre-packaging-code",
            "text": "In this exercise, you will create an installation of a Bayesian inference package (OpenBUGS) and then create a wrapper script to unpack that installation to run jobs. It should take 30-35 minutes.",
            "title": "Wednesday Exercise 1.4: Pre-packaging Code"
        },
        {
            "location": "/materials/day2/part3-ex4-prepackaged/#background",
            "text": "Some software cannot be compiled into a single executable, whether you compile it yourself (as in  Exercise 3.1 ) or download it already compiled (as in  Exercise 3.2 ). In this case, it is necessary to download or create a portable copy of the software and then use a wrapper script (as in the  previous exercise ) to \"install\" the software on a per job basis. This script can either install the software from the source code, or (as in this exercise), unpack a portable software package that you've pre-built yourself.",
            "title": "Background"
        },
        {
            "location": "/materials/day2/part3-ex4-prepackaged/#our-software-example",
            "text": "For this exercise, we will be using the Bayseian inference package OpenBUGS. OpenBUGS is a good example of software that is not compiled to a single executable; it has multiple executables as well as a helper library.    Do an internet search to find the Open BUGS software downloads page.    Create a directory for this exercise on the CHTC submit server  learn.chtc.wisc.edu  ( not   training.osgconnect.net ),     Because you can't download the OpenBUGS source tarball directly, download it from our \"squid\" webserver:   username@learn $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/OpenBUGS-3.2.3.tar.gz",
            "title": "Our Software Example"
        },
        {
            "location": "/materials/day2/part3-ex4-prepackaged/#where-to-prepare",
            "text": "Our goal is to pre-build an OpenBUGS installation, and then write a script that will unpack that installation and run a simulation.    Where can we create this pre-built installation? Based on the end of the lecture, what are our options and which would be most appropriate? Make a guess before moving on.    Because we're on the CHTC-based submit node ( learn.chtc.wisc.edu ), we have the option of using an interactive job to build the OpenBUGS installation. This is a good option because the submit server is already busy with lots of users and we don't know how long the OpenBUGS install will take. We'll also target specific build servers with extra tools by adding some special requirements to our interactive job.     Copy the following lines into a file named  build.submit  log = build.log\n\nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\ntransfer_input_files =\n\n+IsBuildJob = true\nrequirements = (IsBuildSlot == true)\n\nrequest_cpus = 1\nrequest_disk = 2GB\nrequest_memory = 2GB\n\nqueue    Note the lack of executable. Condor doesn't need an executable for this job because it will be interactive, meaning  you  are running the commands instead of Condor.    In order to create the installation, we will need the source code to come with us. The  transfer_input_files  line is blank -  fill it in with the name of our Open BUGS source tarball .    To request an interactive job, we will add a  -i  flag to the  condor_submit  command. The whole command you enter should look like this:   username@learn $  condor_submit -i build.submit",
            "title": "Where to Prepare"
        },
        {
            "location": "/materials/day2/part3-ex4-prepackaged/#read-through-installation-documentation",
            "text": "While you're waiting for the interactive job to start, you can start reading the Open BUGS installation documentation online.   Find the installation instructions for Open BUGS.  On the downloads page, there are short instructions for how to install Open BUGS. There are two options shown for installation -- which should we use?  The first installation option given uses  sudo  -- which is an administrative permission that you won't have as a normal user. Luckily, as described in the instructions, you can use the  --prefix  option to set where Open BUGS will be installed, which will allow us to install it without administrative permissions.",
            "title": "Read Through Installation Documentation"
        },
        {
            "location": "/materials/day2/part3-ex4-prepackaged/#installation",
            "text": "Your interactive job should have started by now and we've learned about installing our program. Let's test it out.    Before we follow the installation instructions, we should create a directory to hold our installation. You can create this in the current directory.   username@host $  mkdir openbugs    Now run the commands to unpack the source code:   username@host $  tar -zxf OpenBUGS-3.2.3.tar.gz username@host $   cd  OpenBUGS-3.2.3    Now we can follow the second set of installation instructions. For the prefix, we'll use the command  $(pwd)  to capture the name of our current working directory and then a relative path to the  openbugs  directory we created in step 1:   username@host $  ./configure --prefix = $( pwd ) /../openbugs username@host $  make username@host $  make install    Go back to the job's main working directory :   username@host $   cd  ..  and confirm that our installation procedure created  bin ,   lib , and  share  directories.   username@host $  ls openbugs bin lib share     Now we want to package up our installation, so we can use it in other jobs. We can do this by compressing any necessary directories into a single gzipped tarball.   username@host $  tar -czf openbugs.tar.gz openbugs/    Once everything is complete, type  exit  to leave the interactive job. Make sure that your tarball is in the main working directory - it will be transferred back to the submit server automatically.   username@learn $   exit     Note that we now have two tarballs in our directory -- the  source  tarball ( OpenBUGS-3.2.3.tar.gz ), which we will no longer need and our newly built installation ( openbugs.tar.gz ) which is what we will actually be using to run jobs.",
            "title": "Installation"
        },
        {
            "location": "/materials/day2/part3-ex4-prepackaged/#wrapper-script",
            "text": "Now that we've created our portable installation, we need to write a script that opens and uses the installation, similar to the process we used in the   previous exercise . These steps should be performed back on the submit server ( learn.chtc.wisc.edu ).    Create a script called  run_openbugs.sh . The script will first need to untar our installation, so the script should start out like this:    #!/bin/bash \n\ntar -xzf openbugs.tar.gz    We're going to use the same  $(pwd)  trick from the installation in order to tell the computer how to find Open BUGS. We will do this by setting the  PATH  environment variable, to include the directory where Open BUGS is installed:   export   PATH = $( pwd ) /openbugs/bin: $PATH     Finally, the wrapper script needs to not only setup Open BUGS, but actually run the program. Add the following lines to your  run_openbugs.sh  wrapper script.   OpenBUGS < input.txt > results.txt    Make sure the wrapper script has executable permissions:   username@learn $  chmod u+x run_openbugs.sh",
            "title": "Wrapper Script"
        },
        {
            "location": "/materials/day2/part3-ex4-prepackaged/#run-a-open-bugs-job",
            "text": "We're almost ready! We need two more pieces to run a OpenBUGS job.    Download the necessary input files to your directory on the submit server and then untar them.   username@learn $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool18/openbugs_files.tar.gz username@learn $  tar -xzf openbugs_files.tar.gz    Our last step is to create a submit file for our Open BUGS job. Think about which lines this submit file will need. Make a copy of a previous submit file (you could use the blast submit file from the  previous exercise  as a base) and modify it as you think necessary.    The two most important lines to modify for this job are listed below; check them against your own submit file:   executable = run_openbugs.sh\ntransfer_input_files = openbugs.tar.gz, openbugs_files/  A wrapper script will always be a job's  executable .\nWhen using a wrapper script, you must also always remember to transfer the software/source code using transfer_input_files .   Note  The  /  in the  transfer_input_files  line indicates that we are transferring the  contents  of that directory (which in this case, is what we want), rather than the directory itself.     We also need to add a requirement to ensure that the job will run on a Linux system. This can be done with the line:  requirements = (OpSys == \"LINUX\")    Submit the job with  condor_submit .    Once the job completes, it should produce a  results.txt  file.",
            "title": "Run a Open BUGS job"
        },
        {
            "location": "/materials/day2/part3-ex5-arguments/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } \n\n\n\nBonus: Passing Arguments Through the Wrapper Script\n\u00b6\n\n\nIn this exercise, you will change the wrapper script and submit file from the previous exercise to use arguments.\n\n\nBackground\n\u00b6\n\n\nSo far, our wrapper scripts have had all files and options written out explicitly. However, imagine if you wanted to run the same job multiple times, or even just try out one or two different options or inputs. Instead of writing new wrapper scripts for each job, you can modify the script so that some of the values are set by \narguments\n. Using script arguments will allow you to use the same script for multiple jobs, by providing different inputs or parameters. These arguments are normally passed on the command line:\n\n\nBut in our world of job submission, the arguments will be listed in the submit file, in the arguments line.\n\n\nIdentifying Potential Arguments\n\u00b6\n\n\n\n\nFind the directory you used to submit Open BUGS jobs and open your \nrun_openbugs.sh\n wrapper script.\n\n\nWhat values might we want to input to the script via arguments? Hint: anything that we might want to change if we were to run the script many times.\n\n\n\n\nIn this example, some values we might want to change are the name of the input and output file. These will be the arguments for our script.\n\n\nModifying Files\n\u00b6\n\n\n\n\n\n\nNote the name of the input and output files and open the submit file. Add an arguments line if it doesn't already exist, and fill it with our two chosen arguments: the name of the input file and the name of the output file: \n\n\narguments = input.txt results.txt\n\n\n\n\n\n\n\n\n\nNow go back to the wrapper script. Each scripting language (bash, perl, python, R, etc.) will have its own particular syntax for capturing command line arguments. For bash (the language of our current wrapper script), the variables \n$1\n and \n$2\n represent  the first and second arguments, respectively. (If our script needed three arguments, we would use \n$3\n for the third one). Thus, in  the main command of the script, replace the file names with these variables: \n\n\nOpenBUGS < $1 > $2\n\n\n\n\n\n\n\n\n\nOnce these changes are made, submit your jobs with \ncondor_submit\n. Use \ncondor_q -nobatch\n to see what the job command looks like to HTCondor.",
            "title": "Exercise 3.5"
        },
        {
            "location": "/materials/day2/part3-ex5-arguments/#bonus-passing-arguments-through-the-wrapper-script",
            "text": "In this exercise, you will change the wrapper script and submit file from the previous exercise to use arguments.",
            "title": "Bonus: Passing Arguments Through the Wrapper Script"
        },
        {
            "location": "/materials/day2/part3-ex5-arguments/#background",
            "text": "So far, our wrapper scripts have had all files and options written out explicitly. However, imagine if you wanted to run the same job multiple times, or even just try out one or two different options or inputs. Instead of writing new wrapper scripts for each job, you can modify the script so that some of the values are set by  arguments . Using script arguments will allow you to use the same script for multiple jobs, by providing different inputs or parameters. These arguments are normally passed on the command line:  But in our world of job submission, the arguments will be listed in the submit file, in the arguments line.",
            "title": "Background"
        },
        {
            "location": "/materials/day2/part3-ex5-arguments/#identifying-potential-arguments",
            "text": "Find the directory you used to submit Open BUGS jobs and open your  run_openbugs.sh  wrapper script.  What values might we want to input to the script via arguments? Hint: anything that we might want to change if we were to run the script many times.   In this example, some values we might want to change are the name of the input and output file. These will be the arguments for our script.",
            "title": "Identifying Potential Arguments"
        },
        {
            "location": "/materials/day2/part3-ex5-arguments/#modifying-files",
            "text": "Note the name of the input and output files and open the submit file. Add an arguments line if it doesn't already exist, and fill it with our two chosen arguments: the name of the input file and the name of the output file:   arguments = input.txt results.txt    Now go back to the wrapper script. Each scripting language (bash, perl, python, R, etc.) will have its own particular syntax for capturing command line arguments. For bash (the language of our current wrapper script), the variables  $1  and  $2  represent  the first and second arguments, respectively. (If our script needed three arguments, we would use  $3  for the third one). Thus, in  the main command of the script, replace the file names with these variables:   OpenBUGS < $1 > $2    Once these changes are made, submit your jobs with  condor_submit . Use  condor_q -nobatch  to see what the job command looks like to HTCondor.",
            "title": "Modifying Files"
        },
        {
            "location": "/materials/day2/part4-ex1-python-built/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } \n\n\n\nTuesday Exercise 4.1: Using Python, Pre-Built\n\u00b6\n\n\nIn this exercise, you will install Python, package your installation, and then use it to run jobs. It should take about 20 minutes.\n\n\nBackground\n\u00b6\n\n\nWe chose Python as the language for this example because: a) it is a common language used for scientific computing and b) it has a straightforward installation process and is fairly portable.\n\n\nRunning any Python script requires an installation of the Python interpreter. The Python interpreter is what we're using when we type \npython\n at the command line. In order to run Python jobs on a distributed system, you will need to install the Python interpreter (what we often refer to as just \"installing Python\"), within the job, then run your Python script.\n\n\nThere are two installation approaches. The approach we will cover in this exercise is that of \"pre-building\" the installation (much like we did with OpenBugs \nthis morning\n). We will install Python to a specific directory, and then create a tarball of that installation directory. We can then use our tarball within jobs to run Python scripts.\n\n\nInteractive Job for Pre-Building\n\u00b6\n\n\nThe first step in our job process is building a Python installation that we can package up.\n\n\n\n\nCreate a directory for this exercise on \nlearn.chtc.wisc.edu\n and \ncd\n into it.\n\n\n\n\nDownload the Python source code from \nhttps://www.python.org/\n. \n\n\nusername@learn $\n wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tgz\n\n\n\n\n\n\n\n\n\nOf our options - submit server, interactive job, personal computer - which should we use for this installation/packaging process? Once you have a guess, move to the next step.\n\n\n\n\n\n\nDue to the number of people on our submit server, we shouldn't use the submit server. Your own computer probably doesn't have the right operating system. The best place to install will be an interactive job. For this job, we can use the same interactive submit file as Exercise 3.4, with one change. What is it?\n\n\n\n\n\n\nMake a copy of the interactive submit file from \nExercise 3.4\n and change the \ntransfer_input_files\n line to the Python tarball you just downloaded. Then submit it using the \n-i\n flag. \n\n\nusername@learn $\n condor_submit -i build.submit\n\n\n\n\n\n\n\n\n\nOnce the interactive job begins, we can start our installation process. First, we have to determine how to install Python to a specific location in our working directory.\n\n\n\n\nUntar the Python source tarball and look at the \nREADME.rst\n file in the \nPython-3.7.0\n directory.  You'll want to look for the \"Build Instructions\" header.  What will the main installation steps be?  What command is required for the final installation?  Once you've tried to answer these questions, move to the next step.\n\n\n\n\nThere are some basic installation instructions near the top of the \nREADME\n. Based on that short introduction, we can see the main steps of installation will be: \n\n\n./configure\nmake\nmake test\nsudo make install\n\n\n\n\n\nThis looks a lot like the OpenBUGS installation from earlier today! It turns out that this three-stage process (configure, make, make install) is a common  way to install many software packages.   Also like the OpenBUGS installation, the default installation  location for Python requires \nsudo\n (administrative privileges) to install. However, we'd like to install to a specific location in the working directory  so that we can compress that installation directory into a tarball. How did we do this with OpenBugs? \n\n\n\n\n\n\nWith OpenBugs we used the \n-prefix\n option with the \nconfigure\n script. Let's see if the Python \nconfigure\n script has this option by using the \"help\" option (as suggested in the \nREADME.rst\n file): \n\n\nusername@host $\n ./configure --help\n\n\n\n\n\nSure enough, there's a list of all the different options that can be passed to the \nconfigure\n script, which includes \n--prefix\n.  (To see the \n--prefix\n option, you may need to scroll towards the top of the output.)  Therefore, we can use the  \n$(pwd)\n command in order to set the path correctly, just as we did earlier today.\n\n\n\n\n\n\n\n\n\n\nNow let's actually install Python!\n\n\n\n\n\n\nFrom the job's main working directory\n, create a directory to hold the installation. \n\n\nusername@host $\n \ncd\n \n$_CONDOR_SCRATCH_DIR\n\n\nusername@host $\n mkdir python\n\n\n\n\n\n\n\n\n\nMove into the \nPython-3.7.0\n directory and run the installation commands. These may take a few minutes each. \n\n\nusername@host $\n \ncd\n Python-3.7.0\n\nusername@host $\n ./configure --prefix\n=\n$(\npwd\n)\n/../python\n\nusername@host $\n make\n\nusername@host $\n make install\n\n\n\n\n\n\n\nNote\n\n\nThe installation instructions in the \nREADME.rst\n file have a \nmake test\n step \nbetween the \nmake\n and \nmake install\n steps.  As this step isn't strictly necessary (and takes a long time), it's been omitted above.  \n\n\n\n\n\n\n\n\nIf I move back to the main job working directory, and look in the \npython\n subdirectory, I should see a Python installation. \n\n\nusername@host $\n \ncd\n ..\n\nusername@host $\n ls python/\n\nbin  include  lib  share\n\n\n\n\n\n\n\n\n\n\nI have successfully created a self-contained Python installation. Now it just needs to be tarred up! \n\n\nusername@host $\n tar -czf prebuilt_python.tar.gz python/\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore exiting, we might want to know how we installed Python for later reference.  Enter the following commands to save our history to a file: \n\n\nusername@host $\n \nhistory\n > python_install.txt\n\n\n\n\n\n\n\n\n\nExit the interactive job. \n\n\nusername@host $\n \nexit\n\n\n\n\n\n\n\n\n\n\nPython Script\n\u00b6\n\n\n\n\n\n\nCreate a script with the following lines called \nfib.py\n. \n\n\nimport\n \nsys\n\n\nimport\n \nos\n\n\n\nif\n \nlen\n(\nsys\n.\nargv\n)\n \n!=\n \n2\n:\n\n    \nprint\n(\n'Usage: \n%s\n MAXIMUM'\n \n%\n \n(\nos\n.\npath\n.\nbasename\n(\nsys\n.\nargv\n[\n0\n])))\n\n    \nsys\n.\nexit\n(\n1\n)\n\n\nmaximum\n \n=\n \nint\n(\nsys\n.\nargv\n[\n1\n])\n\n\nn1\n \n=\n \nn2\n \n=\n \n1\n\n\nwhile\n \nn2\n \n<=\n \nmaximum\n:\n\n    \nn1\n,\n \nn2\n \n=\n \nn2\n,\n \nn1\n \n+\n \nn2\n\n\nprint\n(\n'The greatest Fibonacci number up to \n%d\n is \n%d\n'\n \n%\n \n(\nmaximum\n,\n \nn1\n))\n\n\n\n\n\n\n\n\n\n\nWhat command line arguments does this script take? Try running it on the submit server.\n\n\n\n\n\n\nWrapper Script\n\u00b6\n\n\nWe now have our Python installation and our Python script - we just need to write a wrapper script to run them.\n\n\n\n\nWhat steps do you think the wrapper script needs to perform? Create a file called \nrun_fib.sh\n and write them out in plain English before moving to the next step.\n\n\nOur script will need to\n\n\nuntar our \nprebuilt_python.tar.gz\n file\n\n\naccess the \npython\n command from our installation to run our \nfib.py\n script\n\n\n\n\n\n\nTry turning your plain English steps into commands that the computer can run.\n\n\n\n\nYour final \nrun_fib.sh\n script should look something like this: \n\n\n#!/bin/bash\n\n\ntar xzf prebuilt_python.tar.gz \npython/bin/python3 fib.py \n90\n\n\n\n\n\n\nor\n\n\n#!/bin/bash\n\n\ntar xzf prebuilt_python.tar.gz \n\nexport\n \nPATH\n=\n$(\npwd\n)\n/python/bin:\n$PATH\n \npython3 fib.py \n90\n\n\n\n\n\n\n\n\n\n\nMake sure your \nrun_fib.sh\n script is executable.\n\n\n\n\n\n\nSubmit File\n\u00b6\n\n\n\n\n\n\nMake a copy of a previous submit file in your local directory (the OpenBugs submit file could be a good starting point). What changes need to be made to run this Python job? \n\n\n\n\n\n\nModify your submit file, then make sure you've included the key lines below: \n\n\nexecutable = run_fib.sh\ntransfer_input_files = fib.py, prebuilt_python.tar.gz\n\n\n\n\n\n\n\n\n\nBecause we pre-built our Python installation on a machine running Scientific Linux, version 6.something, we should request machines with similar characteristics. Add the following line to your submit file as well: \n\n\nrequirements = (OpSys == \"LINUX\" && OpSysMajorVer == 6 )\n\n\n\n\n\n\n\n\n\nSubmit the job using \ncondor_submit\n. \n\n\n\n\n\n\nCheck the \n.out\n file to see if the job completed.",
            "title": "Exercise 4.1"
        },
        {
            "location": "/materials/day2/part4-ex1-python-built/#tuesday-exercise-41-using-python-pre-built",
            "text": "In this exercise, you will install Python, package your installation, and then use it to run jobs. It should take about 20 minutes.",
            "title": "Tuesday Exercise 4.1: Using Python, Pre-Built"
        },
        {
            "location": "/materials/day2/part4-ex1-python-built/#background",
            "text": "We chose Python as the language for this example because: a) it is a common language used for scientific computing and b) it has a straightforward installation process and is fairly portable.  Running any Python script requires an installation of the Python interpreter. The Python interpreter is what we're using when we type  python  at the command line. In order to run Python jobs on a distributed system, you will need to install the Python interpreter (what we often refer to as just \"installing Python\"), within the job, then run your Python script.  There are two installation approaches. The approach we will cover in this exercise is that of \"pre-building\" the installation (much like we did with OpenBugs  this morning ). We will install Python to a specific directory, and then create a tarball of that installation directory. We can then use our tarball within jobs to run Python scripts.",
            "title": "Background"
        },
        {
            "location": "/materials/day2/part4-ex1-python-built/#interactive-job-for-pre-building",
            "text": "The first step in our job process is building a Python installation that we can package up.   Create a directory for this exercise on  learn.chtc.wisc.edu  and  cd  into it.   Download the Python source code from  https://www.python.org/ .   username@learn $  wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tgz    Of our options - submit server, interactive job, personal computer - which should we use for this installation/packaging process? Once you have a guess, move to the next step.    Due to the number of people on our submit server, we shouldn't use the submit server. Your own computer probably doesn't have the right operating system. The best place to install will be an interactive job. For this job, we can use the same interactive submit file as Exercise 3.4, with one change. What is it?    Make a copy of the interactive submit file from  Exercise 3.4  and change the  transfer_input_files  line to the Python tarball you just downloaded. Then submit it using the  -i  flag.   username@learn $  condor_submit -i build.submit    Once the interactive job begins, we can start our installation process. First, we have to determine how to install Python to a specific location in our working directory.   Untar the Python source tarball and look at the  README.rst  file in the  Python-3.7.0  directory.  You'll want to look for the \"Build Instructions\" header.  What will the main installation steps be?  What command is required for the final installation?  Once you've tried to answer these questions, move to the next step.   There are some basic installation instructions near the top of the  README . Based on that short introduction, we can see the main steps of installation will be:   ./configure\nmake\nmake test\nsudo make install  This looks a lot like the OpenBUGS installation from earlier today! It turns out that this three-stage process (configure, make, make install) is a common  way to install many software packages.   Also like the OpenBUGS installation, the default installation  location for Python requires  sudo  (administrative privileges) to install. However, we'd like to install to a specific location in the working directory  so that we can compress that installation directory into a tarball. How did we do this with OpenBugs?     With OpenBugs we used the  -prefix  option with the  configure  script. Let's see if the Python  configure  script has this option by using the \"help\" option (as suggested in the  README.rst  file):   username@host $  ./configure --help  Sure enough, there's a list of all the different options that can be passed to the  configure  script, which includes  --prefix .  (To see the  --prefix  option, you may need to scroll towards the top of the output.)  Therefore, we can use the   $(pwd)  command in order to set the path correctly, just as we did earlier today.      Now let's actually install Python!    From the job's main working directory , create a directory to hold the installation.   username@host $   cd   $_CONDOR_SCRATCH_DIR  username@host $  mkdir python    Move into the  Python-3.7.0  directory and run the installation commands. These may take a few minutes each.   username@host $   cd  Python-3.7.0 username@host $  ./configure --prefix = $( pwd ) /../python username@host $  make username@host $  make install   Note  The installation instructions in the  README.rst  file have a  make test  step \nbetween the  make  and  make install  steps.  As this step isn't strictly necessary (and takes a long time), it's been omitted above.       If I move back to the main job working directory, and look in the  python  subdirectory, I should see a Python installation.   username@host $   cd  .. username@host $  ls python/ bin  include  lib  share     I have successfully created a self-contained Python installation. Now it just needs to be tarred up!   username@host $  tar -czf prebuilt_python.tar.gz python/      Before exiting, we might want to know how we installed Python for later reference.  Enter the following commands to save our history to a file:   username@host $   history  > python_install.txt    Exit the interactive job.   username@host $   exit",
            "title": "Interactive Job for Pre-Building"
        },
        {
            "location": "/materials/day2/part4-ex1-python-built/#python-script",
            "text": "Create a script with the following lines called  fib.py .   import   sys  import   os  if   len ( sys . argv )   !=   2 : \n     print ( 'Usage:  %s  MAXIMUM'   %   ( os . path . basename ( sys . argv [ 0 ]))) \n     sys . exit ( 1 )  maximum   =   int ( sys . argv [ 1 ])  n1   =   n2   =   1  while   n2   <=   maximum : \n     n1 ,   n2   =   n2 ,   n1   +   n2  print ( 'The greatest Fibonacci number up to  %d  is  %d '   %   ( maximum ,   n1 ))     What command line arguments does this script take? Try running it on the submit server.",
            "title": "Python Script"
        },
        {
            "location": "/materials/day2/part4-ex1-python-built/#wrapper-script",
            "text": "We now have our Python installation and our Python script - we just need to write a wrapper script to run them.   What steps do you think the wrapper script needs to perform? Create a file called  run_fib.sh  and write them out in plain English before moving to the next step.  Our script will need to  untar our  prebuilt_python.tar.gz  file  access the  python  command from our installation to run our  fib.py  script    Try turning your plain English steps into commands that the computer can run.   Your final  run_fib.sh  script should look something like this:   #!/bin/bash \n\ntar xzf prebuilt_python.tar.gz \npython/bin/python3 fib.py  90   or  #!/bin/bash \n\ntar xzf prebuilt_python.tar.gz  export   PATH = $( pwd ) /python/bin: $PATH  \npython3 fib.py  90     Make sure your  run_fib.sh  script is executable.",
            "title": "Wrapper Script"
        },
        {
            "location": "/materials/day2/part4-ex1-python-built/#submit-file",
            "text": "Make a copy of a previous submit file in your local directory (the OpenBugs submit file could be a good starting point). What changes need to be made to run this Python job?     Modify your submit file, then make sure you've included the key lines below:   executable = run_fib.sh\ntransfer_input_files = fib.py, prebuilt_python.tar.gz    Because we pre-built our Python installation on a machine running Scientific Linux, version 6.something, we should request machines with similar characteristics. Add the following line to your submit file as well:   requirements = (OpSys == \"LINUX\" && OpSysMajorVer == 6 )    Submit the job using  condor_submit .     Check the  .out  file to see if the job completed.",
            "title": "Submit File"
        },
        {
            "location": "/materials/day2/part4-ex2-python-install/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } \n\n\n\nWednesday Exercise 4.2: Python Installation\n\u00b6\n\n\nIn this exercise, you will write a wrapper script that installs Python and then use it to run jobs. This exercise should take about 10 minutes.\n\n\nBackground\n\u00b6\n\n\nIn the previous exercise, we used a method that pre-built Python and then used that pre-built package to run Python scripts. In this exercise, we will use an alternative method for running Python jobs, by writing a wrapper script that installs Python with every job. This exercise should be done in the same directory as the previous exercise - you will need the same Python source code and \nfib.py\n script.\n\n\nWrapper script\n\u00b6\n\n\nOur wrapper script will need to install Python from the source code and then run our \nfib.py\n script.\n\n\n\n\n\n\nBased on the previous exercise, what are the steps we need to install Python? What file can we use for reference?\n\n\n\n\n\n\nWe put our installation steps from the previous exercise into a file called \npython_install.txt\n. Based on this, put the installation steps into a script called \nrun_py.sh\n \n\n\n\n\n\n\nCheck your script against the file below \n\n\n#!/bin/bash\n\nexport PATH\n\nmkdir python\ntar xzf Python-3.7.0.tgz\ncd Python-3.7.0\n./configure --prefix=$(pwd)/../python\nmake\nmake install\ncd ..\n\n\n\n\n\n\n\n\n\nWe also need to run our \nfib.py\n script. We can do so by adding our installation location to the \nPATH\n, or by referencing the installation directly: \n\n\nexport\n \nPATH\n=\n$(\npwd\n)\n/python/bin:\n$PATH\n\n\npython3 fib.py \n90\n\n\n\n\n\n\nor\n\n\npython/bin/python3 fib.py \n90\n\n\n\n\n\n\nChoose whichever method you prefer, and add it to your \nrun_py.sh\n script.\n\n\n\n\n\n\nMake your \nrun_py.sh\n script executable.\n\n\n\n\n\n\nSubmit file\n\u00b6\n\n\nThe submit file for this exercise can be very similar to the \nlast one from Exercise 4.1\n.\n\n\n\n\nMake a copy of the submit file from the last exercise. What lines need to change? Make changes as appropriate.\n\n\nYou need to change the transferred tarball (the Python source, instead of our \nprebuilt_python.tar.gz\n) and the job's executable. Once you've made these changes, submit the job using \ncondor_submit\n.\n\n\nCheck for the results in the \n.out\n file.",
            "title": "Exercise 4.2"
        },
        {
            "location": "/materials/day2/part4-ex2-python-install/#wednesday-exercise-42-python-installation",
            "text": "In this exercise, you will write a wrapper script that installs Python and then use it to run jobs. This exercise should take about 10 minutes.",
            "title": "Wednesday Exercise 4.2: Python Installation"
        },
        {
            "location": "/materials/day2/part4-ex2-python-install/#background",
            "text": "In the previous exercise, we used a method that pre-built Python and then used that pre-built package to run Python scripts. In this exercise, we will use an alternative method for running Python jobs, by writing a wrapper script that installs Python with every job. This exercise should be done in the same directory as the previous exercise - you will need the same Python source code and  fib.py  script.",
            "title": "Background"
        },
        {
            "location": "/materials/day2/part4-ex2-python-install/#wrapper-script",
            "text": "Our wrapper script will need to install Python from the source code and then run our  fib.py  script.    Based on the previous exercise, what are the steps we need to install Python? What file can we use for reference?    We put our installation steps from the previous exercise into a file called  python_install.txt . Based on this, put the installation steps into a script called  run_py.sh      Check your script against the file below   #!/bin/bash\n\nexport PATH\n\nmkdir python\ntar xzf Python-3.7.0.tgz\ncd Python-3.7.0\n./configure --prefix=$(pwd)/../python\nmake\nmake install\ncd ..    We also need to run our  fib.py  script. We can do so by adding our installation location to the  PATH , or by referencing the installation directly:   export   PATH = $( pwd ) /python/bin: $PATH \n\npython3 fib.py  90   or  python/bin/python3 fib.py  90   Choose whichever method you prefer, and add it to your  run_py.sh  script.    Make your  run_py.sh  script executable.",
            "title": "Wrapper script"
        },
        {
            "location": "/materials/day2/part4-ex2-python-install/#submit-file",
            "text": "The submit file for this exercise can be very similar to the  last one from Exercise 4.1 .   Make a copy of the submit file from the last exercise. What lines need to change? Make changes as appropriate.  You need to change the transferred tarball (the Python source, instead of our  prebuilt_python.tar.gz ) and the job's executable. Once you've made these changes, submit the job using  condor_submit .  Check for the results in the  .out  file.",
            "title": "Submit file"
        },
        {
            "location": "/materials/day2/part4-ex3-python-extras/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } \n\n\n\nWednesday Exercise 4.3: Python Extras\n\u00b6\n\n\nIf you have time, try exploring the following problems: \n\n\nArguments\n\u00b6\n\n\nSimilar to what was shown in a \nprevious exercise\n, \ncan you modify your submit file and shell script so that the number provided to the \nfib.py\n script is an argument from the submit file? \n\n\nMultiple Jobs\n\u00b6\n\n\nGiven this list of numbers:\n\n\n0\n25\n80\n110\n250\n3000\n\n\n\n\n\nCan you submit a job for each number? \n\n\nPackages\n\u00b6\n\n\nWe haven't talked about adding Python packages (like \nnumpy\n or \nmatplotlib\n) to the Python installation. Where would that go in the installation process? Try creating a Python installation that includes the \nnumpy\n package.",
            "title": "Exercise 4.3"
        },
        {
            "location": "/materials/day2/part4-ex3-python-extras/#wednesday-exercise-43-python-extras",
            "text": "If you have time, try exploring the following problems:",
            "title": "Wednesday Exercise 4.3: Python Extras"
        },
        {
            "location": "/materials/day2/part4-ex3-python-extras/#arguments",
            "text": "Similar to what was shown in a  previous exercise , \ncan you modify your submit file and shell script so that the number provided to the  fib.py  script is an argument from the submit file?",
            "title": "Arguments"
        },
        {
            "location": "/materials/day2/part4-ex3-python-extras/#multiple-jobs",
            "text": "Given this list of numbers:  0\n25\n80\n110\n250\n3000  Can you submit a job for each number?",
            "title": "Multiple Jobs"
        },
        {
            "location": "/materials/day2/part4-ex3-python-extras/#packages",
            "text": "We haven't talked about adding Python packages (like  numpy  or  matplotlib ) to the Python installation. Where would that go in the installation process? Try creating a Python installation that includes the  numpy  package.",
            "title": "Packages"
        },
        {
            "location": "/materials/day3/part1-ex1-connect-start/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nThursday Exercise 1.1: Try an OSG Connect Software Module\n\u00b6\n\n\nIn this exercise, we will submit a similar job to yesterday, but use OSG Connect's built-in Python software module to run our software. \n\n\nSetup\n\u00b6\n\n\nMake sure you are logged into \ntraining.osgconnect.net\n (the OSG Connect submit server for this workshop). Create a scratch directory using your username in the \n/local-scratch\n folder if one doesn't already exist, and then \ncd\n to that folder. Copy the \nfib.py\n script from \nyesterday\n into this folder. \n\n\nModules on OSG Connect\n\u00b6\n\n\n\n\n\n\nThe software installed in the OSG Connect software repository is able to viewed and used via a module system. To see the available software modules, you can type: \n\n\nusername@training $\n module avail\n\n\n\n\n\n\n\n\n\nIf you want to search for a specific module, you can use the \nmodule spider\n command. For this example, we want to use python, so let's look for it: \n\n\nusername@training $\n module spider python\n\n\n\n\n\nWhat is the name of the available Python modules? \n\n\n\n\n\n\nFinally, in order to use the available software, the software module has to be \"loaded.\" First, let's check which version of Python is available by default: \n\n\nusername@training $\n python --version\n\n\n\n\n\n\n\n\n\nNow, what happens after we load the \npython/3.7.0\n module?\n\n\nusername@training $\n module load python/3.7.0\n\nusername@training $\n python --version\n\n\n\n\n\nNote that we won't be actually running Python on this server, but we'll use the same command inside the job to \"activate\" the Python installation. \n\n\n\n\n\n\nUsing Modules in Jobs\n\u00b6\n\n\n\n\n\n\nTo use the modules we've just seen in jobs, we can load it via a script. Take a moment to consider which commands should go into this script, and then proceed. \n\n\n\n\n\n\nThe job's executable script should look like this: \n\n\n#!/bin/bash\n\nmodule load python/3.7.0\npython3 fib.py 90\n\n\n\n\n\n\n\n\n\nThe submit file should like something like the submit files you used yesterday. Besides including requests for cpus, memory and disk, and transferring the python script, the submit file for this job should include a list of arguments that ensures the job will only run on servers where the OSG Connect software repository and modules are available:\n\n\nrequirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") && (OpSys == \"LINUX\")\n\n\n\n\n\n\n\n\n\nSubmit this job. Does it produce the expected results?",
            "title": "Exercise 1.1"
        },
        {
            "location": "/materials/day3/part1-ex1-connect-start/#thursday-exercise-11-try-an-osg-connect-software-module",
            "text": "In this exercise, we will submit a similar job to yesterday, but use OSG Connect's built-in Python software module to run our software.",
            "title": "Thursday Exercise 1.1: Try an OSG Connect Software Module"
        },
        {
            "location": "/materials/day3/part1-ex1-connect-start/#setup",
            "text": "Make sure you are logged into  training.osgconnect.net  (the OSG Connect submit server for this workshop). Create a scratch directory using your username in the  /local-scratch  folder if one doesn't already exist, and then  cd  to that folder. Copy the  fib.py  script from  yesterday  into this folder.",
            "title": "Setup"
        },
        {
            "location": "/materials/day3/part1-ex1-connect-start/#modules-on-osg-connect",
            "text": "The software installed in the OSG Connect software repository is able to viewed and used via a module system. To see the available software modules, you can type:   username@training $  module avail    If you want to search for a specific module, you can use the  module spider  command. For this example, we want to use python, so let's look for it:   username@training $  module spider python  What is the name of the available Python modules?     Finally, in order to use the available software, the software module has to be \"loaded.\" First, let's check which version of Python is available by default:   username@training $  python --version    Now, what happens after we load the  python/3.7.0  module?  username@training $  module load python/3.7.0 username@training $  python --version  Note that we won't be actually running Python on this server, but we'll use the same command inside the job to \"activate\" the Python installation.",
            "title": "Modules on OSG Connect"
        },
        {
            "location": "/materials/day3/part1-ex1-connect-start/#using-modules-in-jobs",
            "text": "To use the modules we've just seen in jobs, we can load it via a script. Take a moment to consider which commands should go into this script, and then proceed.     The job's executable script should look like this:   #!/bin/bash\n\nmodule load python/3.7.0\npython3 fib.py 90    The submit file should like something like the submit files you used yesterday. Besides including requests for cpus, memory and disk, and transferring the python script, the submit file for this job should include a list of arguments that ensures the job will only run on servers where the OSG Connect software repository and modules are available:  requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") && (OpSys == \"LINUX\")    Submit this job. Does it produce the expected results?",
            "title": "Using Modules in Jobs"
        },
        {
            "location": "/materials/day3/part1-ex2-matlab/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } \n\n\n\nTuesday Exercise 4.1: Matlab\n\u00b6\n\n\nThe goal of this exercise is to compile Matlab code and run it. This exercise will draw on the idea of writing a wrapper script to install and run code, first introduced in \nExercise 3.3\n and should take 25-30 minutes.\n\n\nBackground\n\u00b6\n\n\nMatlab is licensed; however, unlike most licensed software, it has the ability to be compiled and the compiled code can be run without a license. We will be compiling Matlab \n.m\n files into a binary file and running that binary using a set of files called the Matlab runtime.\n\n\nMatlab Code\n\u00b6\n\n\n\n\nLog in to the CHTC submit server (\nlearn.chtc.wisc.edu\n).\n\n\nCreate a directory for this exercise and \ncd\n into it . \n\n\nCopy the following code into a file called \nmatrix.m\n \nA = randi(100,4,4)\nb = randi(100,4,1);\nx = A*b\nsave results.txt x -ascii\n\n\n\n\n\n\n\n\n\nCompiling Matlab Code\n\u00b6\n\n\nThe first step in making Matlab portable is compiling our Matlab script.\nTo compile this code, we need to access the machines with the Matlab compiler installed.\nFor this exercise, we will use the compilers installed on special CHTC build machines.\nIn the CHTC pool, you can't use \nssh\n to directly connect to these machines.\nInstead, you must submit an interactive job (just like in yesterday's \nExercise 3.4\n) that\nspecifically requests these build machines.\n\n\n\n\n\n\nCreate a file called \ncompile.submit\n with the lines below: \n\n\nlog = compile.log\n\nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\ntransfer_input_files = matrix.m\n\n+IsBuildJob = true\nrequirements = ( IsBuildSlot == true )\nrequest_memory = 1GB\nrequest_disk = 100MB\n\nqueue\n\n\n\n\n\n\n\n\n\nYou can initiate the interactive job by using \ncondor_submit\n 's \n-i\n option. Enter the following command: \n\n\nusername@learn $\n condor_submit -i compile.submit\n\n\n\n\n\nMake sure you've submitted this command from \nlearn.chtc.wisc.edu\n! Once the job starts, continue with the following instructions.\n\n\n\n\n\n\nSince you are a guest user on our system, you will need to set your \nHOME\n directory by running this command: \n\n\nusername@build $\n \nexport\n \nHOME\n=\n$PWD\n\n\n\n\n\n\n\n\n\n\nThe Matlab software on these build servers is accessible via modules, just like the software installed on OSG Connect. Check which modules are available and then load the older version of Matlab. \n\n\nusername@build $\n module load MATLAB/R2015b\n\n\n\n\n\n\n\n\n\nOnce the module is loaded (you can check by running \nmodule list\n), compile the \nmatrix.m\n file with this command: \n\n\nusername@build $\n mcc -m -R -singleCompThread -R -nodisplay -R -nojvm matrix.m\n\n\n\n\n\nThe extra arguments to the \nmcc\n command are very important here. Matlab, by default, will run on as many CPUs as it can find. This can be a big problem  when running on someone else's computers, because your Matlab code might interfere with what the owner wants. The \n-singleCompThread\n option  compiles the code to run on a single CPU, avoiding this problem. In addition, the \n-nodisplay\n and \n-nojvm\n options turn off the display (which won't exist  where the code runs). \n\n\n\n\n\n\nTo exit the interactive session, type \nexit\n\n\n\n\n\n\nNow that you're back on the submit server, look at the files that were created by the Matlab compiler. Which one is the compiled binary?\n\n\n\n\n\n\nMatlab Runtime\n\u00b6\n\n\nThe newly compiled binary will require the 2015b Matlab runtime to run. You can download the runtime from the Mathworks website and build it yourself, but to save time, for this exercise you can use the pre-built runtimes hosted by CHTC.\n\n\n\n\nDownload the 2015b Matlab runtime hosted by CHTC: \nusername@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/2015b.tar.gz\n\n\n\n\n\n\n\n\n\nWrapper Script\n\u00b6\n\n\nLike the \nOpenBUGS\n example from earlier this morning, we will need a wrapper script to open the Matlab runtime and then run our compiled Matlab code. Our wrapper script will need to accomplish the following steps:\n\n\n\n\nUnpack the transferred runtime\n\n\nSet the environment variables\n\n\nRun our compiled matlab code\n\n\n\n\nFortunately, the Matlab compiler has pre-written most of this wrapper script for us!\n\n\n\n\n\n\nTake a look at \nrun_matrix.sh\n. Which of the above steps do we need to add? Once you have an idea, move to the next step.\n\n\n\n\n\n\nWe'll need to add commands to unpack the runtime (which will have been transferred with the job). Add this line to the beginning of the \nrun_matrix.sh\n file, after \n#!/bin/bash\n and the comments, but before \nexe_name=$0\n : \n\n\ntar -xzf r2015b.tar.gz\n\n\n\n\n\n\n\n\n\nLook at \nreadme.txt\n to determine what arguments our wrapper script requires. Once you have an idea, move to the next step.\n\n\n\n\n\n\nThe name of the Matlab runtime directory is a required argument to the wrapper script \nrun_matrix.sh\n. We'll have to do a little extra work to find out the name of that directory. Run this command\n\n\ntar -tf r2015b.tar.gz\n\n\n\n\n\n\n\n\n\n\nThe output of the previous command is a list of all the files in the tar.gz file. What is the name of the first folder of the path for each file? This is the name of the runtime directory, and the argument you should pass to \nrun_matrix.sh\n.  \n\n\n\n\n\n\nSubmitting the Job\n\u00b6\n\n\n\n\n\n\nCopy an existing submit file into your current directory. The submit file we used for the \nOpen Bugs\n example would be a good candidate, as that example also used a wrapper script. \n\n\n\n\n\n\nModify your submit file for this job. \n\n\n\n\n\n\nCheck your changes against the list below.\n\n\n\n\n\n\nThe \nexecutable\n for this job is going to be our wrapper script \nrun_matrix.sh\n. \n\n\nexecutable = run_matrix.sh\n\n\n\n\n\n\n\n\n\nYou need to transfer the compiled binary \nmatrix\n, as well as the runtime \n.tar.gz\n file, using \ntransfer_input_files\n. \n\n\ntransfer_input_files = matrix, r2018trainingb.tar.gz\n\n\n\n\n\n\n\n\n\nThe argument for the executable (\nrun_matrix.sh\n) is \"v90\", as that is the name of the un-tarred runtime directory. \n\n\narguments = v90\n\n\n\n\n\n\n\n\n\nWe need to request plenty of disk space for the runtime. \n\n\nrequest_disk = 2GB\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit the job using \ncondor_submit\n.  \n\n\n\n\n\n\nAfter it completes, the job should have produced a file called \nresults.txt\n.",
            "title": "Exercise 1.2"
        },
        {
            "location": "/materials/day3/part1-ex2-matlab/#tuesday-exercise-41-matlab",
            "text": "The goal of this exercise is to compile Matlab code and run it. This exercise will draw on the idea of writing a wrapper script to install and run code, first introduced in  Exercise 3.3  and should take 25-30 minutes.",
            "title": "Tuesday Exercise 4.1: Matlab"
        },
        {
            "location": "/materials/day3/part1-ex2-matlab/#background",
            "text": "Matlab is licensed; however, unlike most licensed software, it has the ability to be compiled and the compiled code can be run without a license. We will be compiling Matlab  .m  files into a binary file and running that binary using a set of files called the Matlab runtime.",
            "title": "Background"
        },
        {
            "location": "/materials/day3/part1-ex2-matlab/#matlab-code",
            "text": "Log in to the CHTC submit server ( learn.chtc.wisc.edu ).  Create a directory for this exercise and  cd  into it .   Copy the following code into a file called  matrix.m   A = randi(100,4,4)\nb = randi(100,4,1);\nx = A*b\nsave results.txt x -ascii",
            "title": "Matlab Code"
        },
        {
            "location": "/materials/day3/part1-ex2-matlab/#compiling-matlab-code",
            "text": "The first step in making Matlab portable is compiling our Matlab script.\nTo compile this code, we need to access the machines with the Matlab compiler installed.\nFor this exercise, we will use the compilers installed on special CHTC build machines.\nIn the CHTC pool, you can't use  ssh  to directly connect to these machines.\nInstead, you must submit an interactive job (just like in yesterday's  Exercise 3.4 ) that\nspecifically requests these build machines.    Create a file called  compile.submit  with the lines below:   log = compile.log\n\nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\ntransfer_input_files = matrix.m\n\n+IsBuildJob = true\nrequirements = ( IsBuildSlot == true )\nrequest_memory = 1GB\nrequest_disk = 100MB\n\nqueue    You can initiate the interactive job by using  condor_submit  's  -i  option. Enter the following command:   username@learn $  condor_submit -i compile.submit  Make sure you've submitted this command from  learn.chtc.wisc.edu ! Once the job starts, continue with the following instructions.    Since you are a guest user on our system, you will need to set your  HOME  directory by running this command:   username@build $   export   HOME = $PWD     The Matlab software on these build servers is accessible via modules, just like the software installed on OSG Connect. Check which modules are available and then load the older version of Matlab.   username@build $  module load MATLAB/R2015b    Once the module is loaded (you can check by running  module list ), compile the  matrix.m  file with this command:   username@build $  mcc -m -R -singleCompThread -R -nodisplay -R -nojvm matrix.m  The extra arguments to the  mcc  command are very important here. Matlab, by default, will run on as many CPUs as it can find. This can be a big problem  when running on someone else's computers, because your Matlab code might interfere with what the owner wants. The  -singleCompThread  option  compiles the code to run on a single CPU, avoiding this problem. In addition, the  -nodisplay  and  -nojvm  options turn off the display (which won't exist  where the code runs).     To exit the interactive session, type  exit    Now that you're back on the submit server, look at the files that were created by the Matlab compiler. Which one is the compiled binary?",
            "title": "Compiling Matlab Code"
        },
        {
            "location": "/materials/day3/part1-ex2-matlab/#matlab-runtime",
            "text": "The newly compiled binary will require the 2015b Matlab runtime to run. You can download the runtime from the Mathworks website and build it yourself, but to save time, for this exercise you can use the pre-built runtimes hosted by CHTC.   Download the 2015b Matlab runtime hosted by CHTC:  username@learn $  wget http://proxy.chtc.wisc.edu/SQUID/2015b.tar.gz",
            "title": "Matlab Runtime"
        },
        {
            "location": "/materials/day3/part1-ex2-matlab/#wrapper-script",
            "text": "Like the  OpenBUGS  example from earlier this morning, we will need a wrapper script to open the Matlab runtime and then run our compiled Matlab code. Our wrapper script will need to accomplish the following steps:   Unpack the transferred runtime  Set the environment variables  Run our compiled matlab code   Fortunately, the Matlab compiler has pre-written most of this wrapper script for us!    Take a look at  run_matrix.sh . Which of the above steps do we need to add? Once you have an idea, move to the next step.    We'll need to add commands to unpack the runtime (which will have been transferred with the job). Add this line to the beginning of the  run_matrix.sh  file, after  #!/bin/bash  and the comments, but before  exe_name=$0  :   tar -xzf r2015b.tar.gz    Look at  readme.txt  to determine what arguments our wrapper script requires. Once you have an idea, move to the next step.    The name of the Matlab runtime directory is a required argument to the wrapper script  run_matrix.sh . We'll have to do a little extra work to find out the name of that directory. Run this command  tar -tf r2015b.tar.gz     The output of the previous command is a list of all the files in the tar.gz file. What is the name of the first folder of the path for each file? This is the name of the runtime directory, and the argument you should pass to  run_matrix.sh .",
            "title": "Wrapper Script"
        },
        {
            "location": "/materials/day3/part1-ex2-matlab/#submitting-the-job",
            "text": "Copy an existing submit file into your current directory. The submit file we used for the  Open Bugs  example would be a good candidate, as that example also used a wrapper script.     Modify your submit file for this job.     Check your changes against the list below.    The  executable  for this job is going to be our wrapper script  run_matrix.sh .   executable = run_matrix.sh    You need to transfer the compiled binary  matrix , as well as the runtime  .tar.gz  file, using  transfer_input_files .   transfer_input_files = matrix, r2018trainingb.tar.gz    The argument for the executable ( run_matrix.sh ) is \"v90\", as that is the name of the un-tarred runtime directory.   arguments = v90    We need to request plenty of disk space for the runtime.   request_disk = 2GB      Submit the job using  condor_submit .      After it completes, the job should have produced a file called  results.txt .",
            "title": "Submitting the Job"
        },
        {
            "location": "/materials/day3/part2-ex1-singularity/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nThursday Exercise 2.1: Use Singularity from OSG Connect\n\u00b6\n\n\nBackground\n\u00b6\n\n\nContainers are another way to manage software installations. We don't have the time to go fully into the details of building and using containers, but can use pre-existing containers to run jobs. \n\n\nOne caveat for using containers: not all systems will support them. HTCondor has built-in features for using Docker and many Open Science Grid resources have Singularity installed, but they are not always available everywhere. \n\n\nSetup\n\u00b6\n\n\nMake sure you are logged into \ntraining.osgconnect.net\n (the OSG Connect submit server for this workshop).  For this exercise (and the next) we will be using Singularity containers that are hosted by OSG Connect, in a very similar way to the software modules. \n\n\nTo get an idea on what container images are available on the OSG, take a look at the directory path \n/cvmfs/singularity.opensciencegrid.org/opensciencegrid\n.  \n\n\nJob Submission\n\u00b6\n\n\nFor this job, we will use the OSG Connect Ubuntu \"Xenial\" image. Copy a submit file you used for a previous exercise and add the following lines: \n\n\nrequirements = HAS_SINGULARITY == true\n+SingularityImage = /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest\n\n\n\n\n\nIf you had other requirements in the submit file, remove them. These options will do two things: \n\n\n\n\nrequire that your job runs on servers that have Singularity installed and can access the OSG Connect repository of Singularity containers\n\n\ntells the job which Singularity container to use\n\n\n\n\nTo test and see if our job is really running in Ubuntu, use a simple script for the job's executable: \n\n\n#!/bin/bash\n\nhostname\nlsb_release -a\n\n\n\n\n\nSubmit the job and look at the output file.",
            "title": "Exercise 2.1"
        },
        {
            "location": "/materials/day3/part2-ex1-singularity/#thursday-exercise-21-use-singularity-from-osg-connect",
            "text": "",
            "title": "Thursday Exercise 2.1: Use Singularity from OSG Connect"
        },
        {
            "location": "/materials/day3/part2-ex1-singularity/#background",
            "text": "Containers are another way to manage software installations. We don't have the time to go fully into the details of building and using containers, but can use pre-existing containers to run jobs.   One caveat for using containers: not all systems will support them. HTCondor has built-in features for using Docker and many Open Science Grid resources have Singularity installed, but they are not always available everywhere.",
            "title": "Background"
        },
        {
            "location": "/materials/day3/part2-ex1-singularity/#setup",
            "text": "Make sure you are logged into  training.osgconnect.net  (the OSG Connect submit server for this workshop).  For this exercise (and the next) we will be using Singularity containers that are hosted by OSG Connect, in a very similar way to the software modules.   To get an idea on what container images are available on the OSG, take a look at the directory path  /cvmfs/singularity.opensciencegrid.org/opensciencegrid .",
            "title": "Setup"
        },
        {
            "location": "/materials/day3/part2-ex1-singularity/#job-submission",
            "text": "For this job, we will use the OSG Connect Ubuntu \"Xenial\" image. Copy a submit file you used for a previous exercise and add the following lines:   requirements = HAS_SINGULARITY == true\n+SingularityImage = /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest  If you had other requirements in the submit file, remove them. These options will do two things:    require that your job runs on servers that have Singularity installed and can access the OSG Connect repository of Singularity containers  tells the job which Singularity container to use   To test and see if our job is really running in Ubuntu, use a simple script for the job's executable:   #!/bin/bash\n\nhostname\nlsb_release -a  Submit the job and look at the output file.",
            "title": "Job Submission"
        },
        {
            "location": "/materials/day3/part2-ex2-tensorflow-singularity/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nThursday Exercise 2.1: Use Singularity to Run Tensorflow (Optional)\n\u00b6\n\n\nIn this tutorial, we see how to submit a [tensorflow](\nhttps://www.tensorflow.org/\n) job on the OSG through [Singularity containers](\nhttps://support.opensciencegrid.org/solution/articles/12000024676-singularity-containers\n). We currently offer CPU and GPU containers for tensorflow (both based on Ubuntu). Here, we focus on a CPU container.\n\n\nSetup\n\u00b6\n\n\nYou should still be logged into \ntraining.osgconnect.net\n (the OSG Connect submit server for this workshop).\n\n\nGet the example files and understand the job requirements.\n\u00b6\n\n\nIn order to run this example quickly, you can download all the files into a new folder using the \ntutorial\n command: \n\n\nusername@training $\n tutorial tensorflow-matmul\n\n\n\n\n\nThis creates a directory \ntutorial-tensorflow-matmul\n. Go inside the directory and see what is inside.\n\n\nusername@training $\n \ncd\n tutorial-tensorflow-matmul\n\nusername@training $\n ls -F\n\n\n\n\n\nYou will see the following files\n\n\ntf_matmul.py            (Python program to multiply two matrices using tensorflow package)\ntf_matmul.submit        (HTCondor Job description file)\ntf_matmul_wrapper.sh    (Job wrapper shell script that executes the python program)\ntf_matmul_gpu.submit    (HTCondor Job description file targeting gpus)\n\n\n\n\n\nNOTE: The file \ntf_matmul_gpu.submit\n is for gpus, but we will not focus on gpus in this exercise. You are welcome to take a look.\n\n\nThe python script `tf_matmul.py` uses tensorflow to perform the matrix multiplication of a `2x2` matrix. \n\n\nThe submit file will have similar requirements and options as our previous job, including: \n\n\nRequirements = HAS_SINGULARITY == True\n\n\n\n\n\nIn addition, we also provide the full path of the image via the keyword \n+SingularityImage\n.\n\n\n+SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest\"\n\n\n\n\n\nSubmit the tensorflow example job\n\u00b6\n\n\nNow submit the job to the OSG.\n\n\nusername@training $\n condor_submit tf_matmul.submit \n\n\n\n\n\nThe job will look for a machine on the OSG that has singularity installed. On a matched machine, the job creates the singularity container from the image \n/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest\n. Inside this container, the program \ntf_matmul.py\n begins to execute. \n\n\nAfter your job completed, you will see an output file \ntf_matmul.output\n. \n\n\nusername@training $\n cat tf_matmul.output \n\nresult of matrix multiplication\n\n\n===============================\n\n\n[[ 1.0000000e+00  0.0000000e+00]\n\n\n [-4.7683716e-07  1.0000002e+00]]\n\n\n===============================\n\n\n\n\n\n\nThe result printed in the output file should be a \n2x2\n identity matrix.",
            "title": "Exercise 2.2"
        },
        {
            "location": "/materials/day3/part2-ex2-tensorflow-singularity/#thursday-exercise-21-use-singularity-to-run-tensorflow-optional",
            "text": "In this tutorial, we see how to submit a [tensorflow]( https://www.tensorflow.org/ ) job on the OSG through [Singularity containers]( https://support.opensciencegrid.org/solution/articles/12000024676-singularity-containers ). We currently offer CPU and GPU containers for tensorflow (both based on Ubuntu). Here, we focus on a CPU container.",
            "title": "Thursday Exercise 2.1: Use Singularity to Run Tensorflow (Optional)"
        },
        {
            "location": "/materials/day3/part2-ex2-tensorflow-singularity/#setup",
            "text": "You should still be logged into  training.osgconnect.net  (the OSG Connect submit server for this workshop).",
            "title": "Setup"
        },
        {
            "location": "/materials/day3/part2-ex2-tensorflow-singularity/#get-the-example-files-and-understand-the-job-requirements",
            "text": "In order to run this example quickly, you can download all the files into a new folder using the  tutorial  command:   username@training $  tutorial tensorflow-matmul  This creates a directory  tutorial-tensorflow-matmul . Go inside the directory and see what is inside.  username@training $   cd  tutorial-tensorflow-matmul username@training $  ls -F  You will see the following files  tf_matmul.py            (Python program to multiply two matrices using tensorflow package)\ntf_matmul.submit        (HTCondor Job description file)\ntf_matmul_wrapper.sh    (Job wrapper shell script that executes the python program)\ntf_matmul_gpu.submit    (HTCondor Job description file targeting gpus)  NOTE: The file  tf_matmul_gpu.submit  is for gpus, but we will not focus on gpus in this exercise. You are welcome to take a look.  The python script `tf_matmul.py` uses tensorflow to perform the matrix multiplication of a `2x2` matrix.   The submit file will have similar requirements and options as our previous job, including:   Requirements = HAS_SINGULARITY == True  In addition, we also provide the full path of the image via the keyword  +SingularityImage .  +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest\"",
            "title": "Get the example files and understand the job requirements."
        },
        {
            "location": "/materials/day3/part2-ex2-tensorflow-singularity/#submit-the-tensorflow-example-job",
            "text": "Now submit the job to the OSG.  username@training $  condor_submit tf_matmul.submit   The job will look for a machine on the OSG that has singularity installed. On a matched machine, the job creates the singularity container from the image  /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest . Inside this container, the program  tf_matmul.py  begins to execute.   After your job completed, you will see an output file  tf_matmul.output .   username@training $  cat tf_matmul.output  result of matrix multiplication  ===============================  [[ 1.0000000e+00  0.0000000e+00]   [-4.7683716e-07  1.0000002e+00]]  ===============================   The result printed in the output file should be a  2x2  identity matrix.",
            "title": "Submit the tensorflow example job"
        },
        {
            "location": "/materials/day3/part2-ex3-docker/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } \n\n\n\nWednesday Exercise 2.3: Using Docker\n\u00b6\n\n\nIn this exercise, you will run the same Python script as the previous exercises, but using a Docker container.\n\n\nSetup\n\u00b6\n\n\nFor this exercise, you will need to be logged into \nlearn.chtc.wisc.edu\n, not \ntraining.osgconnect.net\n. \n\n\nSubmit File Changes\n\u00b6\n\n\n\n\nMake a copy of your submit file from the \nprevious Python exercise\n.\n\n\n\n\nAdd the following three lines to the submit file or modify existing lines to match the lines below: \n\n\nuniverse = docker\ndocker_image = python:3.7.0-stretch\n\n\n\n\n\nHere we are requesting HTCondor's Docker universe and using a pre-built python image that, by default, will be pulled from a public website of Docker images called DockerHub.  The requirements line will ensure that we run on computers whose operating system can support Docker.\n\n\n\n\n\n\nAdjust the executable and arguments lines. The executable can now be the Python script itself, with the appropriate arguments: \n\n\nexecutable = fib.py\narguments = 90\n\n\n\n\n\n\n\n\n\nFinally, we no longer need to transfer a Python tarball (whether source code or pre-built) or our Python script. You can remove both from the \ntransfer_input_files\n line of the submit file.\n\n\n\n\n\n\nPython Script\n\u00b6\n\n\n\n\n\n\nOpen the Python script and add the following line at the top: \n\n\n#!/usr/bin/env python3\n\n\n\n\n\nThis will ensure that the script uses the version of Python that comes in the Docker container.\n\n\n\n\n\n\nOnce these steps are done, submit the job.",
            "title": "Exercise 2.3"
        },
        {
            "location": "/materials/day3/part2-ex3-docker/#wednesday-exercise-23-using-docker",
            "text": "In this exercise, you will run the same Python script as the previous exercises, but using a Docker container.",
            "title": "Wednesday Exercise 2.3: Using Docker"
        },
        {
            "location": "/materials/day3/part2-ex3-docker/#setup",
            "text": "For this exercise, you will need to be logged into  learn.chtc.wisc.edu , not  training.osgconnect.net .",
            "title": "Setup"
        },
        {
            "location": "/materials/day3/part2-ex3-docker/#submit-file-changes",
            "text": "Make a copy of your submit file from the  previous Python exercise .   Add the following three lines to the submit file or modify existing lines to match the lines below:   universe = docker\ndocker_image = python:3.7.0-stretch  Here we are requesting HTCondor's Docker universe and using a pre-built python image that, by default, will be pulled from a public website of Docker images called DockerHub.  The requirements line will ensure that we run on computers whose operating system can support Docker.    Adjust the executable and arguments lines. The executable can now be the Python script itself, with the appropriate arguments:   executable = fib.py\narguments = 90    Finally, we no longer need to transfer a Python tarball (whether source code or pre-built) or our Python script. You can remove both from the  transfer_input_files  line of the submit file.",
            "title": "Submit File Changes"
        },
        {
            "location": "/materials/day3/part2-ex3-docker/#python-script",
            "text": "Open the Python script and add the following line at the top:   #!/usr/bin/env python3  This will ensure that the script uses the version of Python that comes in the Docker container.    Once these steps are done, submit the job.",
            "title": "Python Script"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/",
            "text": "Thursday Exercise 1.1: Understanding Data Requirements\n\u00b6\n\n\nBackground\n\u00b6\n\n\nThis exercise's goal is to learn to think critically about an application's data needs, especially before submitting a large batch of jobs or using tools for delivering large data to jobs. In this exercise we will attempt to understand the input and output of the bioinformatics application \nBLAST\n, which you used yesterday in \nExercise 1.2\n.\n\n\nSetup\n\u00b6\n\n\n\n\nMake sure you are logged into \ntraining.osgconnect.net\n.\n\n\nNavigate to your \nstash\n directory within the home directory, and create a directory for this exercise named \nthur-blast\n.\n\n\n\n\nCopy the blast executables:\n\u00b6\n\n\nuser@training $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/ncbi-blast-2.7.1+-x64-linux.tar.gz\n\nuser@training $\n tar xzf ncbi-blast-2.7.1+-x64-linux.tar.gz\n\n\n\n\n\nCopy the Input Files\n\u00b6\n\n\nTo run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information.\n\n\n\n\nDownload these files to your current directory:\n\n\n\n\nuser@training $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/pdbaa.tar.gz\n\nuser@training $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/mouse.fa\n\n\n\n\n\n\n\nUntar the \npdbaa\n database:\n\n\n\n\nuser@training $\n tar -xzf pdbaa.tar.gz\n\n\n\n\n\nUnderstanding blast\n\u00b6\n\n\nRemember that \nblastx\n is executed in a command like the following:\n\n\nuser@training $\n blastx -db \ndatabase_rootname\n -query \ninput_file\n -out \nresults_file\n\n\n\n\n\n\nIn the above, the \ninput_file\n is a file containing a number of genetic sequences, and the database that these are compared against is made up of several files that begin with the same root name (we previously used the \"pdbaa\" database, whose files all begin with \npdbaa\n). The output from this analysis will be printed to a results file that is also indicated in the command.\n\n\nAdding up data needs\n\u00b6\n\n\nLooking at the files from the \nblastx\n jobs you ran Tuesday, add up the amount of data that was needed for running the job. (If you deleted any of the files from Tuesday, just resubmit the job before proceeding.) Here are the commands that will be most useful to you:\n\n\nHow to see the size of a specific file:\n\n\nuser@training $\n ls -lh \n<FILE>\n\n\n\n\n\n\nHow to see the size of all files in the current directory:\n\n\nuser@training $\n ls -lh\n\n\n\n\n\nHow to determine the total amount of data in the current directory: \n\n\nuser@training $\n du -sh \n<DIRECTORY>\n\n\n\n\n\n\nInput requirements\n\u00b6\n\n\nLooking at Tuesdays's exercise, total up the amount of data in all of the files necessary to run the \nblastx\n job (which will include the executable, itself). Write down this number. Also take note of how much total data in in the \npdbaa\n directory.  Remember, \nblastx\n reads the un-compressed \npdbaa\n files.\n\n\n\n\nCompressed Files\n\n\nRemember, \nblastx\n reads the un-compressed \npdbaa\n files.\n\n\n\n\nOutput requirements\n\u00b6\n\n\nThe output that we care about from \nblastx\n is saved in the file whose name is indicated after the \n-out\n argument to \nblastx\n. However, remember that HTCondor also creates the error, output, and log files, which you'll need to add up, too. Are there any other files? Total all of these together, as well.\n\n\nTalk about this as a group!\n\u00b6\n\n\nOnce you have completed the above tasks, we'll talk about the totals as a group.\n\n\n\n\nHow much disk space is required on the submit server for one blastx run with the input file you used before? (Input data)\n\n\nHow much disk space is required on the worker node? (uncompressed + output data)\n\n\nHow \nmany\n files are needed and created for each run? (Output data)\n\n\nAssuming that each file is read completely by BLAST, and since you know how long blastx runs (time it):\n\n\nAt what rate are files read in?\n\n\nHow many MB/s?\n\n\n\n\n\n\nHow much total disk space would be necessary on the submit server to run 10 jobs? (remember that some of the files will be shared by all 10 jobs, and will not be multiplied)\n\n\n\n\n\n\n\nUp next!\n\u00b6\n\n\nNext you will create a HTCondor submit script to transfer the Blast input files in order to run Blast on a worker nodes. \nNext Exercise",
            "title": "Exercise 1.1"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/#thursday-exercise-11-understanding-data-requirements",
            "text": "",
            "title": "Thursday Exercise 1.1: Understanding Data Requirements"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/#background",
            "text": "This exercise's goal is to learn to think critically about an application's data needs, especially before submitting a large batch of jobs or using tools for delivering large data to jobs. In this exercise we will attempt to understand the input and output of the bioinformatics application  BLAST , which you used yesterday in  Exercise 1.2 .",
            "title": "Background"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/#setup",
            "text": "Make sure you are logged into  training.osgconnect.net .  Navigate to your  stash  directory within the home directory, and create a directory for this exercise named  thur-blast .",
            "title": "Setup"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/#copy-the-blast-executables",
            "text": "user@training $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/ncbi-blast-2.7.1+-x64-linux.tar.gz user@training $  tar xzf ncbi-blast-2.7.1+-x64-linux.tar.gz",
            "title": "Copy the blast executables:"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/#copy-the-input-files",
            "text": "To run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information.   Download these files to your current directory:   user@training $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/pdbaa.tar.gz user@training $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/mouse.fa   Untar the  pdbaa  database:   user@training $  tar -xzf pdbaa.tar.gz",
            "title": "Copy the Input Files"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/#understanding-blast",
            "text": "Remember that  blastx  is executed in a command like the following:  user@training $  blastx -db  database_rootname  -query  input_file  -out  results_file   In the above, the  input_file  is a file containing a number of genetic sequences, and the database that these are compared against is made up of several files that begin with the same root name (we previously used the \"pdbaa\" database, whose files all begin with  pdbaa ). The output from this analysis will be printed to a results file that is also indicated in the command.",
            "title": "Understanding blast"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/#adding-up-data-needs",
            "text": "Looking at the files from the  blastx  jobs you ran Tuesday, add up the amount of data that was needed for running the job. (If you deleted any of the files from Tuesday, just resubmit the job before proceeding.) Here are the commands that will be most useful to you:  How to see the size of a specific file:  user@training $  ls -lh  <FILE>   How to see the size of all files in the current directory:  user@training $  ls -lh  How to determine the total amount of data in the current directory:   user@training $  du -sh  <DIRECTORY>",
            "title": "Adding up data needs"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/#input-requirements",
            "text": "Looking at Tuesdays's exercise, total up the amount of data in all of the files necessary to run the  blastx  job (which will include the executable, itself). Write down this number. Also take note of how much total data in in the  pdbaa  directory.  Remember,  blastx  reads the un-compressed  pdbaa  files.   Compressed Files  Remember,  blastx  reads the un-compressed  pdbaa  files.",
            "title": "Input requirements"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/#output-requirements",
            "text": "The output that we care about from  blastx  is saved in the file whose name is indicated after the  -out  argument to  blastx . However, remember that HTCondor also creates the error, output, and log files, which you'll need to add up, too. Are there any other files? Total all of these together, as well.",
            "title": "Output requirements"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/#talk-about-this-as-a-group",
            "text": "Once you have completed the above tasks, we'll talk about the totals as a group.   How much disk space is required on the submit server for one blastx run with the input file you used before? (Input data)  How much disk space is required on the worker node? (uncompressed + output data)  How  many  files are needed and created for each run? (Output data)  Assuming that each file is read completely by BLAST, and since you know how long blastx runs (time it):  At what rate are files read in?  How many MB/s?    How much total disk space would be necessary on the submit server to run 10 jobs? (remember that some of the files will be shared by all 10 jobs, and will not be multiplied)",
            "title": "Talk about this as a group!"
        },
        {
            "location": "/materials/day4/part1-ex1-data-needs/#up-next",
            "text": "Next you will create a HTCondor submit script to transfer the Blast input files in order to run Blast on a worker nodes.  Next Exercise",
            "title": "Up next!"
        },
        {
            "location": "/materials/day4/part1-ex2-file-transfer/",
            "text": "Thursday Exercise 1.2: File Compression and Testing Resource Requirements\n\u00b6\n\n\nThe objective of this exercise is to refresh yourself on HTCondor file transfer, to implement file compression, and to begin examining the memory and disk space used by your jobs in order to plan larger batches, which we'll tackle in later exercises today.\n\n\nSetup\n\u00b6\n\n\n\n\nMake sure you are still logged into \ntraining.osgconnect.net\n\n\nMake a directory for today's blast exercises named \nthur-blast-data\n inside your \nstash\n directory, and change into it.\n\n\n\n\nThe executable we'll use in this exercise and later today is the same \nblastx\n executable from the \nprevious exercise\n.\n\n\nAlso copy the data from the last exercise into the \nthur-blast-data\n directory. You'll need the \nmouse.fa\n file and the \npdbaa\n directory from the last exercise, but you'll end up making a new submit file.\n\n\nReview: HTCondor File Transfer\n\u00b6\n\n\n\n\nRecall that OSG does \nNOT\n have a shared filesystem!  Instead, HTCondor \ntransfers\n your executable and input files\n(listed with \ntransfer_input_files\n) to a working directory on the execute node, regardless of how these files were\narranged on the submit node.  In this exercise we'll use the same \nblastx\n example job that we used previously, but\nmodify the submit file and test how much memory and disk space it uses on the execute node.\n\n\nStart with a test submit file\n\u00b6\n\n\nWe've started a submit file for you, below, which you'll add to in the remaining steps.\n\n\nexecutable = \ntransfer_input_files = \noutput = test.out\nerror = test.error\nlog = test.log\nrequest_memory = \nrequest_disk = \nrequest_cpus = 1\nrequirements = (OpSys == \"LINUX\")\nqueue\n\n\n\n\n\nImplement file compression\n\u00b6\n\n\nIn our first blast job from yesterday, the database files in the \npdbaa\n directory were all transferred, as is, but we could instead transfer them as a single, compressed file using \ntar\n. For a second test job, let's compress our blast database files to send them to the submit node as a single \ntar.gz\n file, by following the below steps:\n\n\n\n\n\n\nChange into the \npdbaa\n directory and compress the database files into a single file called \npdbaa_files.tar.gz\n using the \ntar\n command. (NOTE: This file will be different from the \npdbaa.tar.gz\n files you downloaded yesterday, because it will only contain the \npdbaa\n files, and not the \npdbaa\n directory, itself.)\n\n\nA typical command for creating a tar file is:\n\n\nuser@training $\n tar cvzf \n<COMPRESSED FILENAME>\n \n<LIST OF FILES OR DIRECTORIES>\n\n\n\n\n\n\nMove this file to the \nthur-blast-data\n directory.\n\n\n\n\n\n\nCreate a wrapper script that will first decompress the \npdbaa_files.tar.gz\n file, and then run blast.\n\n\nBecause this file will now be our submit file \nexecutable\n, we'll also end up transferring the \nblastx\n executable with \ntransfer_input_files\n. In the \nthur-blast-data\n directory, create a new file, called \nblast_wrapper.sh\n, with the following contents:\n\n\n#!/bin/bash\n\ntar xvzf pdbaa_files.tar.gz\n\n./blastx -db pdbaa -query mouse.fa -out mouse.fa.result\n\nrm pdbaa.*\n\n\n\n\n\n\n\nExtra Files!\n\n\nThe last line removes the resulting database files that came from \npdbaa_files.tar.gz\n, as these files would otherwise be copied back to the submit server as perceived output (because they're new files that HTCondor didn't transfer over as input).\n\n\n\n\n\n\n\n\nList the executable and input files\n\u00b6\n\n\nMake sure to update the submit file with the following:\n\n\n\n\nAdd the new \nexecutable\n (the wrapper script you created above)\n\n\nIn \ntransfer_input_files\n, list the \nblastx\n binary, the \npdbaa_files.tar.gz\n file, and the input query file.\n\n\n\n\n\n\nCommas, commas everywhere!\n\n\nRemember that \ntransfer_input_files\n accepts a comma separated list of files, and that you need to list the full location of the \nblastx\n executable (\nblastx\n). There will be no arguments, since the arguments to the \nblastx\n command are now captured in the wrapper script.\n\n\n\n\nPredict memory and disk requests from your data\n\u00b6\n\n\nAlso, think about how much memory and disk to request for this job. It's good to start with values that are a little higher than you think a test job will need, but think about:\n\n\n\n\nHow much memory \nblastx\n would use if it loaded all of the database files \nand\n the query input file into memory.\n\n\nHow much disk space will be necessary on the execute server for the executable, all input files, and all output files (hint: the log file only exists on the submit node).\n\n\nwhether you'd like to request some extra memory or disk space, just in case\n\n\n\n\nLook at the \nlog\n file for your \nblastx\n job from Tuesday, and compare the memory and disk \"Usage\" to what you predicted from the files. Make sure to update the submit file with more accurate memory and disk requests (you may still want to request slightly more than the job actually used).\n\n\nRun the test job\n\u00b6\n\n\nOnce you have finished editing the submit file, go ahead and submit the job. It should take a few minutes to complete, and then you can check to make sure that no unwanted files (especially the \npdbaa\n database files) were copied back at the end of the job.\n\n\nRun a \ndu -sh\n on the directory with this job's input. How does it compare to the directory from Tuesday, and why?\n\n\nWhen you've completed the above, continue with the \nnext exercise\n.",
            "title": "Exercise 1.2"
        },
        {
            "location": "/materials/day4/part1-ex2-file-transfer/#thursday-exercise-12-file-compression-and-testing-resource-requirements",
            "text": "The objective of this exercise is to refresh yourself on HTCondor file transfer, to implement file compression, and to begin examining the memory and disk space used by your jobs in order to plan larger batches, which we'll tackle in later exercises today.",
            "title": "Thursday Exercise 1.2: File Compression and Testing Resource Requirements"
        },
        {
            "location": "/materials/day4/part1-ex2-file-transfer/#setup",
            "text": "Make sure you are still logged into  training.osgconnect.net  Make a directory for today's blast exercises named  thur-blast-data  inside your  stash  directory, and change into it.   The executable we'll use in this exercise and later today is the same  blastx  executable from the  previous exercise .  Also copy the data from the last exercise into the  thur-blast-data  directory. You'll need the  mouse.fa  file and the  pdbaa  directory from the last exercise, but you'll end up making a new submit file.",
            "title": "Setup"
        },
        {
            "location": "/materials/day4/part1-ex2-file-transfer/#review-htcondor-file-transfer",
            "text": "Recall that OSG does  NOT  have a shared filesystem!  Instead, HTCondor  transfers  your executable and input files\n(listed with  transfer_input_files ) to a working directory on the execute node, regardless of how these files were\narranged on the submit node.  In this exercise we'll use the same  blastx  example job that we used previously, but\nmodify the submit file and test how much memory and disk space it uses on the execute node.",
            "title": "Review: HTCondor File Transfer"
        },
        {
            "location": "/materials/day4/part1-ex2-file-transfer/#start-with-a-test-submit-file",
            "text": "We've started a submit file for you, below, which you'll add to in the remaining steps.  executable = \ntransfer_input_files = \noutput = test.out\nerror = test.error\nlog = test.log\nrequest_memory = \nrequest_disk = \nrequest_cpus = 1\nrequirements = (OpSys == \"LINUX\")\nqueue",
            "title": "Start with a test submit file"
        },
        {
            "location": "/materials/day4/part1-ex2-file-transfer/#implement-file-compression",
            "text": "In our first blast job from yesterday, the database files in the  pdbaa  directory were all transferred, as is, but we could instead transfer them as a single, compressed file using  tar . For a second test job, let's compress our blast database files to send them to the submit node as a single  tar.gz  file, by following the below steps:    Change into the  pdbaa  directory and compress the database files into a single file called  pdbaa_files.tar.gz  using the  tar  command. (NOTE: This file will be different from the  pdbaa.tar.gz  files you downloaded yesterday, because it will only contain the  pdbaa  files, and not the  pdbaa  directory, itself.)  A typical command for creating a tar file is:  user@training $  tar cvzf  <COMPRESSED FILENAME>   <LIST OF FILES OR DIRECTORIES>   Move this file to the  thur-blast-data  directory.    Create a wrapper script that will first decompress the  pdbaa_files.tar.gz  file, and then run blast.  Because this file will now be our submit file  executable , we'll also end up transferring the  blastx  executable with  transfer_input_files . In the  thur-blast-data  directory, create a new file, called  blast_wrapper.sh , with the following contents:  #!/bin/bash\n\ntar xvzf pdbaa_files.tar.gz\n\n./blastx -db pdbaa -query mouse.fa -out mouse.fa.result\n\nrm pdbaa.*   Extra Files!  The last line removes the resulting database files that came from  pdbaa_files.tar.gz , as these files would otherwise be copied back to the submit server as perceived output (because they're new files that HTCondor didn't transfer over as input).",
            "title": "Implement file compression"
        },
        {
            "location": "/materials/day4/part1-ex2-file-transfer/#list-the-executable-and-input-files",
            "text": "Make sure to update the submit file with the following:   Add the new  executable  (the wrapper script you created above)  In  transfer_input_files , list the  blastx  binary, the  pdbaa_files.tar.gz  file, and the input query file.    Commas, commas everywhere!  Remember that  transfer_input_files  accepts a comma separated list of files, and that you need to list the full location of the  blastx  executable ( blastx ). There will be no arguments, since the arguments to the  blastx  command are now captured in the wrapper script.",
            "title": "List the executable and input files"
        },
        {
            "location": "/materials/day4/part1-ex2-file-transfer/#predict-memory-and-disk-requests-from-your-data",
            "text": "Also, think about how much memory and disk to request for this job. It's good to start with values that are a little higher than you think a test job will need, but think about:   How much memory  blastx  would use if it loaded all of the database files  and  the query input file into memory.  How much disk space will be necessary on the execute server for the executable, all input files, and all output files (hint: the log file only exists on the submit node).  whether you'd like to request some extra memory or disk space, just in case   Look at the  log  file for your  blastx  job from Tuesday, and compare the memory and disk \"Usage\" to what you predicted from the files. Make sure to update the submit file with more accurate memory and disk requests (you may still want to request slightly more than the job actually used).",
            "title": "Predict memory and disk requests from your data"
        },
        {
            "location": "/materials/day4/part1-ex2-file-transfer/#run-the-test-job",
            "text": "Once you have finished editing the submit file, go ahead and submit the job. It should take a few minutes to complete, and then you can check to make sure that no unwanted files (especially the  pdbaa  database files) were copied back at the end of the job.  Run a  du -sh  on the directory with this job's input. How does it compare to the directory from Tuesday, and why?  When you've completed the above, continue with the  next exercise .",
            "title": "Run the test job"
        },
        {
            "location": "/materials/day4/part1-ex3-blast-split/",
            "text": "Thursday Exercise 1.3: Splitting Large Input for Better Throughput\n\u00b6\n\n\nThe objective of this exercise is to prepare for \nblasting\n a much larger input query file by splitting the input for greater throughput and lower memory and disk requirements. Splitting the input will also mean that we don't have to rely on additional large-data measures for the input query files.\n\n\nSetup\n\u00b6\n\n\n\n\nMake sure you are still logged into \ntraining.osgconnect.net\n\n\nMake sure you are in the directory named \nthur-blast-data\n under the \nstash\n filesystem.\n\n\n\n\nObtain the large input\n\u00b6\n\n\nWe've previously used \nblastx\n to analyze a relatively small input file of test data, \nmouse.fa\n, but let's imagine that you now need to blast a much larger dataset for your research. This dataset can be downloaded with the following command:\n\n\nuser@training $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/mouse_rna.tar.gz\n\n\n\n\n\nAfter un-tar'ing the file, you should be able to confirm that it's size is roughly 100 MB. Not only is this a bit large for file transfer, but it would take hours to complete a single \nblastx\n analysis for it. Also, the single output file would be huge. Compare for yourself to the time and output file size for the mouse.fa input file, according to your test job in the last exercise.\n\n\nSplit the input file\n\u00b6\n\n\nFor \nblast\n, it's scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! (Importantly, blast databases should not be split, because the \nblast\n output includes a score value for each sequence that is calculated relative to the entire length of the database.)\n\n\nBecause genetic sequence data is used heavily across the life science, there are also tools for splitting up the data into smaller files. One of these is called \ngenome tools\n, and you can download a package of precompiled binaries (just like blast) using the following command:\n\n\nuser@training $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/gt-1.5.10-Linux_x86_64-64bit-complete.tar.gz\n\n\n\n\n\nUn-tar the gt package (\ntar -xzvf ...\n), then run it's sequence file splitter as follows, with the target file size of 1 MB:\n\n\nuser@training $\n ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize \n1\n mouse_rna.fa\n\n\n\n\n\nYou'll notice that the result is a set of 100 files, all about the size of 1 MB, and numbered 1 through 100.\n\n\nTest a split job\n\u00b6\n\n\nNow, you'll run a test job to prepare for submitting many jobs, later, where each will use a different input file.\n\n\nModify the submit file\n\u00b6\n\n\nFirst, you'll create a new submit file that passes the input filename as an argument and use a list of applicable filenames. Follow the below steps:\n\n\n\n\n\n\nCopy the submit file to a new file called \nblast_split.sub\n and modify the \"queue\" line of the submit file to the following:\n\n\nqueue inputfile matching mouse_rna.fa.*\n\n\n\n\n\n(If this queue line looks like we should be scanning all of the rna files, wait until the next exercise)\n\n\n\n\n\n\nReplace the \nmouse.fa\n instances in the submit file with \n$(inputfile)\n, and rename the output, log, and error files to use the same \ninputfile\n variable:\n\n\noutput = $(inputfile).out\nerror = $(inputfile).err\nlog = $(inputfile).log\n\n\n\n\n\n\n\n\n\nAdd an \narguments\n line to the submit file so it will pass the name of the input file to the wrapper script\n\n\narguments = $(inputfile)\n\n\n\n\n\n\n\n\n\nAdd the \n$(inputfile)\n to the tranfer_input_files line\n\n\ntransfer_finput_files = ... , $(inputfile)\n\n\n\n\n\n\n\n\n\nUpdate the memory and disk requests, since the new input file is larger and will also produce larger output. It may be best to overestimate to something like 1 GB for each. (After completing this test, you'll be able to update them to a more accurate value.)\n\n\n\n\n\n\nModify the wrapper file\n\u00b6\n\n\nReplace instances of the input file name in the \nblast_wrapper.sh\n script so that it will insert the first argument in place of the input filename, like so:\n\n\n./blastx -db pdbaa -query $1 -out $1.result\n\n\n\n\n\nNOTE: bash shell scripts will use the first argument in place of \n$1\n, the second argument as \n$2\n, etc.\n\n\nSubmit the test job\n\u00b6\n\n\nThis job will take a bit longer than the job in the last exercise, since the input file is larger (by about 3-fold). Again, make sure that only the desired \noutput\n, \nerror\n, and \nresult\n files come back at the end of the job.  In my tests, the jobs ran for 7-12 minutes.\n\n\n\n\nJobs on jobs!\n\n\nBe careful to not submit the job again. Why?  Our queue statement says \n... matching mouse_rna.fa.*\n, and look at the current directory.  There are new files named \nmouse_rna.fa.X.log\n and other files.  Submitting again, the \nqueue\n statement would see these new files, and try to run blast on them!\n\n\nIf you want to remove all of the extra files, you can try:\n\n\nuser@training $\n rm *.error *.log *.out *.result\n\n\n\n\n\n\n\nUpdate the resource requests\n\u00b6\n\n\nAfter the job finishes successfully, examine the \nlog\n file for memory and disk usage, and update the requests in the submit file. In \nExercise 2.1\n (after the next lecture) you'll submit many jobs at once \nand\n use a different method for handling the \npdbaa_files.tar.gz\n file, which is a bit too large to use regular file transfer when submitting many jobs.",
            "title": "Exercise 1.3"
        },
        {
            "location": "/materials/day4/part1-ex3-blast-split/#thursday-exercise-13-splitting-large-input-for-better-throughput",
            "text": "The objective of this exercise is to prepare for  blasting  a much larger input query file by splitting the input for greater throughput and lower memory and disk requirements. Splitting the input will also mean that we don't have to rely on additional large-data measures for the input query files.",
            "title": "Thursday Exercise 1.3: Splitting Large Input for Better Throughput"
        },
        {
            "location": "/materials/day4/part1-ex3-blast-split/#setup",
            "text": "Make sure you are still logged into  training.osgconnect.net  Make sure you are in the directory named  thur-blast-data  under the  stash  filesystem.",
            "title": "Setup"
        },
        {
            "location": "/materials/day4/part1-ex3-blast-split/#obtain-the-large-input",
            "text": "We've previously used  blastx  to analyze a relatively small input file of test data,  mouse.fa , but let's imagine that you now need to blast a much larger dataset for your research. This dataset can be downloaded with the following command:  user@training $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/mouse_rna.tar.gz  After un-tar'ing the file, you should be able to confirm that it's size is roughly 100 MB. Not only is this a bit large for file transfer, but it would take hours to complete a single  blastx  analysis for it. Also, the single output file would be huge. Compare for yourself to the time and output file size for the mouse.fa input file, according to your test job in the last exercise.",
            "title": "Obtain the large input"
        },
        {
            "location": "/materials/day4/part1-ex3-blast-split/#split-the-input-file",
            "text": "For  blast , it's scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! (Importantly, blast databases should not be split, because the  blast  output includes a score value for each sequence that is calculated relative to the entire length of the database.)  Because genetic sequence data is used heavily across the life science, there are also tools for splitting up the data into smaller files. One of these is called  genome tools , and you can download a package of precompiled binaries (just like blast) using the following command:  user@training $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/gt-1.5.10-Linux_x86_64-64bit-complete.tar.gz  Un-tar the gt package ( tar -xzvf ... ), then run it's sequence file splitter as follows, with the target file size of 1 MB:  user@training $  ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize  1  mouse_rna.fa  You'll notice that the result is a set of 100 files, all about the size of 1 MB, and numbered 1 through 100.",
            "title": "Split the input file"
        },
        {
            "location": "/materials/day4/part1-ex3-blast-split/#test-a-split-job",
            "text": "Now, you'll run a test job to prepare for submitting many jobs, later, where each will use a different input file.",
            "title": "Test a split job"
        },
        {
            "location": "/materials/day4/part1-ex3-blast-split/#modify-the-submit-file",
            "text": "First, you'll create a new submit file that passes the input filename as an argument and use a list of applicable filenames. Follow the below steps:    Copy the submit file to a new file called  blast_split.sub  and modify the \"queue\" line of the submit file to the following:  queue inputfile matching mouse_rna.fa.*  (If this queue line looks like we should be scanning all of the rna files, wait until the next exercise)    Replace the  mouse.fa  instances in the submit file with  $(inputfile) , and rename the output, log, and error files to use the same  inputfile  variable:  output = $(inputfile).out\nerror = $(inputfile).err\nlog = $(inputfile).log    Add an  arguments  line to the submit file so it will pass the name of the input file to the wrapper script  arguments = $(inputfile)    Add the  $(inputfile)  to the tranfer_input_files line  transfer_finput_files = ... , $(inputfile)    Update the memory and disk requests, since the new input file is larger and will also produce larger output. It may be best to overestimate to something like 1 GB for each. (After completing this test, you'll be able to update them to a more accurate value.)",
            "title": "Modify the submit file"
        },
        {
            "location": "/materials/day4/part1-ex3-blast-split/#modify-the-wrapper-file",
            "text": "Replace instances of the input file name in the  blast_wrapper.sh  script so that it will insert the first argument in place of the input filename, like so:  ./blastx -db pdbaa -query $1 -out $1.result  NOTE: bash shell scripts will use the first argument in place of  $1 , the second argument as  $2 , etc.",
            "title": "Modify the wrapper file"
        },
        {
            "location": "/materials/day4/part1-ex3-blast-split/#submit-the-test-job",
            "text": "This job will take a bit longer than the job in the last exercise, since the input file is larger (by about 3-fold). Again, make sure that only the desired  output ,  error , and  result  files come back at the end of the job.  In my tests, the jobs ran for 7-12 minutes.   Jobs on jobs!  Be careful to not submit the job again. Why?  Our queue statement says  ... matching mouse_rna.fa.* , and look at the current directory.  There are new files named  mouse_rna.fa.X.log  and other files.  Submitting again, the  queue  statement would see these new files, and try to run blast on them!  If you want to remove all of the extra files, you can try:  user@training $  rm *.error *.log *.out *.result",
            "title": "Submit the test job"
        },
        {
            "location": "/materials/day4/part1-ex3-blast-split/#update-the-resource-requests",
            "text": "After the job finishes successfully, examine the  log  file for memory and disk usage, and update the requests in the submit file. In  Exercise 2.1  (after the next lecture) you'll submit many jobs at once  and  use a different method for handling the  pdbaa_files.tar.gz  file, which is a bit too large to use regular file transfer when submitting many jobs.",
            "title": "Update the resource requests"
        },
        {
            "location": "/materials/day4/part2-ex1-blast-proxy/",
            "text": "Thursday Exercise 2.1: Using a Web Proxy for Large Shared Input\n\u00b6\n\n\nContinuing the series of exercises blasting mouse genetic sequences, the objective of this exercise is to use a web proxy to stage the large database, which will be downloaded into each of many jobs that use the split input files from the last exercise (Exercise 2.3).\n\n\nSetup\n\u00b6\n\n\n\n\nMake sure you are logged into \ntraining.osgconnect.net\n\n\nMake sure you are in the same directory as the previous exercise, \nExercise 2.3\n directory named \nthur-blast-data\n.\n\n\n\n\nPlace the Large File on the Proxy\n\u00b6\n\n\nFirst, you'll need to put the \npdbaa_files.tar.gz\n file onto the Stash web directory. Use the following command:\n\n\nuser@training $\n cp pdbaa_files.tar.gz ~/stash/public/\n\n\n\n\n\nTest a download of the file\n\u00b6\n\n\nOnce the file is placed in the ~/stash/public directory, it can be downloaded from a corresponding URL such as \nhttp://stash.osgconnect.net/~\nusername\n/pdbaa_files.tar.gz\n, where \nusername\n is your username on \ntraining.osgconnect.net\n.\n\n\nUsing the above convention (and from a different directory on \ntraining.osgconnect.net\n, any directory), you can test the download of your \npdbaa_files.tar.gz\n file with a command like the following:\n\n\nuser@training $\n wget http://stash.osgconnect.net/~\nusername\n/pdbaa_files.tar.gz\n\n\n\n\n\nYou may realize that you've been using \nwget\n to download files from a web proxy for many of the previous exercises at the school!\n\n\nRun a New Test Job\n\u00b6\n\n\nNow, you'll repeat the last exercise (with a single input query file) but have HTCondor download the \npdbaa_files.tar.gz\n file from the web proxy, instead of having the file transferred from the submit server.\n\n\nModify the submit file and wrapper script\n\u00b6\n\n\nIn the wrapper script, we have to add some special lines so that we can pull from Stash and use the HTTP proxy. In \nblast_wrapper.sh\n, we will have to add commands to pull the data file:\n\n\n#!/bin/bash\n\n\n\n# Set the http_proxy environment which wget uses\n\n\nexport\n \nhttp_proxy\n=\n$OSG_SQUID_LOCATION\n\n\n\n\n# Copy the pdbaa_files.tar.gz to the worker node\n\n\n# Add the -S argument, so we can see if it was a cache HIT or MISS\n\n\nwget -S http://stash.osgconnect.net/~username/pdbaa_files.tar.gz\n\n\ntar xvzf pdbaa_files.tar.gz\n\n./blastx -db pdbaa -query mouse.fa -out mouse.fa.result\n\nrm pdbaa.*\n\n\n\n\n\nThe new line will download the \npdbaa_files.tar.gz\n from stash, using the closest cache (because \nwget\n will look at the environment variable \nhttp_proxy\n for the newest cache).\n\n\nIn your submit file, you will need to remove the \npdbaa_files.tar.gz\n file from the \ntransfer_input_files\n, because we are using HTTP proxies!\n\n\nSubmit the test job\n\u00b6\n\n\nYou may wish to first remove the log, result, output, and error files from the previous tests, which will be overwritten when the new test job completes.\n\n\nuser@training $\n rm *.error *.out *.result *.log\n\n\n\n\n\nSubmit the test job!\n\n\nWhen the job starts, the wrapper will download the \npdbaa_files.tar.gz\n file from the web proxy. If the jobs takes longer than two minutes, you can assume that it will complete successfully, and then continue with the rest of the exercise.\n\n\nAfter the job completes examine the *.error file generated by the submission. At the top of the file, you will find something like:\n\n\n--2019-07-11 \n18\n:29:14--  http://stash.osgconnect.net/~dweitzel/pdbaa_files.tar.gz\nResolving uct2-squid.mwt2.org \n(\nuct2-squid.mwt2.org\n)\n... \n192\n.170.227.165\nConnecting to uct2-squid.mwt2.org \n(\nuct2-squid.mwt2.org\n)\n|\n192\n.170.227.165\n|\n:3128... connected.\nProxy request sent, awaiting response... \n  HTTP/1.0 \n200\n OK\n  Server: nginx/1.10.2\n  Date: Thu, \n11\n Jul \n2019\n \n18\n:29:12 GMT\n  Content-Type: application/octet-stream\n  Content-Length: \n22105124\n\n  Last-Modified: Mon, \n09\n Jul \n2018\n \n23\n:22:11 GMT\n  ETag: \n\"5b43ee23-1514c24\"\n\n  Accept-Ranges: bytes\n  Age: \n2\n\n  X-Cache: HIT from uct2-squid.mwt2.org\n  Via: \n1\n.1 uct2-squid.mwt2.org:3128 \n(\nsquid/frontier-squid-2.7.STABLE9-27.1.osg33.el6\n)\n\n  Connection: keep-alive\n  Proxy-Connection: keep-alive\nLength: \n22105124\n \n(\n21M\n)\n \n[\napplication/octet-stream\n]\n\n...\n\n\n\n\n\nNotice the \nX-Cache\n line. It says it was a cache HIT from the proxy \ncmsprxy01.colorado.edu\n. Yay! You successfully used a proxy to cache data near your worker node! Notice, the name of the cache may be different.\n\n\nRun all 100 Jobs!\n\u00b6\n\n\nIf all of the previous tests have gone okay, you can prepare to run all 100 jobs that will use the split input files. To make sure you're not going to generate too much data, use the size of files from the previous test to calculate how much total data you're going to add to the \nthur-blast-data\n directory for 100 jobs.\n\n\nMake sure you remove \npdbaa_files.tar.gz\n from the \ntransfer_input_files\n in the \nsplit\n submit file.\n\n\nSubmit all 100 jobs! They may take a while to all complete, but it will still be faster than the many hours it would have taken to blast the single, large \nmouse_rna.fa\n file without splitting it up.  In the meantime, as long as the first several jobs are running for longer than two minutes, you can move on to the next \nexercise",
            "title": "Exercise 2.1"
        },
        {
            "location": "/materials/day4/part2-ex1-blast-proxy/#thursday-exercise-21-using-a-web-proxy-for-large-shared-input",
            "text": "Continuing the series of exercises blasting mouse genetic sequences, the objective of this exercise is to use a web proxy to stage the large database, which will be downloaded into each of many jobs that use the split input files from the last exercise (Exercise 2.3).",
            "title": "Thursday Exercise 2.1: Using a Web Proxy for Large Shared Input"
        },
        {
            "location": "/materials/day4/part2-ex1-blast-proxy/#setup",
            "text": "Make sure you are logged into  training.osgconnect.net  Make sure you are in the same directory as the previous exercise,  Exercise 2.3  directory named  thur-blast-data .",
            "title": "Setup"
        },
        {
            "location": "/materials/day4/part2-ex1-blast-proxy/#place-the-large-file-on-the-proxy",
            "text": "First, you'll need to put the  pdbaa_files.tar.gz  file onto the Stash web directory. Use the following command:  user@training $  cp pdbaa_files.tar.gz ~/stash/public/",
            "title": "Place the Large File on the Proxy"
        },
        {
            "location": "/materials/day4/part2-ex1-blast-proxy/#test-a-download-of-the-file",
            "text": "Once the file is placed in the ~/stash/public directory, it can be downloaded from a corresponding URL such as  http://stash.osgconnect.net/~ username /pdbaa_files.tar.gz , where  username  is your username on  training.osgconnect.net .  Using the above convention (and from a different directory on  training.osgconnect.net , any directory), you can test the download of your  pdbaa_files.tar.gz  file with a command like the following:  user@training $  wget http://stash.osgconnect.net/~ username /pdbaa_files.tar.gz  You may realize that you've been using  wget  to download files from a web proxy for many of the previous exercises at the school!",
            "title": "Test a download of the file"
        },
        {
            "location": "/materials/day4/part2-ex1-blast-proxy/#run-a-new-test-job",
            "text": "Now, you'll repeat the last exercise (with a single input query file) but have HTCondor download the  pdbaa_files.tar.gz  file from the web proxy, instead of having the file transferred from the submit server.",
            "title": "Run a New Test Job"
        },
        {
            "location": "/materials/day4/part2-ex1-blast-proxy/#modify-the-submit-file-and-wrapper-script",
            "text": "In the wrapper script, we have to add some special lines so that we can pull from Stash and use the HTTP proxy. In  blast_wrapper.sh , we will have to add commands to pull the data file:  #!/bin/bash  # Set the http_proxy environment which wget uses  export   http_proxy = $OSG_SQUID_LOCATION   # Copy the pdbaa_files.tar.gz to the worker node  # Add the -S argument, so we can see if it was a cache HIT or MISS  wget -S http://stash.osgconnect.net/~username/pdbaa_files.tar.gz \ntar xvzf pdbaa_files.tar.gz\n\n./blastx -db pdbaa -query mouse.fa -out mouse.fa.result\n\nrm pdbaa.*  The new line will download the  pdbaa_files.tar.gz  from stash, using the closest cache (because  wget  will look at the environment variable  http_proxy  for the newest cache).  In your submit file, you will need to remove the  pdbaa_files.tar.gz  file from the  transfer_input_files , because we are using HTTP proxies!",
            "title": "Modify the submit file and wrapper script"
        },
        {
            "location": "/materials/day4/part2-ex1-blast-proxy/#submit-the-test-job",
            "text": "You may wish to first remove the log, result, output, and error files from the previous tests, which will be overwritten when the new test job completes.  user@training $  rm *.error *.out *.result *.log  Submit the test job!  When the job starts, the wrapper will download the  pdbaa_files.tar.gz  file from the web proxy. If the jobs takes longer than two minutes, you can assume that it will complete successfully, and then continue with the rest of the exercise.  After the job completes examine the *.error file generated by the submission. At the top of the file, you will find something like:  --2019-07-11  18 :29:14--  http://stash.osgconnect.net/~dweitzel/pdbaa_files.tar.gz\nResolving uct2-squid.mwt2.org  ( uct2-squid.mwt2.org ) ...  192 .170.227.165\nConnecting to uct2-squid.mwt2.org  ( uct2-squid.mwt2.org ) | 192 .170.227.165 | :3128... connected.\nProxy request sent, awaiting response... \n  HTTP/1.0  200  OK\n  Server: nginx/1.10.2\n  Date: Thu,  11  Jul  2019   18 :29:12 GMT\n  Content-Type: application/octet-stream\n  Content-Length:  22105124 \n  Last-Modified: Mon,  09  Jul  2018   23 :22:11 GMT\n  ETag:  \"5b43ee23-1514c24\" \n  Accept-Ranges: bytes\n  Age:  2 \n  X-Cache: HIT from uct2-squid.mwt2.org\n  Via:  1 .1 uct2-squid.mwt2.org:3128  ( squid/frontier-squid-2.7.STABLE9-27.1.osg33.el6 ) \n  Connection: keep-alive\n  Proxy-Connection: keep-alive\nLength:  22105124   ( 21M )   [ application/octet-stream ] \n...  Notice the  X-Cache  line. It says it was a cache HIT from the proxy  cmsprxy01.colorado.edu . Yay! You successfully used a proxy to cache data near your worker node! Notice, the name of the cache may be different.",
            "title": "Submit the test job"
        },
        {
            "location": "/materials/day4/part2-ex1-blast-proxy/#run-all-100-jobs",
            "text": "If all of the previous tests have gone okay, you can prepare to run all 100 jobs that will use the split input files. To make sure you're not going to generate too much data, use the size of files from the previous test to calculate how much total data you're going to add to the  thur-blast-data  directory for 100 jobs.  Make sure you remove  pdbaa_files.tar.gz  from the  transfer_input_files  in the  split  submit file.  Submit all 100 jobs! They may take a while to all complete, but it will still be faster than the many hours it would have taken to blast the single, large  mouse_rna.fa  file without splitting it up.  In the meantime, as long as the first several jobs are running for longer than two minutes, you can move on to the next  exercise",
            "title": "Run all 100 Jobs!"
        },
        {
            "location": "/materials/day4/part2-ex2-stashcache-shared/",
            "text": "Thursday Exercise 2.2: Using StashCache for Large Shared Data\n\u00b6\n\n\nThis exercise will use a \nBLAST\n workflow to demonstrate the functionality of StashCache for transferring input files to jobs on OSG.\n\n\nBecause our individual blast jobs from \nExercise 2.1\n would take a bit longer with a larger database (too long for an workable exercise), we'll imagine for this exercise that our \npdbaa_files.tar.gz\n file is too large for a web proxy (larger than ~1 GB). For this exercise, we will use the input from Exercise 2.1, but instead of using the web proxy for the \npdbaa\n database, we will place it in StashCache via the OSG Connect server.\n\n\nStashCache is a distributed set of caches spread across the U.S. They are connected with high bandwidth connections to each other, and to the data origin servers, where your data is originally placed.\n\n\n\n\nWe will be using the command \nstashcp\n to copy files from Stash to the execute hosts.  It has a \ncp\n like syntax.\n\n\nSetup\n\u00b6\n\n\n\n\nMake sure you're logged in to \ntraining.osgconnect.net\n\n\nTransfer the following files from \nExercise 2.1\n to a new directory called \n~/stash/thur-data-stash\n: \nblast_wrapper.sh\n, \nblastx\n, \nmouse_rna.fa.1\n, \nmouse_rna.fa.2\n, \nmouse_rna.fa.3\n, and the most recent submit file.\n\n\n\n\nPlace the Database in StashCache\n\u00b6\n\n\nCopy to your \npublic\n space on OSG Connect\n\u00b6\n\n\nStashCache provides a public space for you to store data which can be accessed through the caching servers. First, you need to move your blast database into this public directory. If you remember the \"public\" directory in your home directory on the OSG Connect server \ntraining.osgconnect.net\n, it's this location where you will place files that need to end up in the StashCache data origin.\n\n\nYou have already placed files in the \n~/stash/public\n directory in the previous exercise in order for it to be accessible to the HTTP proxies. The same directory is accessible to StashCache.\n\n\nAs the \npublic\n directory name indicates, your files placed in the \npublic\n directory will be accessible to anyone's jobs if they know how to use \nstashcp\n, though no one else will be able to edit the files, since only you can \nplace\n or \nchange\n files in your \npublic\n space. For your own work in the future, make sure that you never put any sensitive data in such locations.\n\n\nCheck the file on OSG Connect\n\u00b6\n\n\nNext, you can check for the file and test the command that we'll use in jobs on the OSG Connect login node:\n\n\nuser@training $\n ls ~/stash/public\n\n\n\n\n\nNow, load the \nstashcache\n module, which will allow you to test a copy of the file from StashCache into your home directory on \nlogin.osgconnect.net\n:\n\n\nuser@training $\n module load stashcache\n\nuser@training $\n stashcp /user/\nusername\n/public/pdbaa_files.tar.gz ./\n\n\n\n\n\nYou should now see the \npdbaa_files.tar.gz\n file in your current directory. Notice that we had to include the \n/user\n and \nusername\n in the file path for \nstashcp\n, which make sure you're copying from \nyour\n \npublic\n space.\n\n\nModify the Submit File and Wrapper\n\u00b6\n\n\nYou will have to modify the wrapper and submit files to use StashCache:\n\n\n\n\n\n\nAt the top of the wrapper script (after \n#!/bin/bash\n), add the lines that load the \nstashcache\n module and to copy the \npdbaa_files.tar.gz\n file into the current directory of the job:\n\n\nmodule load stashcache\nstashcp /user/\nusername\n/public/pdbaa_files.tar.gz ./\n\n\n\n\n\n\n\n\n\nSince HTCondor will no longer transfer or download the file for you, make sure to add the following line (or modify your existing \nrm\n command, if you're confident) to make sure the \npdbaa_files.tar.gz\n file is also deleted and not copied back as perceived output.\n\n\nrm pdbaa_files.tar.gz\n\n\n\n\n\n\n\n\n\nDelete the \nwget\n line from the job wrapper script \nblast_wrapper.sh\n\n\n\n\n\n\nAdd the following line to the submit file and update the \"requirements\" statement to require servers with OSG Connect modules (for accessing the \nstashcp\n module), somewhere before the word \nqueue\n.\n\n\n+WantsStashCache = true\nrequirements = (OpSys == \"LINUX\") && (HAS_MODULES =?= true)\n\n\n\n\n\n\n\n\n\nConfirm that your queue statement is correct for the current directory. It should be something like:\n\n\nqueue inputfile matching mouse_rna.fa.*\n\n\n\n\n\n\n\n\n\nAnd that mouse_rna.fa.* files exist in the current directory (you should have copied them from the previous exercise directory).\n\n\nSubmit the Job\n\u00b6\n\n\nNow submit and monitor the job! If your 100 jobs from the previous exercise haven't started running yet, this job will not yet start. However, after it has been running for ~2 minutes, you're safe to continue to the next exercise!\n\n\nNote: Keeping StashCache 'Clean'\n\u00b6\n\n\nJust as for data on a web proxy, it is VERY important to remove old files from StashCache when you no longer need them, especially so that you'll have plenty of space for such files in the future. For example, you would delete (\nrm\n) files from \npublic\n on \ntraining.osgconnect.net\n when you don't need them there anymore, but only after all jobs have finished. The next time you use StashCache after the school, remember to first check for old files that you can delete.",
            "title": "Exercise 2.2"
        },
        {
            "location": "/materials/day4/part2-ex2-stashcache-shared/#thursday-exercise-22-using-stashcache-for-large-shared-data",
            "text": "This exercise will use a  BLAST  workflow to demonstrate the functionality of StashCache for transferring input files to jobs on OSG.  Because our individual blast jobs from  Exercise 2.1  would take a bit longer with a larger database (too long for an workable exercise), we'll imagine for this exercise that our  pdbaa_files.tar.gz  file is too large for a web proxy (larger than ~1 GB). For this exercise, we will use the input from Exercise 2.1, but instead of using the web proxy for the  pdbaa  database, we will place it in StashCache via the OSG Connect server.  StashCache is a distributed set of caches spread across the U.S. They are connected with high bandwidth connections to each other, and to the data origin servers, where your data is originally placed.   We will be using the command  stashcp  to copy files from Stash to the execute hosts.  It has a  cp  like syntax.",
            "title": "Thursday Exercise 2.2: Using StashCache for Large Shared Data"
        },
        {
            "location": "/materials/day4/part2-ex2-stashcache-shared/#setup",
            "text": "Make sure you're logged in to  training.osgconnect.net  Transfer the following files from  Exercise 2.1  to a new directory called  ~/stash/thur-data-stash :  blast_wrapper.sh ,  blastx ,  mouse_rna.fa.1 ,  mouse_rna.fa.2 ,  mouse_rna.fa.3 , and the most recent submit file.",
            "title": "Setup"
        },
        {
            "location": "/materials/day4/part2-ex2-stashcache-shared/#place-the-database-in-stashcache",
            "text": "",
            "title": "Place the Database in StashCache"
        },
        {
            "location": "/materials/day4/part2-ex2-stashcache-shared/#copy-to-your-public-space-on-osg-connect",
            "text": "StashCache provides a public space for you to store data which can be accessed through the caching servers. First, you need to move your blast database into this public directory. If you remember the \"public\" directory in your home directory on the OSG Connect server  training.osgconnect.net , it's this location where you will place files that need to end up in the StashCache data origin.  You have already placed files in the  ~/stash/public  directory in the previous exercise in order for it to be accessible to the HTTP proxies. The same directory is accessible to StashCache.  As the  public  directory name indicates, your files placed in the  public  directory will be accessible to anyone's jobs if they know how to use  stashcp , though no one else will be able to edit the files, since only you can  place  or  change  files in your  public  space. For your own work in the future, make sure that you never put any sensitive data in such locations.",
            "title": "Copy to your public space on OSG Connect"
        },
        {
            "location": "/materials/day4/part2-ex2-stashcache-shared/#check-the-file-on-osg-connect",
            "text": "Next, you can check for the file and test the command that we'll use in jobs on the OSG Connect login node:  user@training $  ls ~/stash/public  Now, load the  stashcache  module, which will allow you to test a copy of the file from StashCache into your home directory on  login.osgconnect.net :  user@training $  module load stashcache user@training $  stashcp /user/ username /public/pdbaa_files.tar.gz ./  You should now see the  pdbaa_files.tar.gz  file in your current directory. Notice that we had to include the  /user  and  username  in the file path for  stashcp , which make sure you're copying from  your   public  space.",
            "title": "Check the file on OSG Connect"
        },
        {
            "location": "/materials/day4/part2-ex2-stashcache-shared/#modify-the-submit-file-and-wrapper",
            "text": "You will have to modify the wrapper and submit files to use StashCache:    At the top of the wrapper script (after  #!/bin/bash ), add the lines that load the  stashcache  module and to copy the  pdbaa_files.tar.gz  file into the current directory of the job:  module load stashcache\nstashcp /user/ username /public/pdbaa_files.tar.gz ./    Since HTCondor will no longer transfer or download the file for you, make sure to add the following line (or modify your existing  rm  command, if you're confident) to make sure the  pdbaa_files.tar.gz  file is also deleted and not copied back as perceived output.  rm pdbaa_files.tar.gz    Delete the  wget  line from the job wrapper script  blast_wrapper.sh    Add the following line to the submit file and update the \"requirements\" statement to require servers with OSG Connect modules (for accessing the  stashcp  module), somewhere before the word  queue .  +WantsStashCache = true\nrequirements = (OpSys == \"LINUX\") && (HAS_MODULES =?= true)    Confirm that your queue statement is correct for the current directory. It should be something like:  queue inputfile matching mouse_rna.fa.*    And that mouse_rna.fa.* files exist in the current directory (you should have copied them from the previous exercise directory).",
            "title": "Modify the Submit File and Wrapper"
        },
        {
            "location": "/materials/day4/part2-ex2-stashcache-shared/#submit-the-job",
            "text": "Now submit and monitor the job! If your 100 jobs from the previous exercise haven't started running yet, this job will not yet start. However, after it has been running for ~2 minutes, you're safe to continue to the next exercise!",
            "title": "Submit the Job"
        },
        {
            "location": "/materials/day4/part2-ex2-stashcache-shared/#note-keeping-stashcache-clean",
            "text": "Just as for data on a web proxy, it is VERY important to remove old files from StashCache when you no longer need them, especially so that you'll have plenty of space for such files in the future. For example, you would delete ( rm ) files from  public  on  training.osgconnect.net  when you don't need them there anymore, but only after all jobs have finished. The next time you use StashCache after the school, remember to first check for old files that you can delete.",
            "title": "Note: Keeping StashCache 'Clean'"
        },
        {
            "location": "/materials/day4/part2-ex3-stashcache-unique/",
            "text": "Thursday Exercise 2.3: Using Stash for unique large input\n\u00b6\n\n\nIn this exercise, we will run a multimedia program that converts and manipulates video files. In particular, we want to convert large \n.mov\n files to smaller (10-100s of MB) \nmp4\n files.  Just like the Blast database in the \nprevious exercise\n, these video files are too large to send to jobs using HTCondor's default file transfer mechanism, so we'll be using the Stash tool to send our data to jobs. This exercise should take 25-30 minutes.\n\n\nData\n\u00b6\n\n\nWe'll start by moving our source movie files into Stash, so that they'll be available to our jobs when they run out on OSG.\n\n\n\n\nLog into \ntraining.osgconnect.net\n and move into the \n~/stash/public\n directory.\n\n\n\n\nThe video files are currently stored on the squid proxy you used earlier. To place them in Stash, download them using \nwget\n: \n\n\nuser@training $\n wget http://proxy.chtc.wisc.edu/osgschool19/videos.tar.gz\n\n\n\n\n\n\n\n\n\nOnce downloaded, untar the \ntar.gz\n file. It should contain three \n.mov\n files. (this may take a while since everyone else is likely doing the same thing)\n\n\n\n\nHow big are the three files? Which is the smallest? (Find out with \nls -lh\n.)\n\n\n\n\nWe're going to need a list of these files later. For now, let's save that list to a file in this directory by running \nls\n and redirecting the output to a file: \n\n\nuser@training $\n ls *.MOV *.mov > movie_list.txt\n\n\n\n\n\n\n\n\n\nOnce you've examined the three \nmov\n files and created the list of files, remove the original \nvideos.tar.gz\n file.\n\n\n\n\n\n\nSoftware\n\u00b6\n\n\nWe'll be using a multi-purpose media tool called \nffmpeg\n  to convert video formats. The basic command to convert a file looks like this: \n\n\nuser@training $\n ./ffmpeg -i input.mov output.mp4\n\n\n\n\n\nIn order to resize our files, we're going to manually set the video bitrate and resize the frames, so that the resulting file is smaller.\n\n\nuser@training $\n ./ffmpeg -i input.mp4 -b:v 400k -s 640x360 output.mp4\n\n\n\n\n\nTo get the \nffmpeg\n program do the following:\n\n\n\n\nOn training.osgconnect.net\n, create a directory for this exercise \n~/thur-data-ffmpeg\n and move into it.\n\n\n\n\nWe'll be downloading the \nffmpeg\n pre-built static binary originally from this page: \nhttp://johnvansickle.com/ffmpeg/\n. \n\n\nuser@training $\n wget http://proxy.chtc.wisc.edu/osgschool19/ffmpeg-release-64bit-static.tar.xz\n\n\n\n\n\n\n\n\n\nOnce the binary is downloaded, un-tar it, and then copy the main \nffmpeg\n program into your current directory: \n\n\nuser@training $\n tar -xf ffmpeg-release-64bit-static.tar.xz\n\nuser@training $\n cp ffmpeg-4.0.1-64bit-static/ffmpeg ./\n\n\n\n\n\n\n\n\n\nScript\n\u00b6\n\n\nWe want to write a script that uses \nffmpeg\n to convert a \n.mov\n file to a smaller format. Our script will need to \ncopy\n that movie file from Stash to the job's current working directory (as in the \nprevious exercise\n, \nrun\n the appropriate \nffmpeg\n command,  and then \nremove\n the original movie file so that it doesn't get transferred back to the submit server. This last step is  particularly important, as otherwise you will have large files transferring into the submit server and filling up your home directory space.\n\n\nCreate a file called \nrun_ffmpeg.sh\n, that does the steps described above. Use the name of the smallest \n.mov\n file in the \nffmpeg\n command. Once you've written your script, check it against the example below: \n\n\n#!/bin/bash\n\n\nmodule load stashcache\nstashcp /user/\nusername\n/public/test_open_terminal.mov ./\n./ffmpeg -i test_open_terminal.mov -b:v 400k -s 640x360 test_open_terminal.mp4\nrm test_open_terminal.mov\n\n\n\n\n\nIn your script, the username should be replaced by your \ntraining.osgconnect.net\n username.\n\n\nUltimately we'll want to submit several jobs (one for each \n.mov\n file), but to start with, we'll run one job to  make sure that everything works.\n\n\nSubmit File\n\u00b6\n\n\nCreate a submit file for this job, based on other submit files from the school (\nThis file, for example\n.) Things to consider:\n\n\n\n\n\n\nWe'll be copying the video file into the job's working directory, so make sure to request enough disk space for the input \nmov\n file and the output \nmp4\n file.  If you're aren't sure how much to request, ask a helper in the room.\n\n\n\n\n\n\nImportant\n Don't list the name of the \n.mov\n in \ntransfer_input_files\n. Our job will be interacting with the input \n.mov\n files solely from within the script we wrote above.\n\n\n\n\n\n\nNote that we \ndo\n need to transfer the \nffmpeg\n program that we downloaded above. \n\n\ntransfer_input_files = ffmpeg\n\n\n\n\n\n\n\n\n\nAdd the same requirements as the previous exercise: \n\n\n+WantsStashCache = true\nrequirements = (OpSys == \"LINUX\") && (HAS_MODULES =?= true)\n\n\n\n\n\n\n\n\n\nInitial Job\n\u00b6\n\n\nWith everything in place, submit the job. Once it finishes, we should check to make sure everything ran as expected:\n\n\n\n\nCheck the directory where you submitted the job. Did the output \n.mp4\n file return?\n\n\nAlso in the directory where you submitted the job - did the original \n.mov\n file return here accidentally?\n\n\nCheck file sizes. How big is the returned \n.mp4\n file? How does that compare to the original \n.mov\n input?\n\n\n\n\nIf your job successfully returned the converted \n.mp4\n file and did \nnot\n transfer the \n.mov\n file to the submit server, and the \n.mp4\n file was appropriately scaled down, then we can go ahead and convert all of the files we uploaded to Stash.\n\n\nMultiple jobs\n\u00b6\n\n\nWe wrote the name of the \n.mov\n file into our \nrun_ffmpeg.sh\n executable script. To submit a set of jobs for all of our \n.mov\n  files, what will we need to change in:\n\n\n\n\nthe script? \n\n\nthe submit file?\n\n\n\n\nOnce you've thought about it, check your reasoning against the instructions below.\n\n\nAdd an argument to your script\n\u00b6\n\n\n\n\nLook at your \nrun_ffmpeg.sh\n script. What values will change for every job?\n\n\nThe input file will change with every job - and don't forget that the output file will too! Let's make them both into arguments. \n\n\n\n\nTo add arguments to a bash script, we use the notation \n$1\n for the first argument (our input file) and \n$2\n for the second argument (our output file name).  The final script should look like this: \n\n\n#!/bin/bash\n\nmodule load stashcache\nstashcp /user/\nusername\n/public/$1 ./\n./ffmpeg -i $1 -b:v 400k -s 640x360 $2\nrm $1\n\n\n\n\n\nNote that we use the input file name multiple times in our script, so we'll have to use \n$1\n multiple times as well.\n\n\nModify your submit file\n\u00b6\n\n\n\n\n\n\nWe now need to tell each job what arguments to use. We will do this by adding an arguments line to our submit file. Because we'll only have the input file name, the \"output\" file name will be the input file name with the \nmp4\n extension. That should look like this: \n\n\narguments = $(mov) $(mov).mp4\n\n\n\n\n\n\n\n\n\nTo set these arguments, we will use the \nqueue .. matching\n syntax that we learned on \nMonday\n. To do so, we need to create a list of our input files. \n\n\n\n\n\n\nIn our submit file, we can then change our queue statement to: \n\n\nqueue mov from movie_list.txt\n\n\n\n\n\n\n\n\n\nCopy the movie_list.txt to your submit directory.\n\n\n\n\n\n\nOnce you've made these changes, try submitting all the jobs!\n\n\nBonus\n\u00b6\n\n\nIf you wanted to set a different output file name, bitrate and/or size for each original movie, how could you modify:\n\n\n\n\nmovie_list.txt\n \n\n\nYour submit file \n\n\nrun_ffmpeg.sh\n\n\n\n\nto do so?\n\n\n\n  \nShow hint\n Here's the changes you can make to the various files:\n\n\n\n\n\n\nmovie_list.txt\n \n\n\nducks.MOV ducks.mp4 500k 1280x720\nteaching.MOV teaching.mp4 400k 320x180\ntest_open_terminal.mov terminal.mp4 600k 640x360\n\n\n\n\n\n\n\n\n\nSubmit file\n\n\narguments = $(mov) $(mp4) $(bitrate) $(size)\n\nqueue mov,mp4,bitrate,size from movie_list.txt\n\n\n\n\n\n\n\n\n\nrun_ffmpeg.sh\n \n\n\n1\n2\n3\n4\n5\n6\n#!/bin/bash\n\n\nmodule load stashcache\nstashcp /user/\nusername\n/public/\n$1\n ./\n./ffmpeg -i \n$1\n -b:v \n$3\n -s \n$4\n \n$2\n\nrm \n$1",
            "title": "Exercise 2.3"
        },
        {
            "location": "/materials/day4/part2-ex3-stashcache-unique/#thursday-exercise-23-using-stash-for-unique-large-input",
            "text": "In this exercise, we will run a multimedia program that converts and manipulates video files. In particular, we want to convert large  .mov  files to smaller (10-100s of MB)  mp4  files.  Just like the Blast database in the  previous exercise , these video files are too large to send to jobs using HTCondor's default file transfer mechanism, so we'll be using the Stash tool to send our data to jobs. This exercise should take 25-30 minutes.",
            "title": "Thursday Exercise 2.3: Using Stash for unique large input"
        },
        {
            "location": "/materials/day4/part2-ex3-stashcache-unique/#data",
            "text": "We'll start by moving our source movie files into Stash, so that they'll be available to our jobs when they run out on OSG.   Log into  training.osgconnect.net  and move into the  ~/stash/public  directory.   The video files are currently stored on the squid proxy you used earlier. To place them in Stash, download them using  wget :   user@training $  wget http://proxy.chtc.wisc.edu/osgschool19/videos.tar.gz    Once downloaded, untar the  tar.gz  file. It should contain three  .mov  files. (this may take a while since everyone else is likely doing the same thing)   How big are the three files? Which is the smallest? (Find out with  ls -lh .)   We're going to need a list of these files later. For now, let's save that list to a file in this directory by running  ls  and redirecting the output to a file:   user@training $  ls *.MOV *.mov > movie_list.txt    Once you've examined the three  mov  files and created the list of files, remove the original  videos.tar.gz  file.",
            "title": "Data"
        },
        {
            "location": "/materials/day4/part2-ex3-stashcache-unique/#software",
            "text": "We'll be using a multi-purpose media tool called  ffmpeg   to convert video formats. The basic command to convert a file looks like this:   user@training $  ./ffmpeg -i input.mov output.mp4  In order to resize our files, we're going to manually set the video bitrate and resize the frames, so that the resulting file is smaller.  user@training $  ./ffmpeg -i input.mp4 -b:v 400k -s 640x360 output.mp4  To get the  ffmpeg  program do the following:   On training.osgconnect.net , create a directory for this exercise  ~/thur-data-ffmpeg  and move into it.   We'll be downloading the  ffmpeg  pre-built static binary originally from this page:  http://johnvansickle.com/ffmpeg/ .   user@training $  wget http://proxy.chtc.wisc.edu/osgschool19/ffmpeg-release-64bit-static.tar.xz    Once the binary is downloaded, un-tar it, and then copy the main  ffmpeg  program into your current directory:   user@training $  tar -xf ffmpeg-release-64bit-static.tar.xz user@training $  cp ffmpeg-4.0.1-64bit-static/ffmpeg ./",
            "title": "Software"
        },
        {
            "location": "/materials/day4/part2-ex3-stashcache-unique/#script",
            "text": "We want to write a script that uses  ffmpeg  to convert a  .mov  file to a smaller format. Our script will need to  copy  that movie file from Stash to the job's current working directory (as in the  previous exercise ,  run  the appropriate  ffmpeg  command,  and then  remove  the original movie file so that it doesn't get transferred back to the submit server. This last step is  particularly important, as otherwise you will have large files transferring into the submit server and filling up your home directory space.  Create a file called  run_ffmpeg.sh , that does the steps described above. Use the name of the smallest  .mov  file in the  ffmpeg  command. Once you've written your script, check it against the example below:   #!/bin/bash \n\nmodule load stashcache\nstashcp /user/ username /public/test_open_terminal.mov ./\n./ffmpeg -i test_open_terminal.mov -b:v 400k -s 640x360 test_open_terminal.mp4\nrm test_open_terminal.mov  In your script, the username should be replaced by your  training.osgconnect.net  username.  Ultimately we'll want to submit several jobs (one for each  .mov  file), but to start with, we'll run one job to  make sure that everything works.",
            "title": "Script"
        },
        {
            "location": "/materials/day4/part2-ex3-stashcache-unique/#submit-file",
            "text": "Create a submit file for this job, based on other submit files from the school ( This file, for example .) Things to consider:    We'll be copying the video file into the job's working directory, so make sure to request enough disk space for the input  mov  file and the output  mp4  file.  If you're aren't sure how much to request, ask a helper in the room.    Important  Don't list the name of the  .mov  in  transfer_input_files . Our job will be interacting with the input  .mov  files solely from within the script we wrote above.    Note that we  do  need to transfer the  ffmpeg  program that we downloaded above.   transfer_input_files = ffmpeg    Add the same requirements as the previous exercise:   +WantsStashCache = true\nrequirements = (OpSys == \"LINUX\") && (HAS_MODULES =?= true)",
            "title": "Submit File"
        },
        {
            "location": "/materials/day4/part2-ex3-stashcache-unique/#initial-job",
            "text": "With everything in place, submit the job. Once it finishes, we should check to make sure everything ran as expected:   Check the directory where you submitted the job. Did the output  .mp4  file return?  Also in the directory where you submitted the job - did the original  .mov  file return here accidentally?  Check file sizes. How big is the returned  .mp4  file? How does that compare to the original  .mov  input?   If your job successfully returned the converted  .mp4  file and did  not  transfer the  .mov  file to the submit server, and the  .mp4  file was appropriately scaled down, then we can go ahead and convert all of the files we uploaded to Stash.",
            "title": "Initial Job"
        },
        {
            "location": "/materials/day4/part2-ex3-stashcache-unique/#multiple-jobs",
            "text": "We wrote the name of the  .mov  file into our  run_ffmpeg.sh  executable script. To submit a set of jobs for all of our  .mov   files, what will we need to change in:   the script?   the submit file?   Once you've thought about it, check your reasoning against the instructions below.",
            "title": "Multiple jobs"
        },
        {
            "location": "/materials/day4/part2-ex3-stashcache-unique/#add-an-argument-to-your-script",
            "text": "Look at your  run_ffmpeg.sh  script. What values will change for every job?  The input file will change with every job - and don't forget that the output file will too! Let's make them both into arguments.    To add arguments to a bash script, we use the notation  $1  for the first argument (our input file) and  $2  for the second argument (our output file name).  The final script should look like this:   #!/bin/bash\n\nmodule load stashcache\nstashcp /user/ username /public/$1 ./\n./ffmpeg -i $1 -b:v 400k -s 640x360 $2\nrm $1  Note that we use the input file name multiple times in our script, so we'll have to use  $1  multiple times as well.",
            "title": "Add an argument to your script"
        },
        {
            "location": "/materials/day4/part2-ex3-stashcache-unique/#modify-your-submit-file",
            "text": "We now need to tell each job what arguments to use. We will do this by adding an arguments line to our submit file. Because we'll only have the input file name, the \"output\" file name will be the input file name with the  mp4  extension. That should look like this:   arguments = $(mov) $(mov).mp4    To set these arguments, we will use the  queue .. matching  syntax that we learned on  Monday . To do so, we need to create a list of our input files.     In our submit file, we can then change our queue statement to:   queue mov from movie_list.txt    Copy the movie_list.txt to your submit directory.    Once you've made these changes, try submitting all the jobs!",
            "title": "Modify your submit file"
        },
        {
            "location": "/materials/day4/part2-ex3-stashcache-unique/#bonus",
            "text": "If you wanted to set a different output file name, bitrate and/or size for each original movie, how could you modify:   movie_list.txt    Your submit file   run_ffmpeg.sh   to do so?  \n   Show hint  Here's the changes you can make to the various files:    movie_list.txt    ducks.MOV ducks.mp4 500k 1280x720\nteaching.MOV teaching.mp4 400k 320x180\ntest_open_terminal.mov terminal.mp4 600k 640x360    Submit file  arguments = $(mov) $(mp4) $(bitrate) $(size)\n\nqueue mov,mp4,bitrate,size from movie_list.txt    run_ffmpeg.sh    1\n2\n3\n4\n5\n6 #!/bin/bash \n\nmodule load stashcache\nstashcp /user/ username /public/ $1  ./\n./ffmpeg -i  $1  -b:v  $3  -s  $4   $2 \nrm  $1",
            "title": "Bonus"
        },
        {
            "location": "/materials/day4/part3-ex1-input/",
            "text": "Thursday Exercise 3.1: Large Input Data\n\u00b6\n\n\nIn this exercise, we will do a similar version of the \nprevious exercise\n.\nThis exercise should take 10-15 minutes.\n\n\nBackground\n\u00b6\n\n\nIn the previous exercises, we used two \"web-based\" tools to stage and deliver our files to jobs:\n\nthe squid web proxy\n  and \nStash\n.\nAnother alternative for handling large files (both input and output), especially if they are unique to each job, is a\nlocal shared filesystem.\nThis is a filesystem that all (or most) of the execute servers can access, so data stored there can be copied to the job\nfrom that system instead of as a transfer or download.\n\n\nFor this example, we'll be submitting the same jobs as the \nprevious exercise\n,\nbut we will stage our data in a shared filesystem local to CHTC.\nThe name of our shared filesystem is Gluster and user directories are found as sub-directories  of the path \n/mnt/gluster\n.\nThis is just one example of what it can look like to use a shared filesystem.\nIf you are running jobs at your own institution, the shared filesystem and how to access it may be different.\n\n\nAccessing the Filesystem\n\u00b6\n\n\n\n\nRunning on \nlearn.chtc.wisc.edu\n\n\nFor these next 2 exercises, we will be using \nlearn.chtc.wisc.edu\n.\n\n\n\n\nBecause our shared filesystem is \nonly\n available on the local CHTC HTCondor pool, you'll need to log into our local\nsubmit server, \nlearn.chtc.wisc.edu\n.\n\n\nOnce you've logged in, navigate to your Gluster directory.\nIt should be at the location \n/mnt/gluster/<USERNAME>\n, where \n<USERNAME>\n is your username on \nlearn.chtc.wisc.edu\n.\n\n\nPrevious Files\n\u00b6\n\n\nData\n\u00b6\n\n\nLike the previous example, we'll start by downloading our source movie files into the Gluster directory.\nRun this command \nin your Gluster directory\n, \n/mnt/gluster/<USERNAME>\n.\n\n\nuser@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/videos.tar.gz\n\n\n\n\n\nWhile the files are copying, feel free to open a second connection to \nlearn.chtc.wisc.edu\n and follow the instructions below.\nOnce the files have finished downloading, untar them.\n\n\nSoftware, Executable, Submit File\n\u00b6\n\n\nBecause these jobs will be similar to the previous exercise, we can copy the software (\nffmpeg\n), our executable\n(\nrun_ffmpeg.sh\n) and submit file from \nuser-training.osgconnect.net\n to \nlearn.chtc.wisc.edu\n, or, feel free to\nreplicate these by following the instructions in the \nprevious exercise\n.\nThese files should go into a sub-directory of your \nhome\n directory, \nnot your Gluster directory\n.\n\n\nCh-ch-ch-changes\n\u00b6\n\n\nWhat changes will we need to make to our previous job submission in order to submit it in CHTC, using the Gluster\nlocation?\nRead on.\n\n\nScript\n\u00b6\n\n\nThe major actions of our script will be the same: \ncopy\n the movie file to the job's current working directory,\n\nrun\n the appropriate \nffmpeg\n command, and then \nremove\n the original movie file.\nThe main difference is that the \nmov\n file will be copied from  your Gluster directory instead of being downloaded from\nStash.\nLike before, your script should remove  that file before the job completes so that it doesn't get transferred back to\nthe submit server.\n\n\n\n\n\n\nRemove the lines in the \nrun_ffmpeg.sh\n that mention \nmodule load\n.\n\n\n\n\n\n\nRemove the \nstashcp\n line\n\n\n\n\n\n\nChange the first command of your \nrun_ffmpeg.sh\n script to only copy one \n.mov\n file: \n\n\ncp /mnt/gluster/<USERNAME>/test_open_terminal.mov ./\n\n\n\n\n\n\n\n\n\nYou should use your username on \nlearn.chtc.wisc.edu\n in the path above.\nIf you have a version of the script that uses arguments instead of the filenames, that's okay.\n\n\nSubmit File\n\u00b6\n\n\n\n\nRemove any previous requirements and add a line to the file (before the final queue statement) that ensures your job\n    will land on computers that have access to Gluster: \nrequirements = (Target.HasGluster == true)\n\n\n\n\n\n\n\n\n\nInitial Job\n\u00b6\n\n\nAs before, we should test our job submission with a single \nmov\n file before submitting jobs for all three.\nAlter your submit file (if necessary) to  run a job that converts the \ntest_open_terminal.mov\n file.\n\n\nOnce the job finishes, check to make sure everything ran as expected:\n\n\n\n\nCheck the directory where you submitted the job. Did the output \n.mp4\n file return?\n\n\nAlso in the directory where you submitted the job - did the original \n.mov\n file return here accidentally?\n\n\nCheck file sizes. How big is the returned \n.mp4\n file? How does that compare to the original \n.mov\n input?\n\n\n\n\nIf your job successfully returned the converted \n.mp4\n file and \nnot\n the \n.mov\n file to the submit server, and the\n\n.mp4\n file was appropriately scaled down, then our script did what it should have.\n\n\nMultiple jobs\n\u00b6\n\n\nChange your submit file as in the previous exercise in order to submit 3 jobs to convert all three files!",
            "title": "Exercise 3.1"
        },
        {
            "location": "/materials/day4/part3-ex1-input/#thursday-exercise-31-large-input-data",
            "text": "In this exercise, we will do a similar version of the  previous exercise .\nThis exercise should take 10-15 minutes.",
            "title": "Thursday Exercise 3.1: Large Input Data"
        },
        {
            "location": "/materials/day4/part3-ex1-input/#background",
            "text": "In the previous exercises, we used two \"web-based\" tools to stage and deliver our files to jobs: the squid web proxy   and  Stash .\nAnother alternative for handling large files (both input and output), especially if they are unique to each job, is a\nlocal shared filesystem.\nThis is a filesystem that all (or most) of the execute servers can access, so data stored there can be copied to the job\nfrom that system instead of as a transfer or download.  For this example, we'll be submitting the same jobs as the  previous exercise ,\nbut we will stage our data in a shared filesystem local to CHTC.\nThe name of our shared filesystem is Gluster and user directories are found as sub-directories  of the path  /mnt/gluster .\nThis is just one example of what it can look like to use a shared filesystem.\nIf you are running jobs at your own institution, the shared filesystem and how to access it may be different.",
            "title": "Background"
        },
        {
            "location": "/materials/day4/part3-ex1-input/#accessing-the-filesystem",
            "text": "Running on  learn.chtc.wisc.edu  For these next 2 exercises, we will be using  learn.chtc.wisc.edu .   Because our shared filesystem is  only  available on the local CHTC HTCondor pool, you'll need to log into our local\nsubmit server,  learn.chtc.wisc.edu .  Once you've logged in, navigate to your Gluster directory.\nIt should be at the location  /mnt/gluster/<USERNAME> , where  <USERNAME>  is your username on  learn.chtc.wisc.edu .",
            "title": "Accessing the Filesystem"
        },
        {
            "location": "/materials/day4/part3-ex1-input/#previous-files",
            "text": "",
            "title": "Previous Files"
        },
        {
            "location": "/materials/day4/part3-ex1-input/#data",
            "text": "Like the previous example, we'll start by downloading our source movie files into the Gluster directory.\nRun this command  in your Gluster directory ,  /mnt/gluster/<USERNAME> .  user@learn $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/videos.tar.gz  While the files are copying, feel free to open a second connection to  learn.chtc.wisc.edu  and follow the instructions below.\nOnce the files have finished downloading, untar them.",
            "title": "Data"
        },
        {
            "location": "/materials/day4/part3-ex1-input/#software-executable-submit-file",
            "text": "Because these jobs will be similar to the previous exercise, we can copy the software ( ffmpeg ), our executable\n( run_ffmpeg.sh ) and submit file from  user-training.osgconnect.net  to  learn.chtc.wisc.edu , or, feel free to\nreplicate these by following the instructions in the  previous exercise .\nThese files should go into a sub-directory of your  home  directory,  not your Gluster directory .",
            "title": "Software, Executable, Submit File"
        },
        {
            "location": "/materials/day4/part3-ex1-input/#ch-ch-ch-changes",
            "text": "What changes will we need to make to our previous job submission in order to submit it in CHTC, using the Gluster\nlocation?\nRead on.",
            "title": "Ch-ch-ch-changes"
        },
        {
            "location": "/materials/day4/part3-ex1-input/#script",
            "text": "The major actions of our script will be the same:  copy  the movie file to the job's current working directory, run  the appropriate  ffmpeg  command, and then  remove  the original movie file.\nThe main difference is that the  mov  file will be copied from  your Gluster directory instead of being downloaded from\nStash.\nLike before, your script should remove  that file before the job completes so that it doesn't get transferred back to\nthe submit server.    Remove the lines in the  run_ffmpeg.sh  that mention  module load .    Remove the  stashcp  line    Change the first command of your  run_ffmpeg.sh  script to only copy one  .mov  file:   cp /mnt/gluster/<USERNAME>/test_open_terminal.mov ./    You should use your username on  learn.chtc.wisc.edu  in the path above.\nIf you have a version of the script that uses arguments instead of the filenames, that's okay.",
            "title": "Script"
        },
        {
            "location": "/materials/day4/part3-ex1-input/#submit-file",
            "text": "Remove any previous requirements and add a line to the file (before the final queue statement) that ensures your job\n    will land on computers that have access to Gluster:  requirements = (Target.HasGluster == true)",
            "title": "Submit File"
        },
        {
            "location": "/materials/day4/part3-ex1-input/#initial-job",
            "text": "As before, we should test our job submission with a single  mov  file before submitting jobs for all three.\nAlter your submit file (if necessary) to  run a job that converts the  test_open_terminal.mov  file.  Once the job finishes, check to make sure everything ran as expected:   Check the directory where you submitted the job. Did the output  .mp4  file return?  Also in the directory where you submitted the job - did the original  .mov  file return here accidentally?  Check file sizes. How big is the returned  .mp4  file? How does that compare to the original  .mov  input?   If your job successfully returned the converted  .mp4  file and  not  the  .mov  file to the submit server, and the .mp4  file was appropriately scaled down, then our script did what it should have.",
            "title": "Initial Job"
        },
        {
            "location": "/materials/day4/part3-ex1-input/#multiple-jobs",
            "text": "Change your submit file as in the previous exercise in order to submit 3 jobs to convert all three files!",
            "title": "Multiple jobs"
        },
        {
            "location": "/materials/day4/part3-ex2-output/",
            "text": "Thursday Exercise 3.2: Large Output Data\n\u00b6\n\n\nIn this exercise, we will run a job that produces a very large output file, based on a few parameters.\nThis exercise should take 15-20 minutes.\n\n\nBackground\n\u00b6\n\n\nThis exercise will be the reverse of the previous exercise!\nInstead of large input/small output, we will be using a program that has no input except for a few arguments on the\ncommand line, but produces a file that is several GB in size.\nAs before, we will need to write a shell script that runs the program and handles the data.\n\n\nThe Program\n\u00b6\n\n\nIf you haven't already, log in to \nlearn.chtc.wisc.edu\n. Download the software package and untar it.\n\n\nuser@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/motif-flanks.tar.gz\n\nuser@learn $\n tar -xzf motif-flanks.tar.gz\n\n\n\n\n\nUse the \ncd\n command to enter the unpacked \nmotif-flanks\n directory.\nTake a look at the \nREADME\n file and then do the following:\n\n\n\n\nCompile the code. \n\n\nRun the program without any arguments. \n\n\nBased on the \nREADME\n, what is the largest amount of data we might expect?\n\n\n\n\nThis program generates all permutations of nucleotide sequences surrounding a given DNA motif.\nWe can choose the length of permutation we want both before and after a motif of our choice.\nTo use this program on the command line and save the output to a \nFASTA\n file, we can use the command:\n\n\nuser@learn $\n ./motif-flanks \n2\n AGTTCATGCCT \n2\n > sequences.fa\n\n\n\n\n\nAccording to the usage information and README, the two numerical arguments can add up to 13, at most, and the middle\nsequence can be any  DNA sequence up to 20 characters.\nThe largest output we can expect is around 4 GB.\n\n\nTest Job\n\u00b6\n\n\nHaving output of up to 4 GB means two things: we will want to run a smaller test before we run the program at its peak,\nand the output data will need to go into a shared location like Gluster, instead of returning to the submit server.\n\n\nFirst, we'll create a shell script to serve as the job's executable.\n\n\n\n\nWhat commands do you need to put in the script?\n    What do you need to do with the \nsequences.fa\n file before the job exits?\n\n\nOur script needs to run the \nmotif-flanks\n command as shown above, redirecting the output to a file called \nsequences.fa\n.\n    Then, after that command completes, the \nsequences.fa\n file should be moved to your Gluster directory, as it is too\n    large to return to the submit server as usual.\n\n\n\n\nWrite the script and then check it against the script below. Yours might look slightly different. \n\n\n#!/bin/sh\n\n./motif-flanks 4 GATTTTCGATC 4 > sequences.fa\nmv sequences.fa /mnt/gluster/<USERNAME>\n\n\n\n\n\nReplacing \n<USERNAME>\n with your own username.\n\n\n\n\n\n\n\n\nNote\n\n\nNote that the two arguments in the script (4 and 4) are much smaller than the total possible for the software\n(two values that add up to 13).\nThis is because we want to run a smaller test before submitting a job with the largest possible combination of arguments.\n\n\n\n\nNext, create a submit file for this job, based on other submit files from the school.\nSome important considerations:\n\n\n\n\nWe're writing our file to the job's working directory, so make sure to request several GB of disk space.\n\n\nAdd a line to the file that ensures your job will land on computers that have access to Gluster\n    (see the file from the \nlast exercise\n).\n\n\nThe \nexecutable\n will be the script you wrote above.\n\n\n\n\nOnce you have a submit file that does all these things, submit the test job.\n\n\nOnce the job has completed, do the following:\n\n\n\n\nCheck the directory where you submitted the job. Has the \nsequences.fa\n file returned there, accidentally?\n\n\nCheck your Gluster directory. Did the \nsequences.fa\n file get copied there successfully?\n\n\nCheck file size. How big is the \nsequences.fa\n file? You can use the \nls -lh\n command with the filename to find out.\n\n\n\n\nIf your job successfully copied the \nsequences.fa\n file to Gluster and did \nnot\n return it to your submission\ndirectory on the submit server, congratulations!\nEverything is working as it should and you can now submit a full job.\n\n\nFinal Job\n\u00b6\n\n\nHaving done a test, it should be straightforward to run a \"full scale\" job.\nEdit your \nrun_motif.sh\n executable so that the \nmotif-flanks\n command  uses larger numerical arguments:\n\n\n./motif-flanks 6 GATTTTCGATC 7 > sequences.fa\n\n\n\n\n\nThen submit your job. When it completes, check the size of the output file in Gluster.",
            "title": "Exercise 3.2"
        },
        {
            "location": "/materials/day4/part3-ex2-output/#thursday-exercise-32-large-output-data",
            "text": "In this exercise, we will run a job that produces a very large output file, based on a few parameters.\nThis exercise should take 15-20 minutes.",
            "title": "Thursday Exercise 3.2: Large Output Data"
        },
        {
            "location": "/materials/day4/part3-ex2-output/#background",
            "text": "This exercise will be the reverse of the previous exercise!\nInstead of large input/small output, we will be using a program that has no input except for a few arguments on the\ncommand line, but produces a file that is several GB in size.\nAs before, we will need to write a shell script that runs the program and handles the data.",
            "title": "Background"
        },
        {
            "location": "/materials/day4/part3-ex2-output/#the-program",
            "text": "If you haven't already, log in to  learn.chtc.wisc.edu . Download the software package and untar it.  user@learn $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/motif-flanks.tar.gz user@learn $  tar -xzf motif-flanks.tar.gz  Use the  cd  command to enter the unpacked  motif-flanks  directory.\nTake a look at the  README  file and then do the following:   Compile the code.   Run the program without any arguments.   Based on the  README , what is the largest amount of data we might expect?   This program generates all permutations of nucleotide sequences surrounding a given DNA motif.\nWe can choose the length of permutation we want both before and after a motif of our choice.\nTo use this program on the command line and save the output to a  FASTA  file, we can use the command:  user@learn $  ./motif-flanks  2  AGTTCATGCCT  2  > sequences.fa  According to the usage information and README, the two numerical arguments can add up to 13, at most, and the middle\nsequence can be any  DNA sequence up to 20 characters.\nThe largest output we can expect is around 4 GB.",
            "title": "The Program"
        },
        {
            "location": "/materials/day4/part3-ex2-output/#test-job",
            "text": "Having output of up to 4 GB means two things: we will want to run a smaller test before we run the program at its peak,\nand the output data will need to go into a shared location like Gluster, instead of returning to the submit server.  First, we'll create a shell script to serve as the job's executable.   What commands do you need to put in the script?\n    What do you need to do with the  sequences.fa  file before the job exits?  Our script needs to run the  motif-flanks  command as shown above, redirecting the output to a file called  sequences.fa .\n    Then, after that command completes, the  sequences.fa  file should be moved to your Gluster directory, as it is too\n    large to return to the submit server as usual.   Write the script and then check it against the script below. Yours might look slightly different.   #!/bin/sh\n\n./motif-flanks 4 GATTTTCGATC 4 > sequences.fa\nmv sequences.fa /mnt/gluster/<USERNAME>  Replacing  <USERNAME>  with your own username.     Note  Note that the two arguments in the script (4 and 4) are much smaller than the total possible for the software\n(two values that add up to 13).\nThis is because we want to run a smaller test before submitting a job with the largest possible combination of arguments.   Next, create a submit file for this job, based on other submit files from the school.\nSome important considerations:   We're writing our file to the job's working directory, so make sure to request several GB of disk space.  Add a line to the file that ensures your job will land on computers that have access to Gluster\n    (see the file from the  last exercise ).  The  executable  will be the script you wrote above.   Once you have a submit file that does all these things, submit the test job.  Once the job has completed, do the following:   Check the directory where you submitted the job. Has the  sequences.fa  file returned there, accidentally?  Check your Gluster directory. Did the  sequences.fa  file get copied there successfully?  Check file size. How big is the  sequences.fa  file? You can use the  ls -lh  command with the filename to find out.   If your job successfully copied the  sequences.fa  file to Gluster and did  not  return it to your submission\ndirectory on the submit server, congratulations!\nEverything is working as it should and you can now submit a full job.",
            "title": "Test Job"
        },
        {
            "location": "/materials/day4/part3-ex2-output/#final-job",
            "text": "Having done a test, it should be straightforward to run a \"full scale\" job.\nEdit your  run_motif.sh  executable so that the  motif-flanks  command  uses larger numerical arguments:  ./motif-flanks 6 GATTTTCGATC 7 > sequences.fa  Then submit your job. When it completes, check the size of the output file in Gluster.",
            "title": "Final Job"
        },
        {
            "location": "/materials/day4/part4-ex1-simple-dag/",
            "text": "pre em { font-style: normal; background-color: yellow; }\n  pre strong { font-style: normal; font-weight: bold; color: \\#008; }\n\n\n\n\nMonday Exercise 4.1: Coordinating a Set of Jobs With a Simple DAG\n\u00b6\n\n\nThe objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job.\n\n\nWhat is DAGMan?\n\u00b6\n\n\nIn short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph.\nFor example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data.  After\nthe sweep runs, you need to collate the results.  This might look like this, assuming you want to sweep over five\nparameters:\n\n\n\n\nDAGMan has many abilities, such as throttling jobs, recovery from failures, and more.  More information about DAGMan can\nbe found at \nin the HTCondor manual\n.\n\n\nSubmitting a Simple DAG\n\u00b6\n\n\nFor our job, we will return briefly to the \nsleep\n program.\n\n\nexecutable              = /bin/sleep\narguments               = 4\nlog                     = simple.log\noutput                  = simple.out\nerror                   = simple.error\nrequest_memory = 1GB\nrequest_disk = 1GB\nrequest_cpus = 1\nqueue\n\n\n\n\n\nWe are going to get a bit more sophisticated in submitting our jobs now.  Let's have three windows open.  In one window,\nyou'll submit the job.  In another you will watch the queue, and in the third you will watch what DAGMan does.\n\n\nFirst we will create the most minimal DAG that can be created: a DAG with just one node.  Put this into a file named\n\nsimple.dag\n.\n\n\nJob Simple simple.sub\n\n\n\n\n\nIn your first window, submit the DAG:\n\n\nusername@learn $\n condor_submit_dag simple.dag\n\n-----------------------------------------------------------------------\n\n\nFile for submitting this DAG to Condor           : simple.dag.condor.sub\n\n\nLog of DAGMan debugging messages                 : simple.dag.dagman.out\n\n\nLog of Condor library output                     : simple.dag.lib.out\n\n\nLog of Condor library error messages             : simple.dag.lib.err\n\n\nLog of the life of condor_dagman itself          : simple.dag.dagman.log\n\n\n\nSubmitting job(s).\n\n\n1 job(s) submitted to cluster 61.\n\n\n-----------------------------------------------------------------------\n\n\n\n\n\n\nIn the second window, watch the queue (what you see may be slightly different):\n\n\nusername@learn $\n watch -n \n10\n condor_q -nobatch -wide:80\n\n\n-- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu\n\n\n ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               \n\n\n  61.0   roy             6/21 22:51   0+00:00:01 R  0   0.3  condor_dagman     \n\n\n\n1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended\n\n\n\n-- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu\n\n\n ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               \n\n\n  61.0   roy             6/21 22:51   0+00:01:25 R  0   0.3  condor_dagman     \n\n\n  62.0   roy             6/21 22:51   0+00:00:00 I  0   0.7  simple 4 10      \n\n\n\n2 jobs; 0 completed, 0 removed, 1 idle, 1 running, 0 held, 0 suspended\n\n\n\n-- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu\n\n\n ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               \n\n\n  61.0   roy             6/21 22:51   0+00:03:47 R  0   0.3  condor_dagman     \n\n\n  62.0   roy             6/21 22:51   0+00:00:03 R  0   0.7  simple 4 10      \n\n\n\n2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended\n\n\n\n-- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu\n\n\n ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               \n\n\n\n0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended\n\n\n#\n<Ctrl-C>\n\n\n\n\n\n\nIn the third window, watch what DAGMan does (what you see may be slightly different):\n\n\nusername@learn $\n tail -f --lines\n=\n500\n simple.dag.dagman.out\n\n6/21/12 22:51:13 Setting maximum accepts per cycle 8.\n\n\n06/21/12 22:51:13 ******************************************************\n\n\n06/21/12 22:51:13 ** condor_scheduniv_exec.61.0 (CONDOR_DAGMAN) STARTING UP\n\n\n06/21/12 22:51:13 ** /usr/bin/condor_dagman\n\n\n06/21/12 22:51:13 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)\n\n\n06/21/12 22:51:13 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON\n\n\n06/21/12 22:51:13 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\n\n\n06/21/12 22:51:13 ** $CondorPlatform: x86_64_rhap_5.7 $\n\n\n06/21/12 22:51:13 ** PID = 5812\n\n\n06/21/12 22:51:13 ** Log last touched 6/21 22:51:00\n\n\n06/21/12 22:51:13 ******************************************************\n\n\n06/21/12 22:51:13 Using config source: /etc/condor/condor_config\n\n\n06/21/12 22:51:13 Using local config sources: \n\n\n06/21/12 22:51:13    /etc/condor/config.d/00-chtc-global.conf\n\n\n06/21/12 22:51:13    /etc/condor/config.d/01-chtc-submit.conf\n\n\n06/21/12 22:51:13    /etc/condor/config.d/02-chtc-flocking.conf\n\n\n06/21/12 22:51:13    /etc/condor/config.d/03-chtc-jobrouter.conf\n\n\n06/21/12 22:51:13    /etc/condor/config.d/04-chtc-blacklist.conf\n\n\n06/21/12 22:51:13    /etc/condor/config.d/99-osg-ss-group.conf\n\n\n06/21/12 22:51:13    /etc/condor/config.d/99-roy-extras.conf\n\n\n06/21/12 22:51:13    /etc/condor/condor_config.local\n\n\n06/21/12 22:51:13 DaemonCore: command socket at <128.104.100.55:60417>\n\n\n06/21/12 22:51:13 DaemonCore: private command socket at <128.104.100.55:60417>\n\n\n06/21/12 22:51:13 Setting maximum accepts per cycle 8.\n\n\n06/21/12 22:51:13 DAGMAN_USE_STRICT setting: 0\n\n\n06/21/12 22:51:13 DAGMAN_VERBOSITY setting: 3\n\n\n06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880\n\n\n06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_ENABLE setting: False\n\n\n06/21/12 22:51:13 DAGMAN_SUBMIT_DELAY setting: 0\n\n\n06/21/12 22:51:13 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6\n\n\n06/21/12 22:51:13 DAGMAN_STARTUP_CYCLE_DETECT setting: False\n\n\n06/21/12 22:51:13 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5\n\n\n06/21/12 22:51:13 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5\n\n\n06/21/12 22:51:13 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114\n\n\n06/21/12 22:51:13 DAGMAN_RETRY_SUBMIT_FIRST setting: True\n\n\n06/21/12 22:51:13 DAGMAN_RETRY_NODE_FIRST setting: False\n\n\n06/21/12 22:51:13 DAGMAN_MAX_JOBS_IDLE setting: 0\n\n\n06/21/12 22:51:13 DAGMAN_MAX_JOBS_SUBMITTED setting: 0\n\n\n06/21/12 22:51:15 DAGMAN_MAX_PRE_SCRIPTS setting: 0\n\n\n06/21/12 22:51:15 DAGMAN_MAX_POST_SCRIPTS setting: 0\n\n\n06/21/12 22:51:15 DAGMAN_ALLOW_LOG_ERROR setting: False\n\n\n06/21/12 22:51:15 DAGMAN_MUNGE_NODE_NAMES setting: True\n\n\n06/21/12 22:51:15 DAGMAN_PROHIBIT_MULTI_JOBS setting: False\n\n\n06/21/12 22:51:15 DAGMAN_SUBMIT_DEPTH_FIRST setting: False\n\n\n06/21/12 22:51:15 DAGMAN_ALWAYS_RUN_POST setting: True\n\n\n06/21/12 22:51:15 DAGMAN_ABORT_DUPLICATES setting: True\n\n\n06/21/12 22:51:15 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True\n\n\n06/21/12 22:51:15 DAGMAN_PENDING_REPORT_INTERVAL setting: 600\n\n\n06/21/12 22:51:15 DAGMAN_AUTO_RESCUE setting: True\n\n\n06/21/12 22:51:15 DAGMAN_MAX_RESCUE_NUM setting: 100\n\n\n06/21/12 22:51:15 DAGMAN_WRITE_PARTIAL_RESCUE setting: True\n\n\n06/21/12 22:51:15 DAGMAN_DEFAULT_NODE_LOG setting: null\n\n\n06/21/12 22:51:15 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True\n\n\n06/21/12 22:51:15 ALL_DEBUG setting: \n\n\n06/21/12 22:51:15 DAGMAN_DEBUG setting: \n\n\n06/21/12 22:51:15 argv[0] == \"condor_scheduniv_exec.61.0\"\n\n\n06/21/12 22:51:15 argv[1] == \"-Lockfile\"\n\n\n06/21/12 22:51:15 argv[2] == \"simple.dag.lock\"\n\n\n06/21/12 22:51:15 argv[3] == \"-AutoRescue\"\n\n\n06/21/12 22:51:15 argv[4] == \"1\"\n\n\n06/21/12 22:51:15 argv[5] == \"-DoRescueFrom\"\n\n\n06/21/12 22:51:15 argv[6] == \"0\"\n\n\n06/21/12 22:51:15 argv[7] == \"-Dag\"\n\n\n06/21/12 22:51:15 argv[8] == \"simple.dag\"\n\n\n06/21/12 22:51:15 argv[9] == \"-CsdVersion\"\n\n\n06/21/12 22:51:15 argv[10] == \"$CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\"\n\n\n06/21/12 22:51:15 argv[11] == \"-Force\"\n\n\n06/21/12 22:51:15 argv[12] == \"-Dagman\"\n\n\n06/21/12 22:51:15 argv[13] == \"/usr/bin/condor_dagman\"\n\n\n06/21/12 22:51:15 Default node log file is: </home/roy/condor/simple.dag.nodes.log>\n\n\n06/21/12 22:51:15 DAG Lockfile will be written to simple.dag.lock\n\n\n06/21/12 22:51:15 DAG Input file is simple.dag\n\n\n06/21/12 22:51:15 Parsing 1 dagfiles\n\n\n06/21/12 22:51:15 Parsing simple.dag ...\n\n\n06/21/12 22:51:15 Dag contains 1 total jobs\n\n\n06/21/12 22:51:15 Sleeping for 12 seconds to ensure ProcessId uniqueness\n\n\n06/21/12 22:51:27 Bootstrapping...\n\n\n06/21/12 22:51:27 Number of pre-completed nodes: 0\n\n\n06/21/12 22:51:27 Registering condor_event_timer...\n\n\n06/21/12 22:51:28 Sleeping for one second for log file consistency\n\n\n06/21/12 22:51:29 MultiLogFiles: truncating log file /home/roy/condor/simple.log\n\n\n06/21/12 22:51:29 Submitting Condor Node Simple job(s)...\n\n\n\n\n\n\nHere's where the job is submitted\n\n\n06/21/12 22:51:29 submitting: condor_submit \n                              -a dag_node_name' '=' 'Simple \n                              -a +DAGManJobId' '=' '61 \n                              -a DAGManJobId' '=' '61 \n                              -a submit_event_notes' '=' 'DAG' 'Node:' 'Simple \n                              -a DAG_STATUS' '=' '0 \n                              -a FAILED_COUNT' '=' '0 \n                              -a +DAGParentNodeNames' '=' '\"\" submit\n06/21/12 22:51:30 From submit: Submitting job(s).\n06/21/12 22:51:30 From submit: 1 job(s) submitted to cluster 62.\n06/21/12 22:51:30   assigned Condor ID (62.0.0)\n06/21/12 22:51:30 Just submitted 1 job this cycle...\n06/21/12 22:51:30 Currently monitoring 1 Condor log file(s)\n06/21/12 22:51:30 Event: ULOG_SUBMIT for Condor Node Simple (62.0.0)\n06/21/12 22:51:30 Number of idle job procs: 1\n06/21/12 22:51:30 Of 1 nodes total:\n06/21/12 22:51:30  Done     Pre   Queued    Post   Ready   Un-Ready   Failed\n06/21/12 22:51:30   ===     ===      ===     ===     ===        ===      ===\n06/21/12 22:51:30     0       0        1       0       0          0        0\n06/21/12 22:51:30 0 job proc(s) currently held\n06/21/12 22:55:05 Currently monitoring 1 Condor log file(s)\n\n\n\n\n\nHere's where DAGMan noticed that the job is running\n\n\n06/21/12 22:55:05 Event: ULOG_EXECUTE for Condor Node Simple (62.0.0)\n06/21/12 22:55:05 Number of idle job procs: 0\n06/21/12 22:55:10 Currently monitoring 1 Condor log file(s)\n06/21/12 22:55:10 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0)\n06/21/12 22:56:05 Currently monitoring 1 Condor log file(s)\n06/21/12 22:56:05 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0)\n\n\n\n\n\nHere's where DAGMan noticed that the job finished.\n\n\n06/21/12 22:56:05 Event: ULOG_JOB_TERMINATED for Condor Node Simple (62.0.0)\n06/21/12 22:56:05 Node Simple job proc (62.0.0) completed successfully.\n06/21/12 22:56:05 Node Simple job completed\n06/21/12 22:56:05 Number of idle job procs: 0\n06/21/12 22:56:05 Of 1 nodes total:\n06/21/12 22:56:05  Done     Pre   Queued    Post   Ready   Un-Ready   Failed\n06/21/12 22:56:05   ===     ===      ===     ===     ===        ===      ===\n06/21/12 22:56:05     1       0        0       0       0          0        0\n06/21/12 22:56:05 0 job proc(s) currently held\n\n\n\n\n\nHere's where DAGMan noticed that all the work is done.\n\n\n06/21/12 22:56:05 All jobs Completed!\n06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxJobs limit (0)\n06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxIdle limit (0)\n06/21/12 22:56:05 Note: 0 total job deferrals because of node category throttles\n06/21/12 22:56:05 Note: 0 total PRE script deferrals because of -MaxPre limit (0)\n06/21/12 22:56:05 Note: 0 total POST script deferrals because of -MaxPost limit (0)\n06/21/12 22:56:05 **** condor_scheduniv_exec.61.0 (condor_DAGMAN) pid 5812 EXITING WITH STATUS 0\n\n\n\n\n\nNow verify your results:\n\n\nusername@learn $\n cat simple.log\n\n000 (062.000.000) 06/21 22:51:30 Job submitted from host: <128.104.100.55:9618?sock=28867_10e4_2>\n\n\n    DAG Node: Simple\n\n\n...\n\n\n001 (062.000.000) 06/21 22:55:00 Job executing on host: <128.104.58.36:46761>\n\n\n...\n\n\n006 (062.000.000) 06/21 22:55:09 Image size of job updated: 750\n\n\n    3  -  MemoryUsage of job (MB)\n\n\n    2324  -  ResidentSetSize of job (KB)\n\n\n...\n\n\n006 (062.000.000) 06/21 22:56:00 Image size of job updated: 780\n\n\n    3  -  MemoryUsage of job (MB)\n\n\n    2324  -  ResidentSetSize of job (KB)\n\n\n...\n\n\n005 (062.000.000) 06/21 22:56:00 Job terminated.\n\n\n    (1) Normal termination (return value 0)\n\n\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage\n\n\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage\n\n\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage\n\n\n        Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage\n\n\n    57  -  Run Bytes Sent By Job\n\n\n    608490  -  Run Bytes Received By Job\n\n\n    57  -  Total Bytes Sent By Job\n\n\n    608490  -  Total Bytes Received By Job\n\n\n    Partitionable Resources :    Usage  Request          \n\n\n       Cpus                 :                 1          \n\n\n       Disk (KB)            :      750      750          \n\n\n       Memory (MB)          :        3        3          \n\n\n...\n\n\n\n\n\n\nLooking at DAGMan's various files, we see that DAGMan itself ran as a job (specifically, a \"scheduler\" universe job).\n\n\nusername@learn $\n ls simple.dag.*\n\nsimple.dag.condor.sub  simple.dag.dagman.log  simple.dag.dagman.out  simple.dag.lib.err  simple.dag.lib.out \n\n\n\nusername@learn $\n cat simple.dag.condor.sub\n\n#\n Filename: simple.dag.condor.sub\n\n#\n Generated by condor_submit_dag simple.dag \n\nuniverse    = scheduler\n\n\nexecutable  = /usr/bin/condor_dagman\n\n\ngetenv      = True\n\n\noutput      = simple.dag.lib.out\n\n\nerror       = simple.dag.lib.err\n\n\nlog     = simple.dag.dagman.log\n\n\nremove_kill_sig = SIGUSR1\n\n\n+OtherJobRemoveRequirements = \"DAGManJobId == $(cluster)\"\n\n\n#\n Note: default on_exit_remove expression:\n\n#\n \n(\n \nExitSignal\n \n=\n?\n=\n \n11\n \n||\n \n(\nExitCode\n \n=\n!\n=\n UNDEFINED \n&&\n ExitCode >\n=\n0\n \n&&\n ExitCode <\n=\n \n2\n))\n\n\n#\n attempts to ensure that DAGMan is automatically\n\n#\n requeued by the schedd \nif\n it exits abnormally or\n\n#\n is killed \n(\ne.g., during a reboot\n)\n.\n\non_exit_remove  = ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2))\n\n\ncopy_to_spool   = False\n\n\narguments   = \"-f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag \n\n\n                              -CsdVersion $CondorVersion:' '7.7.6' 'Apr' '16' '2012' 'BuildID:' '34175' 'PRE-RELEASE-UWCS' '$ -Force -Dagman /usr/bin/condor_dagman\"\n\n\nenvironment = _CONDOR_DAGMAN_LOG=simple.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0\n\n\nqueue\n\n\n\n\n\n\nIf you want to clean up some of these files (you may not want to, at least not yet), run:\n\n\nusername@learn $\n rm simple.dag.*\n\n\n\n\n\nChallenge\n\u00b6\n\n\n\n\nWhat is the scheduler universe? Why does DAGMan use it?",
            "title": "Exercise 4.1"
        },
        {
            "location": "/materials/day4/part4-ex1-simple-dag/#monday-exercise-41-coordinating-a-set-of-jobs-with-a-simple-dag",
            "text": "The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job.",
            "title": "Monday Exercise 4.1: Coordinating a Set of Jobs With a Simple DAG"
        },
        {
            "location": "/materials/day4/part4-ex1-simple-dag/#what-is-dagman",
            "text": "In short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph.\nFor example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data.  After\nthe sweep runs, you need to collate the results.  This might look like this, assuming you want to sweep over five\nparameters:   DAGMan has many abilities, such as throttling jobs, recovery from failures, and more.  More information about DAGMan can\nbe found at  in the HTCondor manual .",
            "title": "What is DAGMan?"
        },
        {
            "location": "/materials/day4/part4-ex1-simple-dag/#submitting-a-simple-dag",
            "text": "For our job, we will return briefly to the  sleep  program.  executable              = /bin/sleep\narguments               = 4\nlog                     = simple.log\noutput                  = simple.out\nerror                   = simple.error\nrequest_memory = 1GB\nrequest_disk = 1GB\nrequest_cpus = 1\nqueue  We are going to get a bit more sophisticated in submitting our jobs now.  Let's have three windows open.  In one window,\nyou'll submit the job.  In another you will watch the queue, and in the third you will watch what DAGMan does.  First we will create the most minimal DAG that can be created: a DAG with just one node.  Put this into a file named simple.dag .  Job Simple simple.sub  In your first window, submit the DAG:  username@learn $  condor_submit_dag simple.dag -----------------------------------------------------------------------  File for submitting this DAG to Condor           : simple.dag.condor.sub  Log of DAGMan debugging messages                 : simple.dag.dagman.out  Log of Condor library output                     : simple.dag.lib.out  Log of Condor library error messages             : simple.dag.lib.err  Log of the life of condor_dagman itself          : simple.dag.dagman.log  Submitting job(s).  1 job(s) submitted to cluster 61.  -----------------------------------------------------------------------   In the second window, watch the queue (what you see may be slightly different):  username@learn $  watch -n  10  condor_q -nobatch -wide:80 -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu   ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD                   61.0   roy             6/21 22:51   0+00:00:01 R  0   0.3  condor_dagman       1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended  -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu   ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD                   61.0   roy             6/21 22:51   0+00:01:25 R  0   0.3  condor_dagman         62.0   roy             6/21 22:51   0+00:00:00 I  0   0.7  simple 4 10        2 jobs; 0 completed, 0 removed, 1 idle, 1 running, 0 held, 0 suspended  -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu   ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD                   61.0   roy             6/21 22:51   0+00:03:47 R  0   0.3  condor_dagman         62.0   roy             6/21 22:51   0+00:00:03 R  0   0.7  simple 4 10        2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended  -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu   ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD                 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended  # <Ctrl-C>   In the third window, watch what DAGMan does (what you see may be slightly different):  username@learn $  tail -f --lines = 500  simple.dag.dagman.out 6/21/12 22:51:13 Setting maximum accepts per cycle 8.  06/21/12 22:51:13 ******************************************************  06/21/12 22:51:13 ** condor_scheduniv_exec.61.0 (CONDOR_DAGMAN) STARTING UP  06/21/12 22:51:13 ** /usr/bin/condor_dagman  06/21/12 22:51:13 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)  06/21/12 22:51:13 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON  06/21/12 22:51:13 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $  06/21/12 22:51:13 ** $CondorPlatform: x86_64_rhap_5.7 $  06/21/12 22:51:13 ** PID = 5812  06/21/12 22:51:13 ** Log last touched 6/21 22:51:00  06/21/12 22:51:13 ******************************************************  06/21/12 22:51:13 Using config source: /etc/condor/condor_config  06/21/12 22:51:13 Using local config sources:   06/21/12 22:51:13    /etc/condor/config.d/00-chtc-global.conf  06/21/12 22:51:13    /etc/condor/config.d/01-chtc-submit.conf  06/21/12 22:51:13    /etc/condor/config.d/02-chtc-flocking.conf  06/21/12 22:51:13    /etc/condor/config.d/03-chtc-jobrouter.conf  06/21/12 22:51:13    /etc/condor/config.d/04-chtc-blacklist.conf  06/21/12 22:51:13    /etc/condor/config.d/99-osg-ss-group.conf  06/21/12 22:51:13    /etc/condor/config.d/99-roy-extras.conf  06/21/12 22:51:13    /etc/condor/condor_config.local  06/21/12 22:51:13 DaemonCore: command socket at <128.104.100.55:60417>  06/21/12 22:51:13 DaemonCore: private command socket at <128.104.100.55:60417>  06/21/12 22:51:13 Setting maximum accepts per cycle 8.  06/21/12 22:51:13 DAGMAN_USE_STRICT setting: 0  06/21/12 22:51:13 DAGMAN_VERBOSITY setting: 3  06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880  06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_ENABLE setting: False  06/21/12 22:51:13 DAGMAN_SUBMIT_DELAY setting: 0  06/21/12 22:51:13 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6  06/21/12 22:51:13 DAGMAN_STARTUP_CYCLE_DETECT setting: False  06/21/12 22:51:13 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5  06/21/12 22:51:13 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5  06/21/12 22:51:13 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114  06/21/12 22:51:13 DAGMAN_RETRY_SUBMIT_FIRST setting: True  06/21/12 22:51:13 DAGMAN_RETRY_NODE_FIRST setting: False  06/21/12 22:51:13 DAGMAN_MAX_JOBS_IDLE setting: 0  06/21/12 22:51:13 DAGMAN_MAX_JOBS_SUBMITTED setting: 0  06/21/12 22:51:15 DAGMAN_MAX_PRE_SCRIPTS setting: 0  06/21/12 22:51:15 DAGMAN_MAX_POST_SCRIPTS setting: 0  06/21/12 22:51:15 DAGMAN_ALLOW_LOG_ERROR setting: False  06/21/12 22:51:15 DAGMAN_MUNGE_NODE_NAMES setting: True  06/21/12 22:51:15 DAGMAN_PROHIBIT_MULTI_JOBS setting: False  06/21/12 22:51:15 DAGMAN_SUBMIT_DEPTH_FIRST setting: False  06/21/12 22:51:15 DAGMAN_ALWAYS_RUN_POST setting: True  06/21/12 22:51:15 DAGMAN_ABORT_DUPLICATES setting: True  06/21/12 22:51:15 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True  06/21/12 22:51:15 DAGMAN_PENDING_REPORT_INTERVAL setting: 600  06/21/12 22:51:15 DAGMAN_AUTO_RESCUE setting: True  06/21/12 22:51:15 DAGMAN_MAX_RESCUE_NUM setting: 100  06/21/12 22:51:15 DAGMAN_WRITE_PARTIAL_RESCUE setting: True  06/21/12 22:51:15 DAGMAN_DEFAULT_NODE_LOG setting: null  06/21/12 22:51:15 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True  06/21/12 22:51:15 ALL_DEBUG setting:   06/21/12 22:51:15 DAGMAN_DEBUG setting:   06/21/12 22:51:15 argv[0] == \"condor_scheduniv_exec.61.0\"  06/21/12 22:51:15 argv[1] == \"-Lockfile\"  06/21/12 22:51:15 argv[2] == \"simple.dag.lock\"  06/21/12 22:51:15 argv[3] == \"-AutoRescue\"  06/21/12 22:51:15 argv[4] == \"1\"  06/21/12 22:51:15 argv[5] == \"-DoRescueFrom\"  06/21/12 22:51:15 argv[6] == \"0\"  06/21/12 22:51:15 argv[7] == \"-Dag\"  06/21/12 22:51:15 argv[8] == \"simple.dag\"  06/21/12 22:51:15 argv[9] == \"-CsdVersion\"  06/21/12 22:51:15 argv[10] == \"$CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\"  06/21/12 22:51:15 argv[11] == \"-Force\"  06/21/12 22:51:15 argv[12] == \"-Dagman\"  06/21/12 22:51:15 argv[13] == \"/usr/bin/condor_dagman\"  06/21/12 22:51:15 Default node log file is: </home/roy/condor/simple.dag.nodes.log>  06/21/12 22:51:15 DAG Lockfile will be written to simple.dag.lock  06/21/12 22:51:15 DAG Input file is simple.dag  06/21/12 22:51:15 Parsing 1 dagfiles  06/21/12 22:51:15 Parsing simple.dag ...  06/21/12 22:51:15 Dag contains 1 total jobs  06/21/12 22:51:15 Sleeping for 12 seconds to ensure ProcessId uniqueness  06/21/12 22:51:27 Bootstrapping...  06/21/12 22:51:27 Number of pre-completed nodes: 0  06/21/12 22:51:27 Registering condor_event_timer...  06/21/12 22:51:28 Sleeping for one second for log file consistency  06/21/12 22:51:29 MultiLogFiles: truncating log file /home/roy/condor/simple.log  06/21/12 22:51:29 Submitting Condor Node Simple job(s)...   Here's where the job is submitted  06/21/12 22:51:29 submitting: condor_submit \n                              -a dag_node_name' '=' 'Simple \n                              -a +DAGManJobId' '=' '61 \n                              -a DAGManJobId' '=' '61 \n                              -a submit_event_notes' '=' 'DAG' 'Node:' 'Simple \n                              -a DAG_STATUS' '=' '0 \n                              -a FAILED_COUNT' '=' '0 \n                              -a +DAGParentNodeNames' '=' '\"\" submit\n06/21/12 22:51:30 From submit: Submitting job(s).\n06/21/12 22:51:30 From submit: 1 job(s) submitted to cluster 62.\n06/21/12 22:51:30   assigned Condor ID (62.0.0)\n06/21/12 22:51:30 Just submitted 1 job this cycle...\n06/21/12 22:51:30 Currently monitoring 1 Condor log file(s)\n06/21/12 22:51:30 Event: ULOG_SUBMIT for Condor Node Simple (62.0.0)\n06/21/12 22:51:30 Number of idle job procs: 1\n06/21/12 22:51:30 Of 1 nodes total:\n06/21/12 22:51:30  Done     Pre   Queued    Post   Ready   Un-Ready   Failed\n06/21/12 22:51:30   ===     ===      ===     ===     ===        ===      ===\n06/21/12 22:51:30     0       0        1       0       0          0        0\n06/21/12 22:51:30 0 job proc(s) currently held\n06/21/12 22:55:05 Currently monitoring 1 Condor log file(s)  Here's where DAGMan noticed that the job is running  06/21/12 22:55:05 Event: ULOG_EXECUTE for Condor Node Simple (62.0.0)\n06/21/12 22:55:05 Number of idle job procs: 0\n06/21/12 22:55:10 Currently monitoring 1 Condor log file(s)\n06/21/12 22:55:10 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0)\n06/21/12 22:56:05 Currently monitoring 1 Condor log file(s)\n06/21/12 22:56:05 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0)  Here's where DAGMan noticed that the job finished.  06/21/12 22:56:05 Event: ULOG_JOB_TERMINATED for Condor Node Simple (62.0.0)\n06/21/12 22:56:05 Node Simple job proc (62.0.0) completed successfully.\n06/21/12 22:56:05 Node Simple job completed\n06/21/12 22:56:05 Number of idle job procs: 0\n06/21/12 22:56:05 Of 1 nodes total:\n06/21/12 22:56:05  Done     Pre   Queued    Post   Ready   Un-Ready   Failed\n06/21/12 22:56:05   ===     ===      ===     ===     ===        ===      ===\n06/21/12 22:56:05     1       0        0       0       0          0        0\n06/21/12 22:56:05 0 job proc(s) currently held  Here's where DAGMan noticed that all the work is done.  06/21/12 22:56:05 All jobs Completed!\n06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxJobs limit (0)\n06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxIdle limit (0)\n06/21/12 22:56:05 Note: 0 total job deferrals because of node category throttles\n06/21/12 22:56:05 Note: 0 total PRE script deferrals because of -MaxPre limit (0)\n06/21/12 22:56:05 Note: 0 total POST script deferrals because of -MaxPost limit (0)\n06/21/12 22:56:05 **** condor_scheduniv_exec.61.0 (condor_DAGMAN) pid 5812 EXITING WITH STATUS 0  Now verify your results:  username@learn $  cat simple.log 000 (062.000.000) 06/21 22:51:30 Job submitted from host: <128.104.100.55:9618?sock=28867_10e4_2>      DAG Node: Simple  ...  001 (062.000.000) 06/21 22:55:00 Job executing on host: <128.104.58.36:46761>  ...  006 (062.000.000) 06/21 22:55:09 Image size of job updated: 750      3  -  MemoryUsage of job (MB)      2324  -  ResidentSetSize of job (KB)  ...  006 (062.000.000) 06/21 22:56:00 Image size of job updated: 780      3  -  MemoryUsage of job (MB)      2324  -  ResidentSetSize of job (KB)  ...  005 (062.000.000) 06/21 22:56:00 Job terminated.      (1) Normal termination (return value 0)          Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage          Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage          Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage          Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage      57  -  Run Bytes Sent By Job      608490  -  Run Bytes Received By Job      57  -  Total Bytes Sent By Job      608490  -  Total Bytes Received By Job      Partitionable Resources :    Usage  Request                   Cpus                 :                 1                   Disk (KB)            :      750      750                   Memory (MB)          :        3        3            ...   Looking at DAGMan's various files, we see that DAGMan itself ran as a job (specifically, a \"scheduler\" universe job).  username@learn $  ls simple.dag.* simple.dag.condor.sub  simple.dag.dagman.log  simple.dag.dagman.out  simple.dag.lib.err  simple.dag.lib.out   username@learn $  cat simple.dag.condor.sub #  Filename: simple.dag.condor.sub #  Generated by condor_submit_dag simple.dag  universe    = scheduler  executable  = /usr/bin/condor_dagman  getenv      = True  output      = simple.dag.lib.out  error       = simple.dag.lib.err  log     = simple.dag.dagman.log  remove_kill_sig = SIGUSR1  +OtherJobRemoveRequirements = \"DAGManJobId == $(cluster)\"  #  Note: default on_exit_remove expression: #   (   ExitSignal   = ? =   11   ||   ( ExitCode   = ! =  UNDEFINED  &&  ExitCode > = 0   &&  ExitCode < =   2 ))  #  attempts to ensure that DAGMan is automatically #  requeued by the schedd  if  it exits abnormally or #  is killed  ( e.g., during a reboot ) . on_exit_remove  = ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2))  copy_to_spool   = False  arguments   = \"-f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag                                 -CsdVersion $CondorVersion:' '7.7.6' 'Apr' '16' '2012' 'BuildID:' '34175' 'PRE-RELEASE-UWCS' '$ -Force -Dagman /usr/bin/condor_dagman\"  environment = _CONDOR_DAGMAN_LOG=simple.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0  queue   If you want to clean up some of these files (you may not want to, at least not yet), run:  username@learn $  rm simple.dag.*",
            "title": "Submitting a Simple DAG"
        },
        {
            "location": "/materials/day4/part4-ex1-simple-dag/#challenge",
            "text": "What is the scheduler universe? Why does DAGMan use it?",
            "title": "Challenge"
        },
        {
            "location": "/materials/day4/part4-ex2-mandelbrot/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 4.2: A Brief Detour Through the Mandelbrot Set\n\u00b6\n\n\nBefore we explore using DAGs to implement workflows, let\u2019s get a more interesting job. Let\u2019s make pretty pictures!\n\n\nWe have a small program that draws pictures of the Mandelbrot set. You can \nread about the Mandelbrot set on Wikipedia\n, or you can simply appreciate the pretty pictures. It\u2019s a fractal.\n\n\nWe have a simple program that can draw the Mandelbrot set. It's called \ngoatbrot\n.\n\n\nRunning goatbrot From the Command Line\n\u00b6\n\n\nYou can generate the Mandelbrot set as a quick test with two simple commands.\n\n\n\n\n\n\nGenerate a PPM image of the Mandelbrot set:\n\n\nusername@learn $\n goatbrot -i \n1000\n -o tile_000000_000000.ppm -c \n0\n,0 -w \n3\n -s \n1000\n,1000\n\n\n\n\n\nThe \ngoatbroat\n program takes several parameters. Let's break them down:\n\n\n\n\n-i 1000\n The number of iterations. Bigger numbers generate more accurate images but are slower to run.\n\n\n-o tile_000000_000000.ppm\n The output file to generate.\n\n\n-c 0,0\n The center point of the image. Here it is the point (0,0).\n\n\n-w 3\n The width of the image. Here is 3.\n\n\n-s 1000,1000\n The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall.\n\n\n\n\n\n\n\n\nConvert the image to the JPEG format (using a built-in program called \nconvert\n):\n\n\nusername@learn $\n convert tile_000000_000000.ppm mandel.jpg\n\n\n\n\n\n\n\n\n\nDividing the Work into Smaller Pieces\n\u00b6\n\n\nThe Mandelbrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations (an HTC approach!) then stitched them together? Once we do that, we can run each \ngoatbroat\n in parallel in our cluster. Here's an example you can run by hand.\n\n\n\n\n\n\nRun goatbroat 4 times:\n\n\nusername@learn $\n goatbrot -i \n1000\n -o tile_000000_000000.ppm -c -0.75,0.75 -w \n1\n.5 -s \n500\n,500\n\nusername@learn $\n goatbrot -i \n1000\n -o tile_000000_000001.ppm -c \n0\n.75,0.75 -w \n1\n.5 -s \n500\n,500 \n\nusername@learn $\n goatbrot -i \n1000\n -o tile_000001_000000.ppm -c -0.75,-0.75 -w \n1\n.5 -s \n500\n,500 \n\nusername@learn $\n goatbrot -i \n1000\n -o tile_000001_000001.ppm -c \n0\n.75,-0.75 -w \n1\n.5 -s \n500\n,500\n\n\n\n\n\n\n\n\n\nStitch the small images together into the complete image (in JPEG format):\n\n\nusername@learn $\n montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 mandel.jpg\n\n\n\n\n\n\n\n\n\nThis will produce the same image as above. We divided the image space into a 2\u00d72 grid and ran \ngoatbrot\n on each section of the grid. The built-in \nmontage\n program stitches the files together and writes out the final image in JPEG format.\n\n\nView the Image!\n\u00b6\n\n\nRun the commands above so that you have the Mandelbrot image. \nWhen you create the image, you might wonder how you can view it. \nIf you're comfortable with \nscp\n or another method, you can copy it back to your computer to view it. Otherwise you can view it in your web browser in three easy steps:\n\n\n\n\n\n\nMake your web directory (you only need to do this once):\n\n\nusername@learn $\n \ncd\n ~\n\nusername@learn $\n mkdir public_html \n\nusername@learn $\n chmod \n0711\n . \n\nusername@learn $\n chmod \n0755\n public_html\n\n\n\n\n\n\n\n\n\nCopy the image into your web directory (the below command assumes you're back in the directory where you created mandel.jpg):\n\n\nusername@learn $\n cp mandel.jpg ~/public_html/\n\n\n\n\n\n\n\n\n\nAccess \nhttp://learn.chtc.wisc.edu/~\n<USERNAME>\n/mandel.jpg\n in your web browser (change \n to your username on \nlearn.chtc.wisc.edu\n).",
            "title": "Exercise 4.2"
        },
        {
            "location": "/materials/day4/part4-ex2-mandelbrot/#monday-exercise-42-a-brief-detour-through-the-mandelbrot-set",
            "text": "Before we explore using DAGs to implement workflows, let\u2019s get a more interesting job. Let\u2019s make pretty pictures!  We have a small program that draws pictures of the Mandelbrot set. You can  read about the Mandelbrot set on Wikipedia , or you can simply appreciate the pretty pictures. It\u2019s a fractal.  We have a simple program that can draw the Mandelbrot set. It's called  goatbrot .",
            "title": "Monday Exercise 4.2: A Brief Detour Through the Mandelbrot Set"
        },
        {
            "location": "/materials/day4/part4-ex2-mandelbrot/#running-goatbrot-from-the-command-line",
            "text": "You can generate the Mandelbrot set as a quick test with two simple commands.    Generate a PPM image of the Mandelbrot set:  username@learn $  goatbrot -i  1000  -o tile_000000_000000.ppm -c  0 ,0 -w  3  -s  1000 ,1000  The  goatbroat  program takes several parameters. Let's break them down:   -i 1000  The number of iterations. Bigger numbers generate more accurate images but are slower to run.  -o tile_000000_000000.ppm  The output file to generate.  -c 0,0  The center point of the image. Here it is the point (0,0).  -w 3  The width of the image. Here is 3.  -s 1000,1000  The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall.     Convert the image to the JPEG format (using a built-in program called  convert ):  username@learn $  convert tile_000000_000000.ppm mandel.jpg",
            "title": "Running goatbrot From the Command Line"
        },
        {
            "location": "/materials/day4/part4-ex2-mandelbrot/#dividing-the-work-into-smaller-pieces",
            "text": "The Mandelbrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations (an HTC approach!) then stitched them together? Once we do that, we can run each  goatbroat  in parallel in our cluster. Here's an example you can run by hand.    Run goatbroat 4 times:  username@learn $  goatbrot -i  1000  -o tile_000000_000000.ppm -c -0.75,0.75 -w  1 .5 -s  500 ,500 username@learn $  goatbrot -i  1000  -o tile_000000_000001.ppm -c  0 .75,0.75 -w  1 .5 -s  500 ,500  username@learn $  goatbrot -i  1000  -o tile_000001_000000.ppm -c -0.75,-0.75 -w  1 .5 -s  500 ,500  username@learn $  goatbrot -i  1000  -o tile_000001_000001.ppm -c  0 .75,-0.75 -w  1 .5 -s  500 ,500    Stitch the small images together into the complete image (in JPEG format):  username@learn $  montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 mandel.jpg    This will produce the same image as above. We divided the image space into a 2\u00d72 grid and ran  goatbrot  on each section of the grid. The built-in  montage  program stitches the files together and writes out the final image in JPEG format.",
            "title": "Dividing the Work into Smaller Pieces"
        },
        {
            "location": "/materials/day4/part4-ex2-mandelbrot/#view-the-image",
            "text": "Run the commands above so that you have the Mandelbrot image. \nWhen you create the image, you might wonder how you can view it. \nIf you're comfortable with  scp  or another method, you can copy it back to your computer to view it. Otherwise you can view it in your web browser in three easy steps:    Make your web directory (you only need to do this once):  username@learn $   cd  ~ username@learn $  mkdir public_html  username@learn $  chmod  0711  .  username@learn $  chmod  0755  public_html    Copy the image into your web directory (the below command assumes you're back in the directory where you created mandel.jpg):  username@learn $  cp mandel.jpg ~/public_html/    Access  http://learn.chtc.wisc.edu/~ <USERNAME> /mandel.jpg  in your web browser (change   to your username on  learn.chtc.wisc.edu ).",
            "title": "View the Image!"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nMonday Exercise 4.3: A More Complex DAG\n\u00b6\n\n\nThe objective of this exercise is to run a real set of jobs with DAGMan.\n\n\nMake Your Job Submission Files\n\u00b6\n\n\nWe'll run our \ngoatbrot\n example. If you didn't read about it yet, \nplease do so now\n. We are going to make a DAG with four simultaneous jobs (\ngoatbrot\n) and one final node to stitch them together (\nmontage\n). This means we have five jobs. We're going to run \ngoatbrot\n with more iterations (100,000) so each job will take longer to run.\n\n\nYou can create your five jobs. The goatbrot jobs are very similar to each other, but they have slightly different parameters and output files.\n\n\ngoatbrot1.sub\n\u00b6\n\n\nexecutable              = /usr/local/bin/goatbrot\narguments               = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm\nlog                     = goatbrot.log\noutput                  = goatbrot.out.0.0\nerror                   = goatbrot.err.0.0\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nqueue\n\n\n\n\n\ngoatbrot2.sub\n\u00b6\n\n\nexecutable              = /usr/local/bin/goatbrot\narguments               = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm\nlog                     = goatbrot.log\noutput                  = goatbrot.out.0.1\nerror                   = goatbrot.err.0.1\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nqueue\n\n\n\n\n\ngoatbrot3.sub\n\u00b6\n\n\nexecutable              = /usr/local/bin/goatbrot\narguments               = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm\nlog                     = goatbrot.log\noutput                  = goatbrot.out.1.0\nerror                   = goatbrot.err.1.0\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nqueue\n\n\n\n\n\ngoatbrot4.sub\n\u00b6\n\n\nexecutable              = /usr/local/bin/goatbrot\narguments               = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm\nlog                     = goatbrot.log\noutput                  = goatbrot.out.1.1\nerror                   = goatbrot.err.1.1\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nqueue\n\n\n\n\n\nmontage.sub\n\u00b6\n\n\nYou should notice that the \ntransfer_input_files\n statement refers to the files created by the other jobs.\n\n\nexecutable              = /usr/bin/montage\narguments               = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandel-from-dag.jpg\ntransfer_input_files    = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm\noutput                  = montage.out\nerror                   = montage.err\nlog                     = montage.log\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nrequirements            = OpSysMajorVer =?= 7\nqueue\n\n\n\n\n\nMake your DAG\n\u00b6\n\n\nIn a file called \ngoatbrot.dag\n, you have your DAG specification:\n\n\nJOB g1 goatbrot1.sub\nJOB g2 goatbrot2.sub\nJOB g3 goatbrot3.sub\nJOB g4 goatbrot4.sub\nJOB montage montage.sub\nPARENT g1 g2 g3 g4 CHILD montage\n\n\n\n\n\nAsk yourself: do you know how we ensure that all the \ngoatbrot\n commands can run simultaneously and all of them will complete before we run the montage job?\n\n\nRunning the DAG\n\u00b6\n\n\nSubmit your DAG:\n\n\nusername@learn $\n condor_submit_dag goatbrot.dag\n\n-----------------------------------------------------------------------\n\n\nFile for submitting this DAG to Condor           : goatbrot.dag.condor.sub\n\n\nLog of DAGMan debugging messages                 : goatbrot.dag.dagman.out\n\n\nLog of Condor library output                     : goatbrot.dag.lib.out\n\n\nLog of Condor library error messages             : goatbrot.dag.lib.err\n\n\nLog of the life of condor_dagman itself          : goatbrot.dag.dagman.log\n\n\n\nSubmitting job(s).\n\n\n1 job(s) submitted to cluster 71.\n\n\n\n-----------------------------------------------------------------------\n\n\n\n\n\n\nWatch Your DAG\n\u00b6\n\n\nLet\u2019s follow the progress of the whole DAG:\n\n\n\n\n\n\nUse the \nwatch\n command to run \ncondor_q -nobatch -wide:80\n every 10 seconds:\n\n\nusername@learn $\n watch -n \n10\n condor_q -nobatch -wide:80\n\n\n\n\n\nHere we see DAGMan running:\n \n\n\n ID  OWNER  SUBMITTED   RUN_TIME ST PRI SIZE CMD \n\n\n71.0 roy   6/22 17:39 0+00:00:03 R  0    0.3 condor_dagman\n\n\n\n\n\n\nDAGMan has submitted the goatbrot jobs, but they haven't started running yet\n\n\n ID  OWNER SUBMITTED   RUN_TIME ST PRI SIZE CMD \n\n\n71.0 roy  6/22 17:39 0+00:00:17 R  0    0.3 condor_dagman \n\n\n72.0 roy  6/22 17:39 0+00:00:00 I  0    0.0 goatbrot -i 100000 \n\n\n73.0 roy  6/22 17:39 0+00:00:00 I  0    0.0 goatbrot -i 100000 \n\n\n74.0 roy  6/22 17:39 0+00:00:00 I  0    0.0 goatbrot -i 100000 \n\n\n75.0 roy  6/22 17:39 0+00:00:00 I  0    0.0 goatbrot -i 100000\n\n\n\n\n\n\nThey're running\n \n\n\n ID  OWNER SUBMITTED   RUN_TIME ST PRI SIZE CMD\n\n\n71.0 roy  6/22 17:39 0+00:07:15 R  0    0.3 condor_dagman \n\n\n72.0 roy  6/22 17:39 0+00:00:03 R  0    0.0 goatbrot -i 100000 \n\n\n73.0 roy  6/22 17:39 0+00:00:03 R  0    0.0 goatbrot -i 100000 \n\n\n74.0 roy  6/22 17:39 0+00:00:03 R  0    0.0 goatbrot -i 100000 \n\n\n75.0 roy  6/22 17:39 0+00:00:03 R  0    0.0 goatbrot -i 100000\n\n\n\n\n\n\nThey finished, but DAGMan hasn't noticed yet. It only checks periodically:\n\n\n ID  OWNER SUBMITTED   RUN_TIME ST PRI SIZE CMD \n\n\n71.0 roy  6/22 17:39 0+00:08:46 R  0    0.3 condor_dagman\n\n\n\n\n\n\nEventually, you'll see the montage job submitted, then running, then leave the queue, and then DAGMan will leave the queue.\n\n\n\n\n\n\nExamine your results. For some reason, goatbrot prints everything to stderr, not stdout.\n\n\nusername@learn $\n cat goatbrot.err.0.0 \n\nComplex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i\n\n\n\nOutput image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no\n\n\n\nMandelbrot: Max Iterations: 100000 Continuous: no\n\n\n\nGoatbrot: Multithreading: not supported in this build\n\n\n\nCompleted: 100.0%\n\n\n\n\n\n\n\n\n\n\nExamine your log files (\ngoatbrot.log\n and \nmontage.log\n) and DAGMan output file (\ngoatbrot.dag.dagman.out\n). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file?\n\n\n\n\nAs you did earlier, copy the resulting \nmandel-from-dag.jpg\n to your \npublic_html\n directory, then access it from your web browser. Does the image look correct?\n\n\nClean up your results by removing all of the \ngoatbrot.dag.*\n files if you like. Be careful to not delete the \ngoatbrot.dag\n file.\n\n\n\n\nBonus Challenge\n\u00b6\n\n\n\n\nRe-run your DAG. When jobs are running, try \ncondor_q -nobatch -dag\n. What does it do differently?\n\n\nChallenge, if you have time: Make a bigger DAG by making more tiles in the same area.",
            "title": "Exercise 4.3"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/#monday-exercise-43-a-more-complex-dag",
            "text": "The objective of this exercise is to run a real set of jobs with DAGMan.",
            "title": "Monday Exercise 4.3: A More Complex DAG"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/#make-your-job-submission-files",
            "text": "We'll run our  goatbrot  example. If you didn't read about it yet,  please do so now . We are going to make a DAG with four simultaneous jobs ( goatbrot ) and one final node to stitch them together ( montage ). This means we have five jobs. We're going to run  goatbrot  with more iterations (100,000) so each job will take longer to run.  You can create your five jobs. The goatbrot jobs are very similar to each other, but they have slightly different parameters and output files.",
            "title": "Make Your Job Submission Files"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/#goatbrot1sub",
            "text": "executable              = /usr/local/bin/goatbrot\narguments               = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm\nlog                     = goatbrot.log\noutput                  = goatbrot.out.0.0\nerror                   = goatbrot.err.0.0\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nqueue",
            "title": "goatbrot1.sub"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/#goatbrot2sub",
            "text": "executable              = /usr/local/bin/goatbrot\narguments               = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm\nlog                     = goatbrot.log\noutput                  = goatbrot.out.0.1\nerror                   = goatbrot.err.0.1\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nqueue",
            "title": "goatbrot2.sub"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/#goatbrot3sub",
            "text": "executable              = /usr/local/bin/goatbrot\narguments               = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm\nlog                     = goatbrot.log\noutput                  = goatbrot.out.1.0\nerror                   = goatbrot.err.1.0\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nqueue",
            "title": "goatbrot3.sub"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/#goatbrot4sub",
            "text": "executable              = /usr/local/bin/goatbrot\narguments               = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm\nlog                     = goatbrot.log\noutput                  = goatbrot.out.1.1\nerror                   = goatbrot.err.1.1\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nqueue",
            "title": "goatbrot4.sub"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/#montagesub",
            "text": "You should notice that the  transfer_input_files  statement refers to the files created by the other jobs.  executable              = /usr/bin/montage\narguments               = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandel-from-dag.jpg\ntransfer_input_files    = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm\noutput                  = montage.out\nerror                   = montage.err\nlog                     = montage.log\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nrequirements            = OpSysMajorVer =?= 7\nqueue",
            "title": "montage.sub"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/#make-your-dag",
            "text": "In a file called  goatbrot.dag , you have your DAG specification:  JOB g1 goatbrot1.sub\nJOB g2 goatbrot2.sub\nJOB g3 goatbrot3.sub\nJOB g4 goatbrot4.sub\nJOB montage montage.sub\nPARENT g1 g2 g3 g4 CHILD montage  Ask yourself: do you know how we ensure that all the  goatbrot  commands can run simultaneously and all of them will complete before we run the montage job?",
            "title": "Make your DAG"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/#running-the-dag",
            "text": "Submit your DAG:  username@learn $  condor_submit_dag goatbrot.dag -----------------------------------------------------------------------  File for submitting this DAG to Condor           : goatbrot.dag.condor.sub  Log of DAGMan debugging messages                 : goatbrot.dag.dagman.out  Log of Condor library output                     : goatbrot.dag.lib.out  Log of Condor library error messages             : goatbrot.dag.lib.err  Log of the life of condor_dagman itself          : goatbrot.dag.dagman.log  Submitting job(s).  1 job(s) submitted to cluster 71.  -----------------------------------------------------------------------",
            "title": "Running the DAG"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/#watch-your-dag",
            "text": "Let\u2019s follow the progress of the whole DAG:    Use the  watch  command to run  condor_q -nobatch -wide:80  every 10 seconds:  username@learn $  watch -n  10  condor_q -nobatch -wide:80  Here we see DAGMan running:     ID  OWNER  SUBMITTED   RUN_TIME ST PRI SIZE CMD   71.0 roy   6/22 17:39 0+00:00:03 R  0    0.3 condor_dagman   DAGMan has submitted the goatbrot jobs, but they haven't started running yet   ID  OWNER SUBMITTED   RUN_TIME ST PRI SIZE CMD   71.0 roy  6/22 17:39 0+00:00:17 R  0    0.3 condor_dagman   72.0 roy  6/22 17:39 0+00:00:00 I  0    0.0 goatbrot -i 100000   73.0 roy  6/22 17:39 0+00:00:00 I  0    0.0 goatbrot -i 100000   74.0 roy  6/22 17:39 0+00:00:00 I  0    0.0 goatbrot -i 100000   75.0 roy  6/22 17:39 0+00:00:00 I  0    0.0 goatbrot -i 100000   They're running     ID  OWNER SUBMITTED   RUN_TIME ST PRI SIZE CMD  71.0 roy  6/22 17:39 0+00:07:15 R  0    0.3 condor_dagman   72.0 roy  6/22 17:39 0+00:00:03 R  0    0.0 goatbrot -i 100000   73.0 roy  6/22 17:39 0+00:00:03 R  0    0.0 goatbrot -i 100000   74.0 roy  6/22 17:39 0+00:00:03 R  0    0.0 goatbrot -i 100000   75.0 roy  6/22 17:39 0+00:00:03 R  0    0.0 goatbrot -i 100000   They finished, but DAGMan hasn't noticed yet. It only checks periodically:   ID  OWNER SUBMITTED   RUN_TIME ST PRI SIZE CMD   71.0 roy  6/22 17:39 0+00:08:46 R  0    0.3 condor_dagman   Eventually, you'll see the montage job submitted, then running, then leave the queue, and then DAGMan will leave the queue.    Examine your results. For some reason, goatbrot prints everything to stderr, not stdout.  username@learn $  cat goatbrot.err.0.0  Complex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i  Output image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no  Mandelbrot: Max Iterations: 100000 Continuous: no  Goatbrot: Multithreading: not supported in this build  Completed: 100.0%     Examine your log files ( goatbrot.log  and  montage.log ) and DAGMan output file ( goatbrot.dag.dagman.out ). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file?   As you did earlier, copy the resulting  mandel-from-dag.jpg  to your  public_html  directory, then access it from your web browser. Does the image look correct?  Clean up your results by removing all of the  goatbrot.dag.*  files if you like. Be careful to not delete the  goatbrot.dag  file.",
            "title": "Watch Your DAG"
        },
        {
            "location": "/materials/day4/part4-ex3-complex-dag/#bonus-challenge",
            "text": "Re-run your DAG. When jobs are running, try  condor_q -nobatch -dag . What does it do differently?  Challenge, if you have time: Make a bigger DAG by making more tiles in the same area.",
            "title": "Bonus Challenge"
        },
        {
            "location": "/materials/day4/part4-ex4-failed-dag/",
            "text": "Monday Exercise 4.4: Handling a DAG That Fails\n\u00b6\n\n\nThe objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures.\n\n\nBackground\n\u00b6\n\n\nDAGMan can handle a situation where some of the nodes in a DAG fail. DAGMan will run as many nodes as possible, then create a rescue DAG making it easy to continue when the problem is fixed.\n\n\nBreaking Things\n\u00b6\n\n\nRecall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a \n-h\n to the arguments. It will look like this (the change is highlighted in red):\n\n\nexecutable              = /usr/bin/montage\narguments               = \n-h\n tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg\ntransfer_input_files    = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm\noutput                  = montage.out\nerror                   = montage.err\nlog                     = montage.log\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nrequirements            = OpSysMajorVer =?= 6\nqueue\n\n\n\n\n\nSubmit the DAG again:\n\n\nusername@learn $\n condor_submit_dag goatbrot.dag\n\n-----------------------------------------------------------------------\n\n\nFile for submitting this DAG to Condor           : goatbrot.dag.condor.sub\n\n\nLog of DAGMan debugging messages                 : goatbrot.dag.dagman.out\n\n\nLog of Condor library output                     : goatbrot.dag.lib.out\n\n\nLog of Condor library error messages             : goatbrot.dag.lib.err\n\n\nLog of the life of condor_dagman itself          : goatbrot.dag.dagman.log\n\n\n\nSubmitting job(s).\n\n\n1 job(s) submitted to cluster 77.\n\n\n-----------------------------------------------------------------------\n\n\n\n\n\n\nUse watch to watch the jobs until they finish. In a separate window, use \ntail --lines=500 -f goatbrot.dag.dagman.out\n to watch what DAGMan does.\n\n\n06/22/12 17:57:41 Setting maximum accepts per cycle 8.\n\n\n06/22/12 17:57:41 ******************************************************\n\n\n06/22/12 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP\n\n\n06/22/12 17:57:41 ** /usr/bin/condor_dagman\n\n\n06/22/12 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)\n\n\n06/22/12 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON\n\n\n06/22/12 17:57:41 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\n\n\n06/22/12 17:57:41 ** $CondorPlatform: x86_64_rhap_5.7 $\n\n\n06/22/12 17:57:41 ** PID = 26867\n\n\n06/22/12 17:57:41 ** Log last touched time unavailable (No such file or directory)\n\n\n06/22/12 17:57:41 ******************************************************\n\n\n06/22/12 17:57:41 Using config source: /etc/condor/condor_config\n\n\n06/22/12 17:57:41 Using local config sources: \n\n\n06/22/12 17:57:41    /etc/condor/config.d/00-chtc-global.conf\n\n\n06/22/12 17:57:41    /etc/condor/config.d/01-chtc-submit.conf\n\n\n06/22/12 17:57:41    /etc/condor/config.d/02-chtc-flocking.conf\n\n\n06/22/12 17:57:41    /etc/condor/config.d/03-chtc-jobrouter.conf\n\n\n06/22/12 17:57:41    /etc/condor/config.d/04-chtc-blacklist.conf\n\n\n06/22/12 17:57:41    /etc/condor/config.d/99-osg-ss-group.conf\n\n\n06/22/12 17:57:41    /etc/condor/config.d/99-roy-extras.conf\n\n\n06/22/12 17:57:41    /etc/condor/condor_config.local\n\n\n\n\n\n\nBelow is where DAGMan realizes that the montage node failed:\n\n\n06/22/12 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0)\n\n\n06/22/12 18:08:42 Number of idle job procs: 0\n\n\n06/22/12 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0)\n\n\n06/22/12 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0)\n\n\n06/22/12 18:08:42 Node montage job proc (82.0.0) failed with status 1.\n\n\n06/22/12 18:08:42 Number of idle job procs: 0\n\n\n06/22/12 18:08:42 Of 5 nodes total:\n\n\n06/22/12 18:08:42  Done     Pre   Queued    Post   Ready   Un-Ready   Failed\n\n\n06/22/12 18:08:42   ===     ===      ===     ===     ===        ===      ===\n\n\n06/22/12 18:08:42     4       0        0       0       0          0        1\n\n\n06/22/12 18:08:42 0 job proc(s) currently held\n\n\n06/22/12 18:08:42 Aborting DAG...\n\n\n06/22/12 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001...\n\n\n06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0)\n\n\n06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0)\n\n\n06/22/12 18:08:42 Note: 0 total job deferrals because of node category throttles\n\n\n06/22/12 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0)\n\n\n06/22/12 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0)\n\n\n06/22/12 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1\n\n\n\n\n\n\nDAGMan notices that one of the jobs failed because it's exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the resuce DAG?\n\n\nLook at the rescue DAG file. It's called a partial DAG because it indicates what part of the DAG has already been completed.\n\n\nusername@learn $\n cat goatbrot.dag.rescue001\n\n#\n Rescue DAG file, created after running\n\n#\n   the goatbrot.dag DAG file\n\n#\n Created \n6\n/22/2012 \n23\n:08:42 UTC\n\n#\n Rescue DAG version: \n2\n.0.1 \n(\npartial\n)\n\n\n#\n\n\n#\n Total number of Nodes: \n5\n\n\n#\n Nodes premarked DONE: \n4\n\n\n#\n Nodes that failed: \n1\n\n\n#\n   montage,<ENDLIST>\n\n\nDONE g1\n\n\nDONE g2\n\n\nDONE g3\n\n\nDONE g4\n\n\n\n\n\n\nFrom the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending \n-h\n argument. Change montage.sub to look like:\n\n\nexecutable              = /usr/bin/montage\narguments               = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg\ntransfer_input_files    = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm\noutput                  = montage.out\nerror                   = montage.err\nlog                     = montage.log\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nrequirements            = OpSysMajorVer =?= 6\nqueue\n\n\n\n\n\nNow we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG. If you didn't fix the problem, DAGMan would generate another rescue DAG.\n\n\nusername@learn $\n condor_submit_dag goatbrot.dag\n\nRunning rescue DAG 1\n\n\n-----------------------------------------------------------------------\n\n\nFile for submitting this DAG to Condor           : goatbrot.dag.condor.sub\n\n\nLog of DAGMan debugging messages                 : goatbrot.dag.dagman.out\n\n\nLog of Condor library output                     : goatbrot.dag.lib.out\n\n\nLog of Condor library error messages             : goatbrot.dag.lib.err\n\n\nLog of the life of condor_dagman itself          : goatbrot.dag.dagman.log\n\n\n\nSubmitting job(s).\n\n\n1 job(s) submitted to cluster 83.\n\n\n-----------------------------------------------------------------------\n\n\n\nusername@learn $\n tail -f goatbrot.dag.dagman.out\n\n06/23/12 11:30:53 ******************************************************\n\n\n06/23/12 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP\n\n\n06/23/12 11:30:53 ** /usr/bin/condor_dagman\n\n\n06/23/12 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)\n\n\n06/23/12 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON\n\n\n06/23/12 11:30:53 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\n\n\n06/23/12 11:30:53 ** $CondorPlatform: x86_64_rhap_5.7 $\n\n\n06/23/12 11:30:53 ** PID = 28576\n\n\n06/23/12 11:30:53 ** Log last touched 6/22 18:08:42\n\n\n06/23/12 11:30:53 ******************************************************\n\n\n06/23/12 11:30:53 Using config source: /etc/condor/condor_config\n\n\n...\n\n\n\n\n\n\nHere is where DAGMAN notices that there is a rescue DAG\n\n\n06/23/12 11:30:53 Parsing 1 dagfiles\n\n\n06/23/12 11:30:53 Parsing goatbrot.dag ...\n\n\n%\nRED%06/23/12 \n11\n:30:53 Found rescue DAG number \n1\n;\n running goatbrot.dag.rescue001 in combination with normal DAG file\n\n\n06/23/12 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n06/23/12 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001\n\n\n06/23/12 11:30:53 Dag contains 5 total jobs\n\n\n\n\n\n\nShortly thereafter it sees that four jobs have already finished.\n\n\n06/23/12 11:31:05 Bootstrapping...\n\n\n06/23/12 11:31:05 Number of pre-completed nodes: 4\n\n\n06/23/12 11:31:05 Registering condor_event_timer...\n\n\n06/23/12 11:31:06 Sleeping for one second for log file consistency\n\n\n06/23/12 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log\n\n\n\n\n\n\nHere is where DAGMan resubmits the montage job and waits for it to complete.\n\n\n06/23/12 11:31:07 Submitting Condor Node montage job(s)...\n\n\n06/23/12 11:31:07 submitting: condor_submit \n\n\n      -a dag_node_name' '=' 'montage \n\n\n      -a +DAGManJobId' '=' '83 \n\n\n      -a DAGManJobId' '=' '83 \n\n\n      -a submit_event_notes' '=' 'DAG' 'Node:' 'montage \n\n\n      -a DAG_STATUS' '=' '0 \n\n\n      -a FAILED_COUNT' '=' '0 \n\n\n      -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\" \n\n\n      montage.sub\n\n\n06/23/12 11:31:07 From submit: Submitting job(s).\n\n\n06/23/12 11:31:07 From submit: 1 job(s) submitted to cluster 84.\n\n\n06/23/12 11:31:07   assigned Condor ID (84.0.0)\n\n\n06/23/12 11:31:07 Just submitted 1 job this cycle...\n\n\n06/23/12 11:31:07 Currently monitoring 1 Condor log file(s)\n\n\n06/23/12 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0)\n\n\n06/23/12 11:31:07 Number of idle job procs: 1\n\n\n06/23/12 11:31:07 Of 5 nodes total:\n\n\n06/23/12 11:31:07  Done     Pre   Queued    Post   Ready   Un-Ready   Failed\n\n\n06/23/12 11:31:07   ===     ===      ===     ===     ===        ===      ===\n\n\n06/23/12 11:31:07     4       0        1       0       0          0        0\n\n\n06/23/12 11:31:07 0 job proc(s) currently held\n\n\n06/23/12 11:40:22 Currently monitoring 1 Condor log file(s)\n\n\n06/23/12 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0)\n\n\n06/23/12 11:40:22 Number of idle job procs: 0\n\n\n06/23/12 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0)\n\n\n06/23/12 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0)\n\n\n\n\n\n\nThis is where the montage finished.\n\n\n06/23/12 11:40:22 Node montage job proc (84.0.0) completed successfully.\n\n\n06/23/12 11:40:22 Node montage job completed\n\n\n06/23/12 11:40:22 Number of idle job procs: 0\n\n\n06/23/12 11:40:22 Of 5 nodes total:\n\n\n06/23/12 11:40:22  Done     Pre   Queued    Post   Ready   Un-Ready   Failed\n\n\n06/23/12 11:40:22   ===     ===      ===     ===     ===        ===      ===\n\n\n06/23/12 11:40:22     5       0        0       0       0          0        0\n\n\n06/23/12 11:40:22 0 job proc(s) currently held\n\n\n\n\n\n\nAnd here DAGMan decides that the work is all done.\n\n\n06/23/12 11:40:22 All jobs Completed!\n\n\n06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0)\n\n\n06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0)\n\n\n06/23/12 11:40:22 Note: 0 total job deferrals because of node category throttles\n\n\n06/23/12 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0)\n\n\n06/23/12 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0)\n\n\n06/23/12 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0\n\n\n\n\n\n\nSuccess! Now go ahead and clean up.\n\n\nBonus Challenge\n\u00b6\n\n\nIf you have time, add an extra node to the DAG. Copy our original \"simple\" program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure.\n\n\nWrite a POST script that checks the return value. Check \nthe HTCondor manual\n to see how to describe your post script.",
            "title": "Exercise 4.4"
        },
        {
            "location": "/materials/day4/part4-ex4-failed-dag/#monday-exercise-44-handling-a-dag-that-fails",
            "text": "The objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures.",
            "title": "Monday Exercise 4.4: Handling a DAG That Fails"
        },
        {
            "location": "/materials/day4/part4-ex4-failed-dag/#background",
            "text": "DAGMan can handle a situation where some of the nodes in a DAG fail. DAGMan will run as many nodes as possible, then create a rescue DAG making it easy to continue when the problem is fixed.",
            "title": "Background"
        },
        {
            "location": "/materials/day4/part4-ex4-failed-dag/#breaking-things",
            "text": "Recall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a  -h  to the arguments. It will look like this (the change is highlighted in red):  executable              = /usr/bin/montage\narguments               =  -h  tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg\ntransfer_input_files    = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm\noutput                  = montage.out\nerror                   = montage.err\nlog                     = montage.log\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nrequirements            = OpSysMajorVer =?= 6\nqueue  Submit the DAG again:  username@learn $  condor_submit_dag goatbrot.dag -----------------------------------------------------------------------  File for submitting this DAG to Condor           : goatbrot.dag.condor.sub  Log of DAGMan debugging messages                 : goatbrot.dag.dagman.out  Log of Condor library output                     : goatbrot.dag.lib.out  Log of Condor library error messages             : goatbrot.dag.lib.err  Log of the life of condor_dagman itself          : goatbrot.dag.dagman.log  Submitting job(s).  1 job(s) submitted to cluster 77.  -----------------------------------------------------------------------   Use watch to watch the jobs until they finish. In a separate window, use  tail --lines=500 -f goatbrot.dag.dagman.out  to watch what DAGMan does.  06/22/12 17:57:41 Setting maximum accepts per cycle 8.  06/22/12 17:57:41 ******************************************************  06/22/12 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP  06/22/12 17:57:41 ** /usr/bin/condor_dagman  06/22/12 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)  06/22/12 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON  06/22/12 17:57:41 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $  06/22/12 17:57:41 ** $CondorPlatform: x86_64_rhap_5.7 $  06/22/12 17:57:41 ** PID = 26867  06/22/12 17:57:41 ** Log last touched time unavailable (No such file or directory)  06/22/12 17:57:41 ******************************************************  06/22/12 17:57:41 Using config source: /etc/condor/condor_config  06/22/12 17:57:41 Using local config sources:   06/22/12 17:57:41    /etc/condor/config.d/00-chtc-global.conf  06/22/12 17:57:41    /etc/condor/config.d/01-chtc-submit.conf  06/22/12 17:57:41    /etc/condor/config.d/02-chtc-flocking.conf  06/22/12 17:57:41    /etc/condor/config.d/03-chtc-jobrouter.conf  06/22/12 17:57:41    /etc/condor/config.d/04-chtc-blacklist.conf  06/22/12 17:57:41    /etc/condor/config.d/99-osg-ss-group.conf  06/22/12 17:57:41    /etc/condor/config.d/99-roy-extras.conf  06/22/12 17:57:41    /etc/condor/condor_config.local   Below is where DAGMan realizes that the montage node failed:  06/22/12 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0)  06/22/12 18:08:42 Number of idle job procs: 0  06/22/12 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0)  06/22/12 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0)  06/22/12 18:08:42 Node montage job proc (82.0.0) failed with status 1.  06/22/12 18:08:42 Number of idle job procs: 0  06/22/12 18:08:42 Of 5 nodes total:  06/22/12 18:08:42  Done     Pre   Queued    Post   Ready   Un-Ready   Failed  06/22/12 18:08:42   ===     ===      ===     ===     ===        ===      ===  06/22/12 18:08:42     4       0        0       0       0          0        1  06/22/12 18:08:42 0 job proc(s) currently held  06/22/12 18:08:42 Aborting DAG...  06/22/12 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001...  06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0)  06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0)  06/22/12 18:08:42 Note: 0 total job deferrals because of node category throttles  06/22/12 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0)  06/22/12 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0)  06/22/12 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1   DAGMan notices that one of the jobs failed because it's exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the resuce DAG?  Look at the rescue DAG file. It's called a partial DAG because it indicates what part of the DAG has already been completed.  username@learn $  cat goatbrot.dag.rescue001 #  Rescue DAG file, created after running #    the goatbrot.dag DAG file #  Created  6 /22/2012  23 :08:42 UTC #  Rescue DAG version:  2 .0.1  ( partial )  #  #  Total number of Nodes:  5  #  Nodes premarked DONE:  4  #  Nodes that failed:  1  #    montage,<ENDLIST> DONE g1  DONE g2  DONE g3  DONE g4   From the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending  -h  argument. Change montage.sub to look like:  executable              = /usr/bin/montage\narguments               = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg\ntransfer_input_files    = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm\noutput                  = montage.out\nerror                   = montage.err\nlog                     = montage.log\nrequest_memory          = 1GB\nrequest_disk            = 1GB\nrequest_cpus            = 1\nrequirements            = OpSysMajorVer =?= 6\nqueue  Now we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG. If you didn't fix the problem, DAGMan would generate another rescue DAG.  username@learn $  condor_submit_dag goatbrot.dag Running rescue DAG 1  -----------------------------------------------------------------------  File for submitting this DAG to Condor           : goatbrot.dag.condor.sub  Log of DAGMan debugging messages                 : goatbrot.dag.dagman.out  Log of Condor library output                     : goatbrot.dag.lib.out  Log of Condor library error messages             : goatbrot.dag.lib.err  Log of the life of condor_dagman itself          : goatbrot.dag.dagman.log  Submitting job(s).  1 job(s) submitted to cluster 83.  -----------------------------------------------------------------------  username@learn $  tail -f goatbrot.dag.dagman.out 06/23/12 11:30:53 ******************************************************  06/23/12 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP  06/23/12 11:30:53 ** /usr/bin/condor_dagman  06/23/12 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)  06/23/12 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON  06/23/12 11:30:53 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $  06/23/12 11:30:53 ** $CondorPlatform: x86_64_rhap_5.7 $  06/23/12 11:30:53 ** PID = 28576  06/23/12 11:30:53 ** Log last touched 6/22 18:08:42  06/23/12 11:30:53 ******************************************************  06/23/12 11:30:53 Using config source: /etc/condor/condor_config  ...   Here is where DAGMAN notices that there is a rescue DAG  06/23/12 11:30:53 Parsing 1 dagfiles  06/23/12 11:30:53 Parsing goatbrot.dag ...  % RED%06/23/12  11 :30:53 Found rescue DAG number  1 ;  running goatbrot.dag.rescue001 in combination with normal DAG file  06/23/12 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  06/23/12 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001  06/23/12 11:30:53 Dag contains 5 total jobs   Shortly thereafter it sees that four jobs have already finished.  06/23/12 11:31:05 Bootstrapping...  06/23/12 11:31:05 Number of pre-completed nodes: 4  06/23/12 11:31:05 Registering condor_event_timer...  06/23/12 11:31:06 Sleeping for one second for log file consistency  06/23/12 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log   Here is where DAGMan resubmits the montage job and waits for it to complete.  06/23/12 11:31:07 Submitting Condor Node montage job(s)...  06/23/12 11:31:07 submitting: condor_submit         -a dag_node_name' '=' 'montage         -a +DAGManJobId' '=' '83         -a DAGManJobId' '=' '83         -a submit_event_notes' '=' 'DAG' 'Node:' 'montage         -a DAG_STATUS' '=' '0         -a FAILED_COUNT' '=' '0         -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\"         montage.sub  06/23/12 11:31:07 From submit: Submitting job(s).  06/23/12 11:31:07 From submit: 1 job(s) submitted to cluster 84.  06/23/12 11:31:07   assigned Condor ID (84.0.0)  06/23/12 11:31:07 Just submitted 1 job this cycle...  06/23/12 11:31:07 Currently monitoring 1 Condor log file(s)  06/23/12 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0)  06/23/12 11:31:07 Number of idle job procs: 1  06/23/12 11:31:07 Of 5 nodes total:  06/23/12 11:31:07  Done     Pre   Queued    Post   Ready   Un-Ready   Failed  06/23/12 11:31:07   ===     ===      ===     ===     ===        ===      ===  06/23/12 11:31:07     4       0        1       0       0          0        0  06/23/12 11:31:07 0 job proc(s) currently held  06/23/12 11:40:22 Currently monitoring 1 Condor log file(s)  06/23/12 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0)  06/23/12 11:40:22 Number of idle job procs: 0  06/23/12 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0)  06/23/12 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0)   This is where the montage finished.  06/23/12 11:40:22 Node montage job proc (84.0.0) completed successfully.  06/23/12 11:40:22 Node montage job completed  06/23/12 11:40:22 Number of idle job procs: 0  06/23/12 11:40:22 Of 5 nodes total:  06/23/12 11:40:22  Done     Pre   Queued    Post   Ready   Un-Ready   Failed  06/23/12 11:40:22   ===     ===      ===     ===     ===        ===      ===  06/23/12 11:40:22     5       0        0       0       0          0        0  06/23/12 11:40:22 0 job proc(s) currently held   And here DAGMan decides that the work is all done.  06/23/12 11:40:22 All jobs Completed!  06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0)  06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0)  06/23/12 11:40:22 Note: 0 total job deferrals because of node category throttles  06/23/12 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0)  06/23/12 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0)  06/23/12 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0   Success! Now go ahead and clean up.",
            "title": "Breaking Things"
        },
        {
            "location": "/materials/day4/part4-ex4-failed-dag/#bonus-challenge",
            "text": "If you have time, add an extra node to the DAG. Copy our original \"simple\" program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure.  Write a POST script that checks the return value. Check  the HTCondor manual  to see how to describe your post script.",
            "title": "Bonus Challenge"
        },
        {
            "location": "/materials/day4/part4-ex5-challenges/",
            "text": "pre em { font-style: normal; background-color: yellow; }\n  pre strong { font-style: normal; font-weight: bold; color: \\#008; }\n\n\n\n\nMonday Bonus Exercise 4.5: YOUR Jobs and More on Workflows\n\u00b6\n\n\nThe objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job.\n\n\nChallenge 1\n\u00b6\n\n\nDo you have any extra computation that needs to be done? Real work, from your life outside this summer school? If so, try it out on our HTCondor pool. Can't think of something? How about one of the existing distributed computing programs like \ndistributed.net\n, \nSETI@home\n, \nEinstien@Home\n or others that you know. We prefer that you do your own work rather than one of these projects, but they are options.\n\n\nChallenge 2\n\u00b6\n\n\nTry to generate other Mandelbrot images. Some possible locations to look at with goatbroat:\n\n\ngoatbrot -i 1000 -o ex1.ppm -c 0.0016437219722,-0.8224676332988 -w 2e-11 -s 1000,1000\n\n\ngoatbrot -i 1000 -o ex2.ppm -c 0.3958608398437499,-0.13431445312500012 -w 0.0002197265625 -s 1000,1000\n\n\ngoatbrot -i 1000 -o ex3.ppm -c 0.3965859374999999,-0.13378125000000013 -w 0.003515625  -s 1000,1000\n\n\n\n\n\n\nYou can convert ppm files with \nconvert\n, like so:\n\n\nconvert ex1.ppm ex1.jpg\n\n\n\n\n\n\nNow make a movie! Make a series of images where you zoom into a point in the Mandelbrot set gradually. (Those points above may work well.) Assemble these images with the \"convert\" tool which will let you convert a set of JPEG files into an MPEG movie.\n\n\nChallenge 3\n\u00b6\n\n\nTry out Pegasus. Pegasus is a workflow manager that uses DAGMan and can work in a grid environment and/or run across different types of clusters (with other queueing software). It will create the DAGs from abstract DAG descriptions and ensure they are appropriate for the location of the data and computation.\n\n\nLinks to more information:\n\n\n\n\nPegasus Website\n\n\nPegasus Documentation\n\n\nPegasus on OSG Connect (covered Thursday)\n\n\n\n\nIf you have any questions or problems, please feel free to contact the Pegasus team by emailing \npegasus-support@isi.edu\n\n\nNote: Be Nice\n\u00b6\n\n\nPlease be polite. Computers in our HTCondor pool will run multiple jobs at a time, and these computers are shared with your fellow students. If you have jobs that will use significant computational power or memory, limit your jobs to be kind to your neighbors, unless you run your jobs during off-hours.",
            "title": "Bonus Exercises 4.5"
        },
        {
            "location": "/materials/day4/part4-ex5-challenges/#monday-bonus-exercise-45-your-jobs-and-more-on-workflows",
            "text": "The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job.",
            "title": "Monday Bonus Exercise 4.5: YOUR Jobs and More on Workflows"
        },
        {
            "location": "/materials/day4/part4-ex5-challenges/#challenge-1",
            "text": "Do you have any extra computation that needs to be done? Real work, from your life outside this summer school? If so, try it out on our HTCondor pool. Can't think of something? How about one of the existing distributed computing programs like  distributed.net ,  SETI@home ,  Einstien@Home  or others that you know. We prefer that you do your own work rather than one of these projects, but they are options.",
            "title": "Challenge 1"
        },
        {
            "location": "/materials/day4/part4-ex5-challenges/#challenge-2",
            "text": "Try to generate other Mandelbrot images. Some possible locations to look at with goatbroat:  goatbrot -i 1000 -o ex1.ppm -c 0.0016437219722,-0.8224676332988 -w 2e-11 -s 1000,1000  goatbrot -i 1000 -o ex2.ppm -c 0.3958608398437499,-0.13431445312500012 -w 0.0002197265625 -s 1000,1000  goatbrot -i 1000 -o ex3.ppm -c 0.3965859374999999,-0.13378125000000013 -w 0.003515625  -s 1000,1000   You can convert ppm files with  convert , like so:  convert ex1.ppm ex1.jpg   Now make a movie! Make a series of images where you zoom into a point in the Mandelbrot set gradually. (Those points above may work well.) Assemble these images with the \"convert\" tool which will let you convert a set of JPEG files into an MPEG movie.",
            "title": "Challenge 2"
        },
        {
            "location": "/materials/day4/part4-ex5-challenges/#challenge-3",
            "text": "Try out Pegasus. Pegasus is a workflow manager that uses DAGMan and can work in a grid environment and/or run across different types of clusters (with other queueing software). It will create the DAGs from abstract DAG descriptions and ensure they are appropriate for the location of the data and computation.  Links to more information:   Pegasus Website  Pegasus Documentation  Pegasus on OSG Connect (covered Thursday)   If you have any questions or problems, please feel free to contact the Pegasus team by emailing  pegasus-support@isi.edu",
            "title": "Challenge 3"
        },
        {
            "location": "/materials/day4/part4-ex5-challenges/#note-be-nice",
            "text": "Please be polite. Computers in our HTCondor pool will run multiple jobs at a time, and these computers are shared with your fellow students. If you have jobs that will use significant computational power or memory, limit your jobs to be kind to your neighbors, unless you run your jobs during off-hours.",
            "title": "Note: Be Nice"
        },
        {
            "location": "/materials/day5/part1-ex1-science-intro/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nFriday Exercise 1.1: Learn about Joe's Desired Computing Work\n\u00b6\n\n\nIn today's set of hands-on exercises, you'll be examining a set of pre-existing job submissions (scripts and submit files) and then coordinating them to run as a single workflow using a DAG. Along the way, you'll need to test some of the already written submit files. The first part of this exercise introduces the overall steps for the workflow and what pieces already exist (that you won't need to write yourself).\n\n\nJoe's Science and Workflow\n\u00b6\n\n\nJoe Biologist studies the influence of genetic differences (genotypes) on the growth traits (phenotypes) of a common crop. He has come to you with some computing work he\u2019d like to automate as a workflow, so that he can scale out his research and run it on a much larger HTC compute system. \n\n\nJoe starts with a spreadsheet containing phenotype measurements and genotype information for hundreds of plants.  Each phenotype measurement corresponds to a trait that he's interested in learning about.  In order to understand the relationship of these trait phenotypes to the underlying genetic material (the genotypes), he first generates thousands of combinations of the trait's phenotype with all the genotypes.  These \npermutations\n represent hypothetical phenotype-genotype connections that could be meaningful for that trait.  Each of these permutations is then run through a \nQTL mapping\n process, which sifts through all the permutations to find the genotypes that are most important for the trait in question.  Joe has to repeat this process (generating genotype-phenotype permutations and then running a QTL mapping on those permutations) separately for each trait he's studying.  \n\n\nJoe's Current Setup\n\u00b6\n\n\nJoe has run some of this work on a high throughput system -- submitting individual jobs on a small, dedicated HTCondor cluster where he doesn\u2019t have to request CPU, disk, or RAM.  \n\n\n(As you learn more details below, it might be helpful to draw a diagram that describes the steps of his workflow.)\n\n\nSpecifically, for one trait, his workflow looks like this: \n\n\n\n\nUploading an input file (\ninput.csv\n) to the submit server that has \nall\n of the genotypic and phenotypic information for multiple traits.  \n\n\nJoe then uses a submit file, perl wrapper script (\nrunR.pl\n) and custom R script (\nrun_perm.R\n) to submit jobs that generate the permutations for a single trait.  The perl script is the job's executable.  Besides setting up the job environment, it takes several arguments: \n\n\nThe name of the R script that creates the permutations\n\n\nWhich trait (and therefore which phenotype) it's using to create permutations\n\n\nHow many permutations to create per job\n\n\nWhen running multiple permutation jobs at once for a trait, a number to identify the job. \n\n\n\n\n\n\nAfter the permutation job(s), a shell script creates a tarball of the permutation files for that trait (which may have been generated by multiple jobs).  This is a short-running script that runs directly on the submit server.  \n\n\nThe second job (the QTL mapping step) uses the tarball of trait permutations from the previous step, the same perl wrapper script (\nrunR.pl\n), and a different R script (\nrun_qtl.R\n) to run the QTL mapping on the permutations generated in the first batch of jobs.  \n\n\nFinally, a second shell script creates a tarball of the QTL mapping output on the submit server.   \n\n\n\n\nHe has to perform all of these steps for each trait he wants to analyze.  \n\n\nThat's a lot of steps!  Joe Biologist is getting annoyed by how many manual job submissions and summary scripts he has to run, especially as he's thinking of scaling out his work in a way that would be much more cumbersome.  As one example, when Joe has previously run the \npermutation\n step (generating 10,000 permutations for a single trait) as one Condor job on his own cluster, the job runs for hours.  To solve that problem in the past, he split the single permutation-creating Condor job into multiple, shorter Condor jobs, each creating fewer permutations, but he doesn\u2019t remember the details of how many jobs and how many permutations per job worked well. Furthermore, Joe would like to scale up to 100,000 permutations (from 10,000) per trait this time.   Unlike the \npermutation\n step, the \nQTL\n step finishes quite quickly; even with 10,000 permutations completed for each trait, the \nQTL\n step completes within several minutes.\n\n\nJoe knows that he needs to modify and optimize his submissions to run jobs that create more permutations and that DAGMan might help him automate all these steps.  He's never used DAGMan, so he's asking you to help him organize all the steps into an optimized DAG workflow.\n\n\nView Joe's files\n\u00b6\n\n\n(but don't do anything with them until the next \nexercise\n, where you'll plan necessary developments of a workflow for Joe.)\n\n\nLog in to \nlearn.chtc.wisc.edu\n and move to a desired location in your home directory. Enter the following commands to download and decompress/untar Joe\u2019s job ingredients:\n\n\nusername@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/WorkflowExercise.tar.gz\n\nusername@learn $\n tar -xzf WorkflowExercise.tar.gz\n\n\n\n\n\nYou can now navigate into the \nWorkflowExercise\n directory to view the full ingredients for Joe\u2019s Computing work. Review all of these files based upon the information below and refer back to it as you proceed through the remaining exercises (1.2, 2.1, 2.2). (If you've been drawing a diagram of Joe's work so far, feel free to annotate with some of this information.)\n\n\nBased upon Joe\u2019s description and submit files you determine the following details for analyzing \ntrait 1\n: \n\n\nPermutation Step\n\u00b6\n\n\nThe command the job should run: \n./runR.pl 1_$(Process) run_perm.R 1 $(Process) 10000\n\n\nThe submit file for the permutation jobs is \npermutation1.submit\n, including the following: \n\n\nexecutable = runR.pl\narguments = 1_$(Process) run_perm.R 1 $(Process) 10000\ntransfer_input_files = run_perm.R, input.csv, RLIBS.tar.gz\n\n\n\n\n\nThe arguments mean the following:\n\n\n\n\n1_$(Process)\n, used by \nrunR.pl\n to name its log file\n\n\nrun_perm.R\n, which R script we want to run\n\n\n1\n, trait column in \ninput.csv\n\n\n$(Process)\n, used by \nrun_perm.R\n for naming the permutation output\n\n\n10000\n, indicates that 10,000 permutations should be done for each individual job process.  If we use this submit file to create multiple jobs, each of them will create 10,000 permutations.  \n\n\n\n\nAs output, this job will create:\n\n\n\n\na series of .log and .out files, named from the first argument (\n1_$(Process)\n). This looks like: \nrunR.1_0.out\n, \nrunR.1_0.log\n, \nrunR.1_0.err\n\n\nthe actual output file, named using the \n1\n and \n$(Process)\n arguments, like so: \nperm_part.1_0.Rdat\n\n\n\n\nCombine Permutation Output\n\u00b6\n\n\ntarit.sh\n is run after the trait's \npermutation\n step with the column number as an argument to compress \nperm_part.1_0.Rdat\n (or potentially multiple such files named according to \nperm_part.1_*.Rdat\n, see above) for the QTL step.\n\n\nSample execution: \n\n\nusername@host $\n ./tarit.sh \n1\n\n\n\n\n\n\nwhere \n1\n is the trait column number.\n\n\nQTL Mapping Step\n\u00b6\n\n\nThe command the job should run: \n./runR.pl qtl_1 qtl.R 1\n\n\nThe submit file for the permutation jobs is \nqtl1.submit\n, containing the following: \n\n\nexecutable = runR.pl\narguments qtl_1 qtl.R 1\ntransfer_input_files = qtl.R, input.csv, RLIBS.tar.gz, perm_combined_1.tar.gz\n\n\n\n\n\nNote that \nperm_combined_1.tar.gz\n was created by \ntarit.sh\n.\n\n\nThe arguments mean the following:\n\n\n\n\nqtl_1\n: used by \nrunR.pl\n to name its log / output / error files\n\n\nqtl.R\n: which R script we want to run\n\n\n1\n: trait column in \ninput.csv\n, will also be used to name the output files\n\n\n\n\nAs output, this job will create:\n\n\n\n\na series of .log and .out files, named from the first argument (\nqtl_1\n).\n\n\nseveral output files named for the trait argument \n1\n: \nperm_combined_1.Rdat\n, \nperm_summary_1.txt\n, \nsim_geno_results_1.Rdat\n, \nqtl_1.Rdat\n, \nrefined_qtl_summary_1.txt\n, \nrefined qtl_1.Rdat\n, and \nfit_qtl_results_1.Rdat\n\n\n\n\nCombine All Output\n\u00b6\n\n\nresults_1.tar.gz\n is made by running \ntaritall.sh\n with the column number as an argument after the QTL step for that trait finishes, where \"1\" reflects the trait column number in the output above.\n\n\nSample execution: \n\n\nusername@host $\n ./taritall.sh \n1\n\n\n\n\n\n\nwhere \n1\n is the trait column number.\n\n\nOther Files\n\u00b6\n\n\nThe other files in the workflow directory (\npermutation2.submit\n/\nqtl2.submit\n and \npermutation3.submit\n/\nqtl3.submit\n) are used to submit the \npermutation\n and \nqtl\n jobs for traits 2 and 3, respectively.",
            "title": "Exercise 1.1"
        },
        {
            "location": "/materials/day5/part1-ex1-science-intro/#friday-exercise-11-learn-about-joes-desired-computing-work",
            "text": "In today's set of hands-on exercises, you'll be examining a set of pre-existing job submissions (scripts and submit files) and then coordinating them to run as a single workflow using a DAG. Along the way, you'll need to test some of the already written submit files. The first part of this exercise introduces the overall steps for the workflow and what pieces already exist (that you won't need to write yourself).",
            "title": "Friday Exercise 1.1: Learn about Joe's Desired Computing Work"
        },
        {
            "location": "/materials/day5/part1-ex1-science-intro/#joes-science-and-workflow",
            "text": "Joe Biologist studies the influence of genetic differences (genotypes) on the growth traits (phenotypes) of a common crop. He has come to you with some computing work he\u2019d like to automate as a workflow, so that he can scale out his research and run it on a much larger HTC compute system.   Joe starts with a spreadsheet containing phenotype measurements and genotype information for hundreds of plants.  Each phenotype measurement corresponds to a trait that he's interested in learning about.  In order to understand the relationship of these trait phenotypes to the underlying genetic material (the genotypes), he first generates thousands of combinations of the trait's phenotype with all the genotypes.  These  permutations  represent hypothetical phenotype-genotype connections that could be meaningful for that trait.  Each of these permutations is then run through a  QTL mapping  process, which sifts through all the permutations to find the genotypes that are most important for the trait in question.  Joe has to repeat this process (generating genotype-phenotype permutations and then running a QTL mapping on those permutations) separately for each trait he's studying.",
            "title": "Joe's Science and Workflow"
        },
        {
            "location": "/materials/day5/part1-ex1-science-intro/#joes-current-setup",
            "text": "Joe has run some of this work on a high throughput system -- submitting individual jobs on a small, dedicated HTCondor cluster where he doesn\u2019t have to request CPU, disk, or RAM.    (As you learn more details below, it might be helpful to draw a diagram that describes the steps of his workflow.)  Specifically, for one trait, his workflow looks like this:    Uploading an input file ( input.csv ) to the submit server that has  all  of the genotypic and phenotypic information for multiple traits.    Joe then uses a submit file, perl wrapper script ( runR.pl ) and custom R script ( run_perm.R ) to submit jobs that generate the permutations for a single trait.  The perl script is the job's executable.  Besides setting up the job environment, it takes several arguments:   The name of the R script that creates the permutations  Which trait (and therefore which phenotype) it's using to create permutations  How many permutations to create per job  When running multiple permutation jobs at once for a trait, a number to identify the job.     After the permutation job(s), a shell script creates a tarball of the permutation files for that trait (which may have been generated by multiple jobs).  This is a short-running script that runs directly on the submit server.    The second job (the QTL mapping step) uses the tarball of trait permutations from the previous step, the same perl wrapper script ( runR.pl ), and a different R script ( run_qtl.R ) to run the QTL mapping on the permutations generated in the first batch of jobs.    Finally, a second shell script creates a tarball of the QTL mapping output on the submit server.      He has to perform all of these steps for each trait he wants to analyze.    That's a lot of steps!  Joe Biologist is getting annoyed by how many manual job submissions and summary scripts he has to run, especially as he's thinking of scaling out his work in a way that would be much more cumbersome.  As one example, when Joe has previously run the  permutation  step (generating 10,000 permutations for a single trait) as one Condor job on his own cluster, the job runs for hours.  To solve that problem in the past, he split the single permutation-creating Condor job into multiple, shorter Condor jobs, each creating fewer permutations, but he doesn\u2019t remember the details of how many jobs and how many permutations per job worked well. Furthermore, Joe would like to scale up to 100,000 permutations (from 10,000) per trait this time.   Unlike the  permutation  step, the  QTL  step finishes quite quickly; even with 10,000 permutations completed for each trait, the  QTL  step completes within several minutes.  Joe knows that he needs to modify and optimize his submissions to run jobs that create more permutations and that DAGMan might help him automate all these steps.  He's never used DAGMan, so he's asking you to help him organize all the steps into an optimized DAG workflow.",
            "title": "Joe's Current Setup"
        },
        {
            "location": "/materials/day5/part1-ex1-science-intro/#view-joes-files",
            "text": "(but don't do anything with them until the next  exercise , where you'll plan necessary developments of a workflow for Joe.)  Log in to  learn.chtc.wisc.edu  and move to a desired location in your home directory. Enter the following commands to download and decompress/untar Joe\u2019s job ingredients:  username@learn $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool19/WorkflowExercise.tar.gz username@learn $  tar -xzf WorkflowExercise.tar.gz  You can now navigate into the  WorkflowExercise  directory to view the full ingredients for Joe\u2019s Computing work. Review all of these files based upon the information below and refer back to it as you proceed through the remaining exercises (1.2, 2.1, 2.2). (If you've been drawing a diagram of Joe's work so far, feel free to annotate with some of this information.)  Based upon Joe\u2019s description and submit files you determine the following details for analyzing  trait 1 :",
            "title": "View Joe's files"
        },
        {
            "location": "/materials/day5/part1-ex1-science-intro/#permutation-step",
            "text": "The command the job should run:  ./runR.pl 1_$(Process) run_perm.R 1 $(Process) 10000  The submit file for the permutation jobs is  permutation1.submit , including the following:   executable = runR.pl\narguments = 1_$(Process) run_perm.R 1 $(Process) 10000\ntransfer_input_files = run_perm.R, input.csv, RLIBS.tar.gz  The arguments mean the following:   1_$(Process) , used by  runR.pl  to name its log file  run_perm.R , which R script we want to run  1 , trait column in  input.csv  $(Process) , used by  run_perm.R  for naming the permutation output  10000 , indicates that 10,000 permutations should be done for each individual job process.  If we use this submit file to create multiple jobs, each of them will create 10,000 permutations.     As output, this job will create:   a series of .log and .out files, named from the first argument ( 1_$(Process) ). This looks like:  runR.1_0.out ,  runR.1_0.log ,  runR.1_0.err  the actual output file, named using the  1  and  $(Process)  arguments, like so:  perm_part.1_0.Rdat",
            "title": "Permutation Step"
        },
        {
            "location": "/materials/day5/part1-ex1-science-intro/#combine-permutation-output",
            "text": "tarit.sh  is run after the trait's  permutation  step with the column number as an argument to compress  perm_part.1_0.Rdat  (or potentially multiple such files named according to  perm_part.1_*.Rdat , see above) for the QTL step.  Sample execution:   username@host $  ./tarit.sh  1   where  1  is the trait column number.",
            "title": "Combine Permutation Output"
        },
        {
            "location": "/materials/day5/part1-ex1-science-intro/#qtl-mapping-step",
            "text": "The command the job should run:  ./runR.pl qtl_1 qtl.R 1  The submit file for the permutation jobs is  qtl1.submit , containing the following:   executable = runR.pl\narguments qtl_1 qtl.R 1\ntransfer_input_files = qtl.R, input.csv, RLIBS.tar.gz, perm_combined_1.tar.gz  Note that  perm_combined_1.tar.gz  was created by  tarit.sh .  The arguments mean the following:   qtl_1 : used by  runR.pl  to name its log / output / error files  qtl.R : which R script we want to run  1 : trait column in  input.csv , will also be used to name the output files   As output, this job will create:   a series of .log and .out files, named from the first argument ( qtl_1 ).  several output files named for the trait argument  1 :  perm_combined_1.Rdat ,  perm_summary_1.txt ,  sim_geno_results_1.Rdat ,  qtl_1.Rdat ,  refined_qtl_summary_1.txt ,  refined qtl_1.Rdat , and  fit_qtl_results_1.Rdat",
            "title": "QTL Mapping Step"
        },
        {
            "location": "/materials/day5/part1-ex1-science-intro/#combine-all-output",
            "text": "results_1.tar.gz  is made by running  taritall.sh  with the column number as an argument after the QTL step for that trait finishes, where \"1\" reflects the trait column number in the output above.  Sample execution:   username@host $  ./taritall.sh  1   where  1  is the trait column number.",
            "title": "Combine All Output"
        },
        {
            "location": "/materials/day5/part1-ex1-science-intro/#other-files",
            "text": "The other files in the workflow directory ( permutation2.submit / qtl2.submit  and  permutation3.submit / qtl3.submit ) are used to submit the  permutation  and  qtl  jobs for traits 2 and 3, respectively.",
            "title": "Other Files"
        },
        {
            "location": "/materials/day5/part1-ex2-plan-workflow/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nFriday Exercise 1.2: Plan Joe's Workflow\n\u00b6\n\n\nThis exercise outlines our goal (creating a production workflow) and the steps needed to achieve it. Before starting this section, make sure to first read \nExercise 1.1\n, which has important background information about Joe's intended work and how he has submitted jobs so far.\n\n\nYour Mission\n\u00b6\n\n\nYour goal is to plan out Joe\u2019s workflow based upon small-scale test jobs and, later, to write a full-scale DAG to actually run his workflow in production. In particular, you will need to do the following:\n\n\n\n\n\n\nOptimize number of permutation jobs:\n Joe wants to take advantage of high-throughput parallelization.  This means, for one trait, determining how many jobs to submit for the \npermutation\n step (where each job is calculating a portion of the new total of 100,000 permutations per trait).  For each trait, is it better to run 10 jobs that each generate 10,000 permutations?  Or 1000 jobs that generate 10 permutations?  You'll have to find out.  \n\n\n\n\n\n\nOptimize memory and cpu requests\n: For both the \npermutation\n and \nQTL mapping\n steps, what values should you use for \nrequest_memory\n and \nrequest_cpus\n?\n\n\n\n\n\n\nPut all the pieces together in a single DAG file\n: We want to create a DAG that runs the \npermutation\n and \nQTL mapping\n jobs for each of the three traits, with the necessary \ntar\n scripts running at the appropriate times on the submit server.  \n\n\n\n\n\n\nWe'll be working on the first two steps during this session, with final testing and the last step (DAG creation) after the break in \nExercise 2.1\n.\n\n\n\n\nNote\n\n\nIn what follows, \nthe only files you will need to modify are the submit files\n (and as specifically instructed). You will not need to modify any of the input files, output files, or scripts/executables/programs. It is advisable that you split up some of the work within your pair or group in order to be time-efficient.\n\n\n\n\nDraw a Diagram\n\u00b6\n\n\nIf you haven't already -- draw a digram of Joe's workflow, based on what you \nlearned from Joe\n.  Keep in mind that there are 3 traits for which the \npermutation\n and \nQTL mapping\n steps need to be completed, but these trait analyses are each completely independent (each of the steps described needs to be run for trait 1, trait 2, and trait 3, but they don't overlap at all).  (If you started drawing a diagram while reading the previous page, just extend it here.)  \n\n\nThink about what Joe's intended workflow means for the shape of the DAG, including PARENT-CHILD dependencies for JOBs in the DAG and the fact that the \npermutation\n step could be broken up into multiple processes of fewer total permutations, each.  The tar steps will probably need to be PRE or POST scripts (you decide which is best).\nWe'll come back to this diagram in the \nnext exercise\n after the break, when it's time to construct the full DAG.\n\n\nOptimize Job Components\n\u00b6\n\n\nBefore we assemble the full DAG, we want to optimize each component of the workflow. This will include 3 optimization steps.  \n\n\n\n\nNote\n\n\nEventually we'll want to apply our job optimizations to the submit files for all three traits, but for testing purposes, it's okay to just focus on one trait.  In your group, pick whether you want to use trait 1, 2, or 3 for testing and use the submit files for that trait for all of your tests.  \n\n\n\n\n1. Optimize length of \npermutation\n jobs, test resource use\n\u00b6\n\n\nTo get good HTC scaling, we want the jobs that run for the permutation step to each take somewhere between 10 minutes and 1 hour (ideally ~30 minutes).  So our first optimization task is to determine the number of permutations that can be generated by a single job in this amount of time.  (Eventually - as part of the final workflow - we'll submit batches of these optimized permutation jobs so that we generate 100,000 total permutations for each trait.)  \n\n\nTo determine the right number of permutations per job, you will need to run some test jobs, to see how long it takes a single job to create, say, 10, 100, or 1000 permutations.  \n\n\nTo run these test jobs:\n\n\n\n\nAdd \nrequest_cpus = 1\n according to Joe's indication\n\n\nAdd reasonable first guesses for \nrequest_memory\n and \nrequest_disk\n (say, 1 GB?).\n\n\n\n\nMake a few copies of this submit file so that you can change the last argument (the number of permutations) from \"10000\" to \"10\", \"100\", or \"1000\", e.g. from: \n\n\narguments = arguments = 1_$(Process) run_perm.R 1 $(Process) 10000\n\n\n\n\n\nto something like: \n\n\narguments = arguments = 1_$(Process) run_perm.R 1 $(Process) 10\n\n\n\n\n\nYou'll want to run multiple tests here (\n10\n as given above, and probably at least \n100\n and \n1000\n).  You can split this testing with a partner.  \n\n\n\n\n\n\nGetting ready for the next step:\n\n\n\n\nAfter each set of \npermutation\n tests finishes, you\u2019ll need to use \ntarit.sh\n (with the correct argument) before running the test jobs for the QTL step.\n\n\n\n\n2. Test the \nQTL mapping\n job\n\u00b6\n\n\nOnce you have results from your previous \npermutation\n step testing, you can start testing the \nQTL mapping\n job.  \n\n\n\n\nAdd lines for \nrequest_memory\n, \nrequest_disk\n, and \nrequest_cpus\n to one of the \nQTL\n submit files.  The resource needs (RAM and disk space) and execution time of each \nQTL mapping\n job will likely increase with the total number of permutations from the previous \npermutation\n step, though the execution time will likely still be short (according to Joe). \n\n\nSubmit the modified \nQTL\n submit file.\n\n\nWhen the QTL job completes (hopefully quickly!) look at the log file.  \n\n\n\n\nIf you have time, try running the QTL job again with results from a different permutation test (that may be a different sized input).  Does the disk and memory usage change?  \n\n\n3. Optimization planning\n\u00b6\n\n\nIn order to optimize Joe's overall workflow, we can choose the following values, based on our test jobs from steps 1 and 2:\n\n\n\n\n\n\nHow many permutations should each permutation job create to run in about 30 minutes?  10? 100? 1000?  Something in-between?  Based on your testing, choose an appropriate value. \n\n\n\n\nNote\n\n\nYou can use the \"condor_history\" feature (similar to condor_q, but for completed jobs) to easily view and compare the \"RUNTIME\" for jobs in a \"cluster\" (using the cluster value as an argument to \ncondor_history\n).\n\n\n\n\n\n\n\n\nFor each trait, we want to generate 100,000 permutations.  How many jobs do you need to submit (for each trait) to generate this amount, based on the number of permutations created per job (what you chose in the previous point)?  Essentially, you want \nnumber of jobs\n X \npermutations per job\n to equal 100,000 total permutations...and do that for each of the three phenotype traits. \n\n\n\n\n\n\nMake sure to examine the log files of both your \npermutation\n and \nQTL\n test jobs, so that you can extrapolate how much memory and disk should be requested in the submit files for the full-scale DAG.\n\n\n\n\n\n\nWhen you're done with 3, move on to \nExercise 2.1",
            "title": "Exercise 1.2"
        },
        {
            "location": "/materials/day5/part1-ex2-plan-workflow/#friday-exercise-12-plan-joes-workflow",
            "text": "This exercise outlines our goal (creating a production workflow) and the steps needed to achieve it. Before starting this section, make sure to first read  Exercise 1.1 , which has important background information about Joe's intended work and how he has submitted jobs so far.",
            "title": "Friday Exercise 1.2: Plan Joe's Workflow"
        },
        {
            "location": "/materials/day5/part1-ex2-plan-workflow/#your-mission",
            "text": "Your goal is to plan out Joe\u2019s workflow based upon small-scale test jobs and, later, to write a full-scale DAG to actually run his workflow in production. In particular, you will need to do the following:    Optimize number of permutation jobs:  Joe wants to take advantage of high-throughput parallelization.  This means, for one trait, determining how many jobs to submit for the  permutation  step (where each job is calculating a portion of the new total of 100,000 permutations per trait).  For each trait, is it better to run 10 jobs that each generate 10,000 permutations?  Or 1000 jobs that generate 10 permutations?  You'll have to find out.      Optimize memory and cpu requests : For both the  permutation  and  QTL mapping  steps, what values should you use for  request_memory  and  request_cpus ?    Put all the pieces together in a single DAG file : We want to create a DAG that runs the  permutation  and  QTL mapping  jobs for each of the three traits, with the necessary  tar  scripts running at the appropriate times on the submit server.      We'll be working on the first two steps during this session, with final testing and the last step (DAG creation) after the break in  Exercise 2.1 .   Note  In what follows,  the only files you will need to modify are the submit files  (and as specifically instructed). You will not need to modify any of the input files, output files, or scripts/executables/programs. It is advisable that you split up some of the work within your pair or group in order to be time-efficient.",
            "title": "Your Mission"
        },
        {
            "location": "/materials/day5/part1-ex2-plan-workflow/#draw-a-diagram",
            "text": "If you haven't already -- draw a digram of Joe's workflow, based on what you  learned from Joe .  Keep in mind that there are 3 traits for which the  permutation  and  QTL mapping  steps need to be completed, but these trait analyses are each completely independent (each of the steps described needs to be run for trait 1, trait 2, and trait 3, but they don't overlap at all).  (If you started drawing a diagram while reading the previous page, just extend it here.)    Think about what Joe's intended workflow means for the shape of the DAG, including PARENT-CHILD dependencies for JOBs in the DAG and the fact that the  permutation  step could be broken up into multiple processes of fewer total permutations, each.  The tar steps will probably need to be PRE or POST scripts (you decide which is best).\nWe'll come back to this diagram in the  next exercise  after the break, when it's time to construct the full DAG.",
            "title": "Draw a Diagram"
        },
        {
            "location": "/materials/day5/part1-ex2-plan-workflow/#optimize-job-components",
            "text": "Before we assemble the full DAG, we want to optimize each component of the workflow. This will include 3 optimization steps.     Note  Eventually we'll want to apply our job optimizations to the submit files for all three traits, but for testing purposes, it's okay to just focus on one trait.  In your group, pick whether you want to use trait 1, 2, or 3 for testing and use the submit files for that trait for all of your tests.",
            "title": "Optimize Job Components"
        },
        {
            "location": "/materials/day5/part1-ex2-plan-workflow/#1-optimize-length-of-permutation-jobs-test-resource-use",
            "text": "To get good HTC scaling, we want the jobs that run for the permutation step to each take somewhere between 10 minutes and 1 hour (ideally ~30 minutes).  So our first optimization task is to determine the number of permutations that can be generated by a single job in this amount of time.  (Eventually - as part of the final workflow - we'll submit batches of these optimized permutation jobs so that we generate 100,000 total permutations for each trait.)    To determine the right number of permutations per job, you will need to run some test jobs, to see how long it takes a single job to create, say, 10, 100, or 1000 permutations.    To run these test jobs:   Add  request_cpus = 1  according to Joe's indication  Add reasonable first guesses for  request_memory  and  request_disk  (say, 1 GB?).   Make a few copies of this submit file so that you can change the last argument (the number of permutations) from \"10000\" to \"10\", \"100\", or \"1000\", e.g. from:   arguments = arguments = 1_$(Process) run_perm.R 1 $(Process) 10000  to something like:   arguments = arguments = 1_$(Process) run_perm.R 1 $(Process) 10  You'll want to run multiple tests here ( 10  as given above, and probably at least  100  and  1000 ).  You can split this testing with a partner.      Getting ready for the next step:   After each set of  permutation  tests finishes, you\u2019ll need to use  tarit.sh  (with the correct argument) before running the test jobs for the QTL step.",
            "title": "1. Optimize length of permutation jobs, test resource use"
        },
        {
            "location": "/materials/day5/part1-ex2-plan-workflow/#2-test-the-qtl-mapping-job",
            "text": "Once you have results from your previous  permutation  step testing, you can start testing the  QTL mapping  job.     Add lines for  request_memory ,  request_disk , and  request_cpus  to one of the  QTL  submit files.  The resource needs (RAM and disk space) and execution time of each  QTL mapping  job will likely increase with the total number of permutations from the previous  permutation  step, though the execution time will likely still be short (according to Joe).   Submit the modified  QTL  submit file.  When the QTL job completes (hopefully quickly!) look at the log file.     If you have time, try running the QTL job again with results from a different permutation test (that may be a different sized input).  Does the disk and memory usage change?",
            "title": "2. Test the QTL mapping job"
        },
        {
            "location": "/materials/day5/part1-ex2-plan-workflow/#3-optimization-planning",
            "text": "In order to optimize Joe's overall workflow, we can choose the following values, based on our test jobs from steps 1 and 2:    How many permutations should each permutation job create to run in about 30 minutes?  10? 100? 1000?  Something in-between?  Based on your testing, choose an appropriate value.    Note  You can use the \"condor_history\" feature (similar to condor_q, but for completed jobs) to easily view and compare the \"RUNTIME\" for jobs in a \"cluster\" (using the cluster value as an argument to  condor_history ).     For each trait, we want to generate 100,000 permutations.  How many jobs do you need to submit (for each trait) to generate this amount, based on the number of permutations created per job (what you chose in the previous point)?  Essentially, you want  number of jobs  X  permutations per job  to equal 100,000 total permutations...and do that for each of the three phenotype traits.     Make sure to examine the log files of both your  permutation  and  QTL  test jobs, so that you can extrapolate how much memory and disk should be requested in the submit files for the full-scale DAG.    When you're done with 3, move on to  Exercise 2.1",
            "title": "3. Optimization planning"
        },
        {
            "location": "/materials/day5/part2-ex1-execute-workflow/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nFriday Exercise 2.1: Execute a Production Workflow\n\u00b6\n\n\nIn this exercise, you will:\n\n\n\n\nfinish testing workflow steps,\n\n\nwrite the DAG for the workflow you planned, and\n\n\nsubmit this workflow on \nlearn.chtc.wisc.edu\n\n\n\n\nThere are bonus tasks in \nExercise 2.2\n, if you get through this part quickly, including running the workflow on the Open Science Grid.\n\n\nSteps to Take\n\u00b6\n\n\n\n\nNote\n\n\nWhile one person in your group works on step 1, someone else can work on step 2.\n\n\n\n\n1. Finalize submit files\n\u00b6\n\n\nIn the \nprevious exercise\n we ran some tests to find the optimal values for the \npermutation\n and \nqtl\n job steps.  Now we want to implement these values and confirm that they work. \n\n\n\n\nNote\n\n\nIf your \nWorkflowExercise\n directory has gotten cluttered, feel free to rename it and redownload / untar a fresh copy before proceeding.  \n\n\n\n\n\n\nBased on the values you chose in the last item of the previous exercise, modify \nall\n of the \npermutation\n and \nQTL\n submit files as follows: \n\n\nIn the \npermutation\n submit files change the final value in the \narguments\n line to the number of permutations that takes about 30 minutes to create (item 1 from the last section of the previous exercise).  \n\n\nIn the \npermutation\n submit file, change the \nqueue\n statement to submit the number of jobs you chose in item 2 from the last section of the previous exercise.  \n\n\nIn \nall\n submit files, add or modify requests for cpus, memory and disk based on your tests.  \n\n\n\n\n\n\nWe're now going to do a final test of your modified \npermutation\n and \nqtl\n submit files for one trait.  \n\n\nSubmit one of these newly modified \npermutation\n submit files.  If your estimate of the necessary permutations per process (for a ~30-minute run time) was not close enough, modify and test the \npermutation\n jobs again.\n\n\nOnce the \npermutation\n jobs complete successfully, use \ntarit.sh\n to package the output from the test \npermutation\n step just above. Then run the corresponding \nQTL\n job. As with the just-completed \npermutation\n test, you will only need to submit one of the QTL submit files to confirm the approximate memory and disk needed. Make sure all of the desired output files for the QTL step are created to confirm success.\n\n\n\n\n\n\nDon't forget to test \ntaritall.sh\n after a successful QTL job, to make sure it works as expected.\n\n\n\n\nAfter the optimized \npermutation\n and \nQTL\n tests, copy all output, error, and log files to a new directory to prepare for the production workflow.\n\n\n2. Create a DAG\n\u00b6\n\n\nWrite a single DAG file for the workflow, including:\n\n\n\n\nJOB\n lines for each submit file\n\n\nPARENT x CHILD y\n lines as necessary\n\n\nSCRIPT PRE\n and/or \nSCRIPT POST\n lines for the tar steps\n\n\n\n\n\n\nNote\n\n\nYou may need to think about how each \ntar\n step works for deciding on \"PRE\" or \"POST\" scripts for each.\n\n\n\n\nIf you need a refresher on what a DAG looks like, see \nthis exercise from Thursday\n or the \nHTCondor manual\n\n\nTo \nquickly\n check that you've got the details of the DAG correct, you can modify the \npermutation\n submit files to run a) fewer permutations per job (in the \narguments\n line) and b) fewer jobs overall (after \nqueue\n).  \n\n\n3. Run a production workflow\n\u00b6\n\n\nOnce you have run a quick test of the DAG and you know all the steps are working together, you can submit a full-scale run of the DAG! To do so, make sure your \npermutation\n and \nQTL\n submit files have all of the appropriate values (permutations per job, number of permutation jobs, resource requests) based on your tests.  (Remember, this should be about 100,000 total permutations for each trait.)  Then submit the DAG. If you have any issues, consult the log and out files for the DAG and jobs, and modify your approach at any of the previous steps. While the full-scale DAG is running, you may wish to further detail your drawn workflow, including information regarding resource usage. Share all submit and DAG files with one another so everyone has a copy.\n\n\nIf you have time (even while step 3 is running smoothly), move on to the Bonus Tasks in \nExercise 2.2",
            "title": "Exercise 2.1"
        },
        {
            "location": "/materials/day5/part2-ex1-execute-workflow/#friday-exercise-21-execute-a-production-workflow",
            "text": "In this exercise, you will:   finish testing workflow steps,  write the DAG for the workflow you planned, and  submit this workflow on  learn.chtc.wisc.edu   There are bonus tasks in  Exercise 2.2 , if you get through this part quickly, including running the workflow on the Open Science Grid.",
            "title": "Friday Exercise 2.1: Execute a Production Workflow"
        },
        {
            "location": "/materials/day5/part2-ex1-execute-workflow/#steps-to-take",
            "text": "Note  While one person in your group works on step 1, someone else can work on step 2.",
            "title": "Steps to Take"
        },
        {
            "location": "/materials/day5/part2-ex1-execute-workflow/#1-finalize-submit-files",
            "text": "In the  previous exercise  we ran some tests to find the optimal values for the  permutation  and  qtl  job steps.  Now we want to implement these values and confirm that they work.    Note  If your  WorkflowExercise  directory has gotten cluttered, feel free to rename it and redownload / untar a fresh copy before proceeding.      Based on the values you chose in the last item of the previous exercise, modify  all  of the  permutation  and  QTL  submit files as follows:   In the  permutation  submit files change the final value in the  arguments  line to the number of permutations that takes about 30 minutes to create (item 1 from the last section of the previous exercise).    In the  permutation  submit file, change the  queue  statement to submit the number of jobs you chose in item 2 from the last section of the previous exercise.    In  all  submit files, add or modify requests for cpus, memory and disk based on your tests.      We're now going to do a final test of your modified  permutation  and  qtl  submit files for one trait.    Submit one of these newly modified  permutation  submit files.  If your estimate of the necessary permutations per process (for a ~30-minute run time) was not close enough, modify and test the  permutation  jobs again.  Once the  permutation  jobs complete successfully, use  tarit.sh  to package the output from the test  permutation  step just above. Then run the corresponding  QTL  job. As with the just-completed  permutation  test, you will only need to submit one of the QTL submit files to confirm the approximate memory and disk needed. Make sure all of the desired output files for the QTL step are created to confirm success.    Don't forget to test  taritall.sh  after a successful QTL job, to make sure it works as expected.   After the optimized  permutation  and  QTL  tests, copy all output, error, and log files to a new directory to prepare for the production workflow.",
            "title": "1. Finalize submit files"
        },
        {
            "location": "/materials/day5/part2-ex1-execute-workflow/#2-create-a-dag",
            "text": "Write a single DAG file for the workflow, including:   JOB  lines for each submit file  PARENT x CHILD y  lines as necessary  SCRIPT PRE  and/or  SCRIPT POST  lines for the tar steps    Note  You may need to think about how each  tar  step works for deciding on \"PRE\" or \"POST\" scripts for each.   If you need a refresher on what a DAG looks like, see  this exercise from Thursday  or the  HTCondor manual  To  quickly  check that you've got the details of the DAG correct, you can modify the  permutation  submit files to run a) fewer permutations per job (in the  arguments  line) and b) fewer jobs overall (after  queue ).",
            "title": "2. Create a DAG"
        },
        {
            "location": "/materials/day5/part2-ex1-execute-workflow/#3-run-a-production-workflow",
            "text": "Once you have run a quick test of the DAG and you know all the steps are working together, you can submit a full-scale run of the DAG! To do so, make sure your  permutation  and  QTL  submit files have all of the appropriate values (permutations per job, number of permutation jobs, resource requests) based on your tests.  (Remember, this should be about 100,000 total permutations for each trait.)  Then submit the DAG. If you have any issues, consult the log and out files for the DAG and jobs, and modify your approach at any of the previous steps. While the full-scale DAG is running, you may wish to further detail your drawn workflow, including information regarding resource usage. Share all submit and DAG files with one another so everyone has a copy.  If you have time (even while step 3 is running smoothly), move on to the Bonus Tasks in  Exercise 2.2",
            "title": "3. Run a production workflow"
        },
        {
            "location": "/materials/day5/part2-ex2-workflow-tuning/",
            "text": "pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } \n\n\n\nFriday Exercise 2.2: Workflow Optimization and Scaling\n\u00b6\n\n\nIf you finish the entire workflow and are thirsty for more, try any of the following in whatever order you like:\n\n\nBonus 1\n\u00b6\n\n\nRerun the DAG again with four times the permutations \nper job\n (but fewer processes, keeping a total of 100,000 permutations per trait). Which DAG finished in an overall faster time? Why?\n\n\nBonus 2\n\u00b6\n\n\nYou probably noticed that the job processes from the \npermutation\n step create many log, out, and error files. Modify the \npermutation\n submit files to better organize these files into subdirectories (check out HTCondor's \nIntitialDir\n feature and/or DAG's \nDIR\n features). You may wish to always test the DAG using fewer permutations and permutations processes for a quick turnaround.\n\n\nBonus 3\n\u00b6\n\n\nTake the workflow to the submit server for the Open Science Grid (\nosg-learn.chtc.wisc.edu\n), and run it there.\n\n\nWhat happens?\n\n\n\n\nDo all of the jobs complete successfully? (Check for the desired output files and examine the \nrunR_*.out\n files, which are created by the \nrunR.pl\n wrapper and indicate any errors from the R script, even if HTCondor thought the job was successful.)\n\n\nIf there are any errors (likely related to machine differences and/or software dependencies), implement a RETRY for jobs that fail, accounting for the fact that a DAG \nRETRY\n statement applies to an \nentire\n submit file (\nJOB\n), while you might actually need to account for process-specific errors.\n\n\n\n\nBonus 4\n\u00b6\n\n\nThis isn't actual a bonus, but links to a sample workflow diagram and DAG schematic: \n\n\n\n\nSample Workflow Diagram\n\n\nSample DAG Outline\n\n\n\n\nAnd also how to download and look at a solution workflow: \n\n\nusername@learn $\n wget http://proxy.chtc.wisc.edu/SQUID/osgschool18/CompleteWorkflow.tar.gz\n\nusername@learn $\n tar -xzf CompleteWorkflow.tar.gz",
            "title": "Exercise 2.2"
        },
        {
            "location": "/materials/day5/part2-ex2-workflow-tuning/#friday-exercise-22-workflow-optimization-and-scaling",
            "text": "If you finish the entire workflow and are thirsty for more, try any of the following in whatever order you like:",
            "title": "Friday Exercise 2.2: Workflow Optimization and Scaling"
        },
        {
            "location": "/materials/day5/part2-ex2-workflow-tuning/#bonus-1",
            "text": "Rerun the DAG again with four times the permutations  per job  (but fewer processes, keeping a total of 100,000 permutations per trait). Which DAG finished in an overall faster time? Why?",
            "title": "Bonus 1"
        },
        {
            "location": "/materials/day5/part2-ex2-workflow-tuning/#bonus-2",
            "text": "You probably noticed that the job processes from the  permutation  step create many log, out, and error files. Modify the  permutation  submit files to better organize these files into subdirectories (check out HTCondor's  IntitialDir  feature and/or DAG's  DIR  features). You may wish to always test the DAG using fewer permutations and permutations processes for a quick turnaround.",
            "title": "Bonus 2"
        },
        {
            "location": "/materials/day5/part2-ex2-workflow-tuning/#bonus-3",
            "text": "Take the workflow to the submit server for the Open Science Grid ( osg-learn.chtc.wisc.edu ), and run it there.  What happens?   Do all of the jobs complete successfully? (Check for the desired output files and examine the  runR_*.out  files, which are created by the  runR.pl  wrapper and indicate any errors from the R script, even if HTCondor thought the job was successful.)  If there are any errors (likely related to machine differences and/or software dependencies), implement a RETRY for jobs that fail, accounting for the fact that a DAG  RETRY  statement applies to an  entire  submit file ( JOB ), while you might actually need to account for process-specific errors.",
            "title": "Bonus 3"
        },
        {
            "location": "/materials/day5/part2-ex2-workflow-tuning/#bonus-4",
            "text": "This isn't actual a bonus, but links to a sample workflow diagram and DAG schematic:    Sample Workflow Diagram  Sample DAG Outline   And also how to download and look at a solution workflow:   username@learn $  wget http://proxy.chtc.wisc.edu/SQUID/osgschool18/CompleteWorkflow.tar.gz username@learn $  tar -xzf CompleteWorkflow.tar.gz",
            "title": "Bonus 4"
        },
        {
            "location": "/logistics/",
            "text": "OSG User School 2019 Logistics\n\u00b6\n\n\nThe following pages describe some of the important information about your visit to Madison for the OSG User School.\nPlease read them carefully.  There will be other pages with local details soon.\n\n\n\n\nVisa requirements for non-resident aliens\n\n\nTravel planning to and from Madison\n\n\nHotel information\n\n\nLocal transportation within Madison\n\n\nSchool location\n\n\nMeals at the School\n\n\nFun things to do in Madison\n and \nWednesday activities\n\n\n\n\nAs always: If you have questions, email us at \nuser-school@opensciencegrid.org\n.  Use that email address for all emails\nabout the organization of the OSG School.\n\n\nGeneral Information About the School Schedule\n\u00b6\n\n\nTravel Schedule\n\u00b6\n\n\nFor planning your trip:\n\n\n\n\nArrive on Sunday, July 14, 2019,\n prior to 6 p.m. (if possible).  Classes begin on Monday morning, but there is a\n  welcome dinner on Sunday evening for all participants (including instructors).  This is a nice way to get to know each\n  other and to start the week.\n\n\n\n\n\n\n\n\n\nDepart on Saturday, July 20, 2019,\n any time.  The School ends with another dinner on Friday evening, so it is best\n  to stay that night.\n\n\n\n\nIf we offered to pay for your hotel room, we will pay for the six nights of this schedule.\n\n\nSchool Hours\n\u00b6\n\n\nThe School is Monday through Friday, 9:00 a.m. to about 5:00 p.m.; the lecture hall doors open at about 8:00 a.m. for\nbreakfast, and there will be optional work sessions on Monday, Tuesday, Wednesday, and Thursday evenings.  A detailed\nschedule will be available before the School begins.\n\n\nContact Information\n\u00b6\n\n\nIf you have questions, do not wait to contact us!\n\n\nuser-school@opensciencegrid.org",
            "title": "General information"
        },
        {
            "location": "/logistics/#osg-user-school-2019-logistics",
            "text": "The following pages describe some of the important information about your visit to Madison for the OSG User School.\nPlease read them carefully.  There will be other pages with local details soon.   Visa requirements for non-resident aliens  Travel planning to and from Madison  Hotel information  Local transportation within Madison  School location  Meals at the School  Fun things to do in Madison  and  Wednesday activities   As always: If you have questions, email us at  user-school@opensciencegrid.org .  Use that email address for all emails\nabout the organization of the OSG School.",
            "title": "OSG User School 2019 Logistics"
        },
        {
            "location": "/logistics/#general-information-about-the-school-schedule",
            "text": "",
            "title": "General Information About the School Schedule"
        },
        {
            "location": "/logistics/#travel-schedule",
            "text": "For planning your trip:   Arrive on Sunday, July 14, 2019,  prior to 6 p.m. (if possible).  Classes begin on Monday morning, but there is a\n  welcome dinner on Sunday evening for all participants (including instructors).  This is a nice way to get to know each\n  other and to start the week.     Depart on Saturday, July 20, 2019,  any time.  The School ends with another dinner on Friday evening, so it is best\n  to stay that night.   If we offered to pay for your hotel room, we will pay for the six nights of this schedule.",
            "title": "Travel Schedule"
        },
        {
            "location": "/logistics/#school-hours",
            "text": "The School is Monday through Friday, 9:00 a.m. to about 5:00 p.m.; the lecture hall doors open at about 8:00 a.m. for\nbreakfast, and there will be optional work sessions on Monday, Tuesday, Wednesday, and Thursday evenings.  A detailed\nschedule will be available before the School begins.",
            "title": "School Hours"
        },
        {
            "location": "/logistics/#contact-information",
            "text": "If you have questions, do not wait to contact us!  user-school@opensciencegrid.org",
            "title": "Contact Information"
        },
        {
            "location": "/logistics/personal-info/",
            "text": "Documentation Requirements for Non-Resident Aliens\n\u00b6\n\n\nThis page is for Non-Resident Aliens only.\n  If you are a United States citizen or\npermanent resident, this page does not apply to you.\n\n\nFor the University of Wisconsin to pay for your travel, hotel, or meal expenses, we must have certain personal\ninformation from you.  We collect as little information as possible and do not share it except with University\nadministrative staff who need it.  Most of what we need comes from the online form you completed after accepting our\ninvitation to attend.\n\n\nWhen you come to the School in Madison, we will need to look at and verify your travel documents.  Please bring all\ntravel documents to the School!\n\n\nNote: If you are not being reimbursed for any meals and/or travel for the school or if you are from UW-Madison or the\nMadison area, these documents are not required.\n\n\nTasks To Do Now\n\u00b6\n\n\nPlease check your passport and visa for travel in the United States now.  Make sure that all documents are valid from\nnow and until after the School ends.  If any documents are expired or will expire before the end of the School:\n\n\n\n\nTell us immediately, so that we can help you\n\n\nBegin the process for updating your documents immediately\n\n\nDo whatever you can to expedite the update process\n\n\n\n\nThe University of Wisconsin cannot pay for or reimburse you for costs without valid travel documents.  We have no\ncontrol over this policy and there are no exceptions.\n\n\nIf you are in the United States on a J-1 Scholar visa,\n there\nare extra steps needed to make the University and Federal government happy.  If you have a J-1 visa and have not heard\nfrom us about it already, please email us immediately so that we can help.\n\n\nDocuments to Bring to the School\n\u00b6\n\n\nWhen you come to Madison, you must bring:\n\n\n\n\nPassport\n\n\nU.S. visa\n\n\nU.S. Customs and Border Protection \nform I-94\n\n\nIf you entered the U.S. before 30 April 2013, the I-94 should be stapled into your passport \u2014 do not remove it!\n\n\nIf you entered the U.S. after 30 April 2013, the I-94 is stored electronically;\n  \nyou can request a copy to print from CBP\n\n\nIf you are Canadian, you may use a second form of picture ID instead of the I-94 if you did not obtain an I-94.\n\n\n\n\n\n\nAdditional forms specified in the table below:\n\n\n\n\n\n\n\n\n\n\nIf you have this visa\n\n\nWe will also need\n\n\n\n\n\n\n\n\n\n\nF-1 (Student)\n\n\nForm I-20 (original document, not a copy)\n\n\n\n\n\n\nJ-1 (Visitor)\n\n\nForm DS-2019 (original document, not a copy)\n\n\n\n\n\n\nVisa Waiver Program\n\n\nPaper copy of ESTA Authorization\n\n\n\n\n\n\n\n\nPlease bring all required information and documents to the School, especially on Tuesday, July 16.  School staff will\nmake copies of the documents and return them to you as quickly as possible.  We will announce further details in class.",
            "title": "Visa requirements"
        },
        {
            "location": "/logistics/personal-info/#documentation-requirements-for-non-resident-aliens",
            "text": "This page is for Non-Resident Aliens only.   If you are a United States citizen or\npermanent resident, this page does not apply to you.  For the University of Wisconsin to pay for your travel, hotel, or meal expenses, we must have certain personal\ninformation from you.  We collect as little information as possible and do not share it except with University\nadministrative staff who need it.  Most of what we need comes from the online form you completed after accepting our\ninvitation to attend.  When you come to the School in Madison, we will need to look at and verify your travel documents.  Please bring all\ntravel documents to the School!  Note: If you are not being reimbursed for any meals and/or travel for the school or if you are from UW-Madison or the\nMadison area, these documents are not required.",
            "title": "Documentation Requirements for Non-Resident Aliens"
        },
        {
            "location": "/logistics/personal-info/#tasks-to-do-now",
            "text": "Please check your passport and visa for travel in the United States now.  Make sure that all documents are valid from\nnow and until after the School ends.  If any documents are expired or will expire before the end of the School:   Tell us immediately, so that we can help you  Begin the process for updating your documents immediately  Do whatever you can to expedite the update process   The University of Wisconsin cannot pay for or reimburse you for costs without valid travel documents.  We have no\ncontrol over this policy and there are no exceptions.  If you are in the United States on a J-1 Scholar visa,  there\nare extra steps needed to make the University and Federal government happy.  If you have a J-1 visa and have not heard\nfrom us about it already, please email us immediately so that we can help.",
            "title": "Tasks To Do Now"
        },
        {
            "location": "/logistics/personal-info/#documents-to-bring-to-the-school",
            "text": "When you come to Madison, you must bring:   Passport  U.S. visa  U.S. Customs and Border Protection  form I-94  If you entered the U.S. before 30 April 2013, the I-94 should be stapled into your passport \u2014 do not remove it!  If you entered the U.S. after 30 April 2013, the I-94 is stored electronically;\n   you can request a copy to print from CBP  If you are Canadian, you may use a second form of picture ID instead of the I-94 if you did not obtain an I-94.    Additional forms specified in the table below:      If you have this visa  We will also need      F-1 (Student)  Form I-20 (original document, not a copy)    J-1 (Visitor)  Form DS-2019 (original document, not a copy)    Visa Waiver Program  Paper copy of ESTA Authorization     Please bring all required information and documents to the School, especially on Tuesday, July 16.  School staff will\nmake copies of the documents and return them to you as quickly as possible.  We will announce further details in class.",
            "title": "Documents to Bring to the School"
        },
        {
            "location": "/logistics/travel/",
            "text": "Travel To and From Madison\n\u00b6\n\n\nWhether we offered to pay your travel costs or not, please make sure that we get a copy of your travel plans so that we\nknow when to expect you here and can plan accurately.  (If we offered to pay for your hotel room, we will make a room\nreservation and pay for the six nights of the schedule.)\n\n\nFind the numbered section below that applies to you:\n\n\n1. We Offered to Pay for Your Travel\n\u00b6\n\n\nOur goal is to find reasonable and comfortable travel options for you to come to Madison and return home.  At the same\ntime, we must stay within our budget and follow all University of Wisconsin rules about arranging and paying for your\ntravel costs.  So let\u2019s work together to find something that makes sense for everyone.\n\n\nHere are a few ideas that have worked in the past to help some School travelers:\n\n\n\n\n\n\nIf you are less than 300 miles from Madison, consider driving; we can reimburse mileage and tolls up to a point.  Or\n  look into bus routes, especially from larger cities like Chicago.\n\n\n\n\n\n\nIf you fly, it may be much cheaper to fly to Milwaukee (1\u00bd hours away) or Chicago (2\u00bd hours away), then\n  take a direct bus to Madison.  The buses are very comfortable, have wi-fi, and run frequently.  See below.\n\n\n\n\n\n\nAlso if you fly, be flexible about departure times \u2014 early and late flights are often the least expensive.  Now we do\n  not like very early or very late flights any more than you do, so we will work hard to find reasonable flight times.\n\n\n\n\n\n\nNote:\n Please try to complete your travel plans by mid-June, before rates go up significantly.\n\n\nTravel by Airplane\n\u00b6\n\n\nDo NOT buy your own airline tickets\n.  University rules require that our travel\nagency, Fox World Travel (FWT), purchases your tickets.\n\n\nUse the following information to get air travel tickets:\n\n\n\n\n\n\nGather your travel information: full legal name (as it is written on government IDs), date of birth, phone number;\n  travel dates and starting/ending location(s); and any travel preferences.  Then, contact Fox World Travel directly:\n\n\n\n\nBy email:\n send to \nuwgroups@foxworldtravel.com\n and include your name (e.g., \u201cT. Cartwright\u201d) and\n  \nGroup Code 09UW3749\n in the subject line.\n\n\nBy phone:\n From the U.S., call 866-230-8787 toll free, or internationally, call +1\u00a0920-230-6467.  Select\n  option 4, and tell them you are using \nGroup Code 09UW3749\n\n\n\n\n\n\n\n\nWe must review and approve \nmost\n trips.  FWT is authorized to purchase tickets directly only in the easiest cases.\n  But if the FWT agent says that your trip must be reviewed, do not worry!  It just means that we need to check our\n  budget, make sure that we have considered all reasonable options, and are following all University rules.  Hopefully\n  we can approve your first choice, otherwise we will work with you and FWT to find one or more reasonable options.\n  Some common reasons for a trip needing review are: total trip cost over $475, travel starting and ending at different\n  locations, and travel on dates other than July 14 and 20.\n\n\n\n\n\n\nApproval takes time, so it may take up to a day to get final confirmation.  Airplane tickets cannot be held without\n  purchase over a weekend, so it is best to avoid contacting FWT late on Fridays.  Also note that FWT and\n  UW\u2013Madison are closed on Monday, May 27, for Memorial Day.\n\n\n\n\n\n\nPlease be considerate of the FWT agent(s) you work with.  They work hard to find good options for you, but they must\n  also follow our rules.  If you feel that they are not providing the options that you want, you should email us.  We\n  will try to understand the options that you want and that FWT is providing, and help resolve any issues.  Do not argue\n  with the FWT agents, especially about options you find online\u00a0\u2014 there are many reasons why that option\n  might not be available to us.\n\n\n\n\n\n\nTravel by Bus\n\u00b6\n\n\nFor some nearby locations, or in addition to air travel to Chicago or Milwaukee, it may be helpful to take a bus to\nMadison.  Bus companies that School travelers have used a great deal in the past are:\n\n\n\n\nVan Galder Bus\n, especially from Chicago\n\n\nBadger Bus\n, especially from Milwaukee\n\n\n\n\nYou may purchase bus tickets through Fox World Travel (FWT):\n\n\n\n\nBy email:\n send to \nuwgroups@foxworldtravel.com\n and include your name (e.g., \u201cT. Cartwright\u201d) and\n  \nGroup Code 09UW3749\n in the subject line.\n\n\nBy phone:\n From the U.S., call 866-230-8787 toll free, or internationally, call +1\u00a0920-230-6467.  Select\n  option 4, and tell them you are using \nGroup Code 09UW3749\n\n\n\n\nOr for maximum flexibility, you may purchase bus tickets yourself \nbefore or on\n the day of travel.  If you purchase\nyour own tickets, you must get our approval for the estimated cost first, then request reimbursement from us after the\nSchool.\n\n\nIf you purchase your own tickets, save the original receipt (even if by email).  It is best to have a detailed receipt\n(including your name, itinerary, date of purchase, and total amount paid), but a regular ticket stub (e.g., without your\nname or date) should work fine.  Just get what you can!\n\n\nBe sure to email us with your bus plans, including:\n\n\n\n\nTransportation provider(s) (e.g., Van Galder bus)\n\n\nArrival date and approximate time\n\n\nDeparture date and approximate time\n\n\nArrival and departure location within Madison\n\n\nActual or estimated cost (indicate which)\n\n\n\n\nTravel by Personal Car\n\u00b6\n\n\nIf you are driving to Madison, you will be reimbursed the mileage rate of $0.58 per mile for the shortest round-trip\ndistance (as calculated by Google Maps), plus tolls.  Also, we will pay for parking costs for the week at the hotel in\nMadison (but not elsewhere).  We recommend keeping your receipts for tolls.\n\n\nNote: Due to the high mileage reimbursement rate, driving can be an expensive option!  We reserve the right to limit\nyour total driving reimbursement, so work with us on the details.\n\n\nTo travel by personal car, please check with us first.  We may search for comparable flight options, to make sure that\ndriving is indeed the least expensive method.\n\n\nBe sure to email us with your travel plans as soon as possible.  Try to include:\n\n\n\n\nDeparture date from home, location (for mileage calculation), and approximate time of arrival in Madison\n\n\nDeparture date and approximate time from Madison, and return location (for mileage calculation) if different than above\n\n\n\n\n2. You Are Paying for Your Travel\n\u00b6\n\n\nIf you are paying for your own travel or if someone else is paying for it, go ahead and make your travel arrangements\nnow!  Just remember to arrive on Sunday, July 14, before 6pm and depart on Saturday, July 20 (or else check with us\nfirst).\n\n\nBe sure to email us with your travel plans as soon as possible.  Try to include:\n\n\n\n\nTransportation provider(s) (e.g., airline)\n\n\nArrival date and approximate time\n\n\nDeparture date and approximate time\n\n\nArrival and departure location within Madison (e.g., airport, bus station, etc.)",
            "title": "Travel planning"
        },
        {
            "location": "/logistics/travel/#travel-to-and-from-madison",
            "text": "Whether we offered to pay your travel costs or not, please make sure that we get a copy of your travel plans so that we\nknow when to expect you here and can plan accurately.  (If we offered to pay for your hotel room, we will make a room\nreservation and pay for the six nights of the schedule.)  Find the numbered section below that applies to you:",
            "title": "Travel To and From Madison"
        },
        {
            "location": "/logistics/travel/#1-we-offered-to-pay-for-your-travel",
            "text": "Our goal is to find reasonable and comfortable travel options for you to come to Madison and return home.  At the same\ntime, we must stay within our budget and follow all University of Wisconsin rules about arranging and paying for your\ntravel costs.  So let\u2019s work together to find something that makes sense for everyone.  Here are a few ideas that have worked in the past to help some School travelers:    If you are less than 300 miles from Madison, consider driving; we can reimburse mileage and tolls up to a point.  Or\n  look into bus routes, especially from larger cities like Chicago.    If you fly, it may be much cheaper to fly to Milwaukee (1\u00bd hours away) or Chicago (2\u00bd hours away), then\n  take a direct bus to Madison.  The buses are very comfortable, have wi-fi, and run frequently.  See below.    Also if you fly, be flexible about departure times \u2014 early and late flights are often the least expensive.  Now we do\n  not like very early or very late flights any more than you do, so we will work hard to find reasonable flight times.    Note:  Please try to complete your travel plans by mid-June, before rates go up significantly.",
            "title": "1. We Offered to Pay for Your Travel"
        },
        {
            "location": "/logistics/travel/#travel-by-airplane",
            "text": "Do NOT buy your own airline tickets .  University rules require that our travel\nagency, Fox World Travel (FWT), purchases your tickets.  Use the following information to get air travel tickets:    Gather your travel information: full legal name (as it is written on government IDs), date of birth, phone number;\n  travel dates and starting/ending location(s); and any travel preferences.  Then, contact Fox World Travel directly:   By email:  send to  uwgroups@foxworldtravel.com  and include your name (e.g., \u201cT. Cartwright\u201d) and\n   Group Code 09UW3749  in the subject line.  By phone:  From the U.S., call 866-230-8787 toll free, or internationally, call +1\u00a0920-230-6467.  Select\n  option 4, and tell them you are using  Group Code 09UW3749     We must review and approve  most  trips.  FWT is authorized to purchase tickets directly only in the easiest cases.\n  But if the FWT agent says that your trip must be reviewed, do not worry!  It just means that we need to check our\n  budget, make sure that we have considered all reasonable options, and are following all University rules.  Hopefully\n  we can approve your first choice, otherwise we will work with you and FWT to find one or more reasonable options.\n  Some common reasons for a trip needing review are: total trip cost over $475, travel starting and ending at different\n  locations, and travel on dates other than July 14 and 20.    Approval takes time, so it may take up to a day to get final confirmation.  Airplane tickets cannot be held without\n  purchase over a weekend, so it is best to avoid contacting FWT late on Fridays.  Also note that FWT and\n  UW\u2013Madison are closed on Monday, May 27, for Memorial Day.    Please be considerate of the FWT agent(s) you work with.  They work hard to find good options for you, but they must\n  also follow our rules.  If you feel that they are not providing the options that you want, you should email us.  We\n  will try to understand the options that you want and that FWT is providing, and help resolve any issues.  Do not argue\n  with the FWT agents, especially about options you find online\u00a0\u2014 there are many reasons why that option\n  might not be available to us.",
            "title": "Travel by Airplane"
        },
        {
            "location": "/logistics/travel/#travel-by-bus",
            "text": "For some nearby locations, or in addition to air travel to Chicago or Milwaukee, it may be helpful to take a bus to\nMadison.  Bus companies that School travelers have used a great deal in the past are:   Van Galder Bus , especially from Chicago  Badger Bus , especially from Milwaukee   You may purchase bus tickets through Fox World Travel (FWT):   By email:  send to  uwgroups@foxworldtravel.com  and include your name (e.g., \u201cT. Cartwright\u201d) and\n   Group Code 09UW3749  in the subject line.  By phone:  From the U.S., call 866-230-8787 toll free, or internationally, call +1\u00a0920-230-6467.  Select\n  option 4, and tell them you are using  Group Code 09UW3749   Or for maximum flexibility, you may purchase bus tickets yourself  before or on  the day of travel.  If you purchase\nyour own tickets, you must get our approval for the estimated cost first, then request reimbursement from us after the\nSchool.  If you purchase your own tickets, save the original receipt (even if by email).  It is best to have a detailed receipt\n(including your name, itinerary, date of purchase, and total amount paid), but a regular ticket stub (e.g., without your\nname or date) should work fine.  Just get what you can!  Be sure to email us with your bus plans, including:   Transportation provider(s) (e.g., Van Galder bus)  Arrival date and approximate time  Departure date and approximate time  Arrival and departure location within Madison  Actual or estimated cost (indicate which)",
            "title": "Travel by Bus"
        },
        {
            "location": "/logistics/travel/#travel-by-personal-car",
            "text": "If you are driving to Madison, you will be reimbursed the mileage rate of $0.58 per mile for the shortest round-trip\ndistance (as calculated by Google Maps), plus tolls.  Also, we will pay for parking costs for the week at the hotel in\nMadison (but not elsewhere).  We recommend keeping your receipts for tolls.  Note: Due to the high mileage reimbursement rate, driving can be an expensive option!  We reserve the right to limit\nyour total driving reimbursement, so work with us on the details.  To travel by personal car, please check with us first.  We may search for comparable flight options, to make sure that\ndriving is indeed the least expensive method.  Be sure to email us with your travel plans as soon as possible.  Try to include:   Departure date from home, location (for mileage calculation), and approximate time of arrival in Madison  Departure date and approximate time from Madison, and return location (for mileage calculation) if different than above",
            "title": "Travel by Personal Car"
        },
        {
            "location": "/logistics/travel/#2-you-are-paying-for-your-travel",
            "text": "If you are paying for your own travel or if someone else is paying for it, go ahead and make your travel arrangements\nnow!  Just remember to arrive on Sunday, July 14, before 6pm and depart on Saturday, July 20 (or else check with us\nfirst).  Be sure to email us with your travel plans as soon as possible.  Try to include:   Transportation provider(s) (e.g., airline)  Arrival date and approximate time  Departure date and approximate time  Arrival and departure location within Madison (e.g., airport, bus station, etc.)",
            "title": "2. You Are Paying for Your Travel"
        },
        {
            "location": "/logistics/hotel/",
            "text": ".hi { font-weight: bold; color: #F60; }\n\n\n\n\nHotel Information\n\u00b6\n\n\nWe reserved a block of rooms at an area hotel for participants from outside\nMadison.\n\n\n\n\nBest Western Premier Park Hotel\n22 South\n  Carroll Street, Madison, WI\n+1\u00a0(608)\u00a0285\u20118000\n\n\n\n\nPlease note:\n We will reserve your room for you, so do\nnot contact the hotel yourself to reserve a room.  Exceptions to this rule are\nrare and clearly communicated.\n\n\nOther important hotel information:\n\n\n\n\nThis hotel has a free shuttle service; see the\n  \nlocal transportation page\n for more\n  information\n\n\nBefore the School, we will send you an email with your hotel confirmation\n  number\n\n\nWe pay only for basic room costs\u00a0\u2014 you must provide a credit card\n  to cover extra costs\n\n\nThere is one School participant per room; to have friends or family stay with\n  you, \nplease ask us now",
            "title": "Hotel information"
        },
        {
            "location": "/logistics/hotel/#hotel-information",
            "text": "We reserved a block of rooms at an area hotel for participants from outside\nMadison.   Best Western Premier Park Hotel 22 South\n  Carroll Street, Madison, WI +1\u00a0(608)\u00a0285\u20118000   Please note:  We will reserve your room for you, so do\nnot contact the hotel yourself to reserve a room.  Exceptions to this rule are\nrare and clearly communicated.  Other important hotel information:   This hotel has a free shuttle service; see the\n   local transportation page  for more\n  information  Before the School, we will send you an email with your hotel confirmation\n  number  We pay only for basic room costs\u00a0\u2014 you must provide a credit card\n  to cover extra costs  There is one School participant per room; to have friends or family stay with\n  you,  please ask us now",
            "title": "Hotel Information"
        },
        {
            "location": "/logistics/local-transportation/",
            "text": ".hi { font-weight: bold; color: rgb(255, 102, 00); }\n\n\n\n\nTravel in Madison\n\u00b6\n\n\nYou are responsible for your own transportation within Madison, but we will help\ncoordinate and can reimburse costs between the airport and your hotel.\n\n\nTravel Between the Madison Airport and Your Hotel\n\u00b6\n\n\nThe School hotel (Best Western) provides free shuttle service from and back to\nthe Madison airport, when available.  \nWe will help organize\ngroups\n to take shuttles and taxis, based on arrival and departure times.\nShuttle/taxi groups will be formed and emailed shortly before the School itself.\n\n\nGenerally, the hotel shuttle is free and direct, so it is the best option\nbetween airport and hotel.  If a shuttle is not available, you may take a taxi\nor ride-sharing service and be reimbursed after the School.  There are several\ngood taxi companies in Madison, including\n\nGreen Cab\n,\n\nBadger Cab\n, and\n\nUnion Cab\n, and both \nLyft\n\nand \nUber\n are active in Madison.  Taxis are readily\navailable at the Madison airport, but nearly everywhere else (e.g., from your\nhotel back to the airport), you should contact the company and request a ride or\nmake a reservation in advance.  Note that we can reimburse you only for taxi or\nride-sharing rides between the airport and hotel/campus area.\n\n\nTravel On and Near Campus\n\u00b6\n\n\nThe Best Western hotel is about 1\u00bc miles (2.1 km) from the classroom\nbuilding.  There are many good options for getting between your hotel and the\nSchool, plus other sights, restaurants, and other activities in Madison.\n\n\nWalking\n\u00b6\n\n\nIt is easy to walk in and around the University of Wisconsin\u2013Madison\ncampus.  Use a mapping app or ask us or your hotel for a map.  In particular,\nState Street\u00a0\u2014 which connects the Capitol Square with the UW\ncampus\u00a0\u2014 is full of great shops and restaurants and is worth\nstrolling along while you are here.\n\n\nHotel Shuttle\n\u00b6\n\n\nSee above; the hotel has a free shuttle that can be used to transport students\nbetween the hotel and the School\u2026 and other places.  Ask the hotel for\nmore information and availability.\n\n\nCity of Madison Metro Bus Service\n\u00b6\n\n\nMany \nMadison Metro\n buses stop near the\nhotels and pass through the University of Wisconsin\u2013Madison campus.  Bus\nfare is $2.00, and transfers from one route to another are free.  If you plan to\nuse buses to travel to and from the School every day, contact us, and we may be\nable to buy a bus pass for you or explain how to do so.\n\n\nThe official\n\nMadison Metro Bus page\n\nprovides route maps and times for all Madison buses.  Google Maps and the\nWisconsin phone app also do a good job.\n\n\nTaxis and Ride-Sharing Services\n\u00b6\n\n\nAs listed above in the airport-hotel section, there are several taxi companies\nand ride sharing services in Madison.  \nNote:\n We can reimburse you only for\ntransportation for School purposes, not personal reasons, and even then, only\nfor trips between the airport and either the hotel or the classroom building.\n\n\nMadison BCycle\n\u00b6\n\n\nMadison is a great city to bike in, and there is even a short-term bike rental\nsystem called \nBCycle\n here.  For instance, it is\npossible to pick up a bike at Union South, next to the classroom building, and\nthen drop it off at one of many locations around the central part of Madison.\nSee the website for more details.  While not terribly expensive, this is not a\ncost that we can reimburse.",
            "title": "Local transportation"
        },
        {
            "location": "/logistics/local-transportation/#travel-in-madison",
            "text": "You are responsible for your own transportation within Madison, but we will help\ncoordinate and can reimburse costs between the airport and your hotel.",
            "title": "Travel in Madison"
        },
        {
            "location": "/logistics/local-transportation/#travel-between-the-madison-airport-and-your-hotel",
            "text": "The School hotel (Best Western) provides free shuttle service from and back to\nthe Madison airport, when available.   We will help organize\ngroups  to take shuttles and taxis, based on arrival and departure times.\nShuttle/taxi groups will be formed and emailed shortly before the School itself.  Generally, the hotel shuttle is free and direct, so it is the best option\nbetween airport and hotel.  If a shuttle is not available, you may take a taxi\nor ride-sharing service and be reimbursed after the School.  There are several\ngood taxi companies in Madison, including Green Cab , Badger Cab , and Union Cab , and both  Lyft \nand  Uber  are active in Madison.  Taxis are readily\navailable at the Madison airport, but nearly everywhere else (e.g., from your\nhotel back to the airport), you should contact the company and request a ride or\nmake a reservation in advance.  Note that we can reimburse you only for taxi or\nride-sharing rides between the airport and hotel/campus area.",
            "title": "Travel Between the Madison Airport and Your Hotel"
        },
        {
            "location": "/logistics/local-transportation/#travel-on-and-near-campus",
            "text": "The Best Western hotel is about 1\u00bc miles (2.1 km) from the classroom\nbuilding.  There are many good options for getting between your hotel and the\nSchool, plus other sights, restaurants, and other activities in Madison.",
            "title": "Travel On and Near Campus"
        },
        {
            "location": "/logistics/local-transportation/#walking",
            "text": "It is easy to walk in and around the University of Wisconsin\u2013Madison\ncampus.  Use a mapping app or ask us or your hotel for a map.  In particular,\nState Street\u00a0\u2014 which connects the Capitol Square with the UW\ncampus\u00a0\u2014 is full of great shops and restaurants and is worth\nstrolling along while you are here.",
            "title": "Walking"
        },
        {
            "location": "/logistics/local-transportation/#hotel-shuttle",
            "text": "See above; the hotel has a free shuttle that can be used to transport students\nbetween the hotel and the School\u2026 and other places.  Ask the hotel for\nmore information and availability.",
            "title": "Hotel Shuttle"
        },
        {
            "location": "/logistics/local-transportation/#city-of-madison-metro-bus-service",
            "text": "Many  Madison Metro  buses stop near the\nhotels and pass through the University of Wisconsin\u2013Madison campus.  Bus\nfare is $2.00, and transfers from one route to another are free.  If you plan to\nuse buses to travel to and from the School every day, contact us, and we may be\nable to buy a bus pass for you or explain how to do so.  The official Madison Metro Bus page \nprovides route maps and times for all Madison buses.  Google Maps and the\nWisconsin phone app also do a good job.",
            "title": "City of Madison Metro Bus Service"
        },
        {
            "location": "/logistics/local-transportation/#taxis-and-ride-sharing-services",
            "text": "As listed above in the airport-hotel section, there are several taxi companies\nand ride sharing services in Madison.   Note:  We can reimburse you only for\ntransportation for School purposes, not personal reasons, and even then, only\nfor trips between the airport and either the hotel or the classroom building.",
            "title": "Taxis and Ride-Sharing Services"
        },
        {
            "location": "/logistics/local-transportation/#madison-bcycle",
            "text": "Madison is a great city to bike in, and there is even a short-term bike rental\nsystem called  BCycle  here.  For instance, it is\npossible to pick up a bike at Union South, next to the classroom building, and\nthen drop it off at one of many locations around the central part of Madison.\nSee the website for more details.  While not terribly expensive, this is not a\ncost that we can reimburse.",
            "title": "Madison BCycle"
        },
        {
            "location": "/logistics/location/",
            "text": "School Location\n\u00b6\n\n\nThe school will be held at the\n\nUniversity of Wisconsin\u2013Madison\n in the\n\nComputer Sciences Department\n, located at\n\n1210 West Dayton Street, Madison, WI, 53706\n.\nThe main classroom is Room 1240.\n\n\nSee the \nlocal transportation\n page for\nsuggestions about getting around Madison.\n\n\nComputer Sciences Building, Room 1240\n\u00b6\n\n\nSchool sessions, breakfasts, and lunches are held in \nRoom 1240\n:\n\n\n\n\nEnter the Computer Sciences building through the glass doors on Dayton Street\n\n\nTurn left and go through two sets of double doors\n\n\nWalk down the hallway just a bit\n\n\nRoom 1240 is on the right, up the ramp or a few stairs\n\n\n\n\nThe Computer Sciences building is a bit confusing, especially if you use other\nentrances.  However, there are signs that guide you to room 1240.  You can\nalways ask people for directions if you get lost.",
            "title": "School location"
        },
        {
            "location": "/logistics/location/#school-location",
            "text": "The school will be held at the University of Wisconsin\u2013Madison  in the Computer Sciences Department , located at 1210 West Dayton Street, Madison, WI, 53706 .\nThe main classroom is Room 1240.  See the  local transportation  page for\nsuggestions about getting around Madison.",
            "title": "School Location"
        },
        {
            "location": "/logistics/location/#computer-sciences-building-room-1240",
            "text": "School sessions, breakfasts, and lunches are held in  Room 1240 :   Enter the Computer Sciences building through the glass doors on Dayton Street  Turn left and go through two sets of double doors  Walk down the hallway just a bit  Room 1240 is on the right, up the ramp or a few stairs   The Computer Sciences building is a bit confusing, especially if you use other\nentrances.  However, there are signs that guide you to room 1240.  You can\nalways ask people for directions if you get lost.",
            "title": "Computer Sciences Building, Room 1240"
        },
        {
            "location": "/logistics/meals/",
            "text": "Meals and Food During the School\n\u00b6\n\n\nAll catered, group meals that the School provides will include vegan and vegetarian options.  If you indicated other\ndietary needs in the survey, we will address those on a case-by-case basis as well.\n\n\nGroup Dinners\n\u00b6\n\n\nOn Sunday evening, there will be a welcome dinner at 6:30 p.m. at Union South (see \nthe schedule\npage\n for details).  Before dinner, someone from the School will be in the lobby of\nthe hotel to meet people.  Then the group at each hotel can walk together to the dinner location.  Or you may travel to\nthe location on your own.\n\n\nOn Friday evening, there will be a closing dinner at 6:30 p.m. at the Fluno Center.\n\n\nSchool Meals\n\u00b6\n\n\nThe School pays for your meals during the School.\n\n\nFirst, we provide the following meals directly:\n\n\n\n\nBreakfasts on Monday through Friday \u2014 held in Computer Sciences Room 1240, the same room as the school sessions\n\n\nLunches on Monday through Friday \u2014 held near the lecture hall\n\n\nThe group dinners described above\n\n\n\n\nFor dinners Monday through Thursday, \nif you are coming from out of town\n, we can reimburse you after the School.  In\nthat case:\n\n\n\n\nKeep receipts for your dinners \u2013 if anything so that you remember how much meals cost!\n\n\nWe can reimburse \nup to\n $27.00, including tax and tip\n\n\nIf it is not on the receipt, be sure to write your tip amount yourself, so you do not forget\n\n\nWe cannot pay for any alcohol, but non-alcoholic drinks are OK \u2014 so pay for any alcohol on a separate bill\n\n\nWe will explain the reimbursement process at the end of the School\n\n\n\n\nAgain, if you live in or near Madison and are part of the UW\u2013Madison community, by University rules we cannot pay for\nyour own dinners during the School.\n\n\nMenus\n\u00b6\n\n\nBelow are the menus for the School-provided meals.  Not all options for meeting dietary needs are listed, because the\nvendors provide them at their discretion.  If you have concerns or questions, email us or talk to any of the organizers\nin person.\n\n\nSunday\n\u00b6\n\n\nDinner:\n\n\n\n\nDinner rolls (vegetarian)\n\n\nCold beverages\n\n\nMixed greens with balsamic and ranch dressings (vegetarian)\n\n\nWisconsin cheese board with crackers (vegetarian, including some GF ones)\n\n\nCurried stew (vegan)\n\n\nBlackened salmon with grapefruit slaw\n\n\nRoasted BBQ chicken (bone in)\n\n\nSeasonal vegetable (vegetarian)\n\n\nSour cream hash brown potatoes (vegetarian)\n\n\nBasmati rice (vegetarian)\n\n\n\n\nAll items will be labeled for dietary concerns.\n\n\nMonday\n\u00b6\n\n\nBreakfast:\n\n\n\n\nBaked peaches and cream French toast\n\n\nFruit salad\n\n\nRoasted red potatoes\n\n\nCoffee, tea, juice, bottled water\n\n\n\n\nLunch:\n\n\n\n\nLasagna (beef and vegetarian)\n\n\nTossed salad\n\n\nGarlic bread sticks\n\n\nAssorted cookies/dessert\n\n\nAssorted soda, bottled water\n\n\n\n\nAfternoon:\n\n\n\n\nCoffee, tea\n\n\n\n\nTuesday\n\u00b6\n\n\nBreakfast (lighter than Monday/Wednesday/Friday):\n\n\n\n\nPastries and baked goods (muffins, bagels, danish)\n\n\nVanilla yogurt and granola\n\n\nCoffee, tea, juice, bottled water\n\n\n\n\nLunch:\n\n\n\n\nBuild-your-own tacos (chicken, beef, and vegan taco filling)\n\n\nTaco toppings, tortillas, and hard taco shells\n\n\nRefried pinto beans\n\n\nSpanish style rice\n\n\nTortilla chips and salsa\n\n\nAssorted cookies/dessert\n\n\nAssorted soda, bottled water\n\n\n\n\nAfternoon:\n\n\n\n\nCoffee, tea\n\n\n\n\nWednesday\n\u00b6\n\n\nBreakfast:\n\n\n\n\nCountry scrambled egg and cheese casserole\n\n\nFruit salad\n\n\nBreakfast potatoes\n\n\nCoffee, tea, juice, bottled water\n\n\n\n\nLunch:\n\n\n\n\nGrilled chicken breast sandwiches\n\n\nBlack bean burgers\n\n\nAll-American potato salad\n\n\nItalian pasta salad\n\n\nPotato chips and French onion dip\n\n\nAssorted cookies/dessert\n\n\nAssorted soda, bottled water\n\n\n\n\nThursday\n\u00b6\n\n\nBreakfast (lighter than Monday/Wednesday/Friday):\n\n\n\n\nPastries and baked goods (muffins, bagels, danish)\n\n\nVanilla yogurt and granola\n\n\nCoffee, tea, juice, bottled water\n\n\n\n\nLunch:\n\n\n\n\nEnchiladas (chicken and vegan)\n\n\nRefried pinto beans\n\n\nSpanish style rice\n\n\nTortilla chips and salsa\n\n\nAssorted cookies/dessert\n\n\nAssorted soda, bottled water\n\n\n\n\nAfternoon:\n\n\n\n\nCoffee, tea\n\n\n\n\nFriday\n\u00b6\n\n\nBreakfast:\n\n\n\n\nSpinach and feta quiche\n\n\nFruit salad\n\n\nPastries and baked goods (muffins, bagels, danish)\n\n\nCoffee, tea, juice, bottled water\n\n\n\n\nLunch:\n\n\n\n\nCurry (chicken and vegetarian)\n\n\nRice\n\n\nVegetable samosas with yogurt sauce\n\n\nNaan bread\n\n\nAssorted cookies/dessert\n\n\nAssorted soda, bottled water\n\n\n\n\nAfternoon:\n\n\n\n\nCoffee, tea\n\n\n\n\nDinner:\n\n\n\n\nLinguine and cavatappi pastas (vegetarian)\n\n\nWild mushroom ravioli with butternut cream sauce (vegetarian)\n\n\nGrilled chicken breast (gluten-free)\n\n\nHomemade beef and pork meatballs\n\n\nItalian vegetable blend (vegan)\n\n\nBreadsticks\n\n\nMarinara (vegan/gluten-free) and Alfredo sauces (vegetarian)\n\n\nSalad\n\n\nTiramisu (vegetarian)\n\n\nCannoli (vegetarian)\n\n\nAssorted beverages",
            "title": "Meals at the School"
        },
        {
            "location": "/logistics/meals/#meals-and-food-during-the-school",
            "text": "All catered, group meals that the School provides will include vegan and vegetarian options.  If you indicated other\ndietary needs in the survey, we will address those on a case-by-case basis as well.",
            "title": "Meals and Food During the School"
        },
        {
            "location": "/logistics/meals/#group-dinners",
            "text": "On Sunday evening, there will be a welcome dinner at 6:30 p.m. at Union South (see  the schedule\npage  for details).  Before dinner, someone from the School will be in the lobby of\nthe hotel to meet people.  Then the group at each hotel can walk together to the dinner location.  Or you may travel to\nthe location on your own.  On Friday evening, there will be a closing dinner at 6:30 p.m. at the Fluno Center.",
            "title": "Group Dinners"
        },
        {
            "location": "/logistics/meals/#school-meals",
            "text": "The School pays for your meals during the School.  First, we provide the following meals directly:   Breakfasts on Monday through Friday \u2014 held in Computer Sciences Room 1240, the same room as the school sessions  Lunches on Monday through Friday \u2014 held near the lecture hall  The group dinners described above   For dinners Monday through Thursday,  if you are coming from out of town , we can reimburse you after the School.  In\nthat case:   Keep receipts for your dinners \u2013 if anything so that you remember how much meals cost!  We can reimburse  up to  $27.00, including tax and tip  If it is not on the receipt, be sure to write your tip amount yourself, so you do not forget  We cannot pay for any alcohol, but non-alcoholic drinks are OK \u2014 so pay for any alcohol on a separate bill  We will explain the reimbursement process at the end of the School   Again, if you live in or near Madison and are part of the UW\u2013Madison community, by University rules we cannot pay for\nyour own dinners during the School.",
            "title": "School Meals"
        },
        {
            "location": "/logistics/meals/#menus",
            "text": "Below are the menus for the School-provided meals.  Not all options for meeting dietary needs are listed, because the\nvendors provide them at their discretion.  If you have concerns or questions, email us or talk to any of the organizers\nin person.",
            "title": "Menus"
        },
        {
            "location": "/logistics/meals/#sunday",
            "text": "Dinner:   Dinner rolls (vegetarian)  Cold beverages  Mixed greens with balsamic and ranch dressings (vegetarian)  Wisconsin cheese board with crackers (vegetarian, including some GF ones)  Curried stew (vegan)  Blackened salmon with grapefruit slaw  Roasted BBQ chicken (bone in)  Seasonal vegetable (vegetarian)  Sour cream hash brown potatoes (vegetarian)  Basmati rice (vegetarian)   All items will be labeled for dietary concerns.",
            "title": "Sunday"
        },
        {
            "location": "/logistics/meals/#monday",
            "text": "Breakfast:   Baked peaches and cream French toast  Fruit salad  Roasted red potatoes  Coffee, tea, juice, bottled water   Lunch:   Lasagna (beef and vegetarian)  Tossed salad  Garlic bread sticks  Assorted cookies/dessert  Assorted soda, bottled water   Afternoon:   Coffee, tea",
            "title": "Monday"
        },
        {
            "location": "/logistics/meals/#tuesday",
            "text": "Breakfast (lighter than Monday/Wednesday/Friday):   Pastries and baked goods (muffins, bagels, danish)  Vanilla yogurt and granola  Coffee, tea, juice, bottled water   Lunch:   Build-your-own tacos (chicken, beef, and vegan taco filling)  Taco toppings, tortillas, and hard taco shells  Refried pinto beans  Spanish style rice  Tortilla chips and salsa  Assorted cookies/dessert  Assorted soda, bottled water   Afternoon:   Coffee, tea",
            "title": "Tuesday"
        },
        {
            "location": "/logistics/meals/#wednesday",
            "text": "Breakfast:   Country scrambled egg and cheese casserole  Fruit salad  Breakfast potatoes  Coffee, tea, juice, bottled water   Lunch:   Grilled chicken breast sandwiches  Black bean burgers  All-American potato salad  Italian pasta salad  Potato chips and French onion dip  Assorted cookies/dessert  Assorted soda, bottled water",
            "title": "Wednesday"
        },
        {
            "location": "/logistics/meals/#thursday",
            "text": "Breakfast (lighter than Monday/Wednesday/Friday):   Pastries and baked goods (muffins, bagels, danish)  Vanilla yogurt and granola  Coffee, tea, juice, bottled water   Lunch:   Enchiladas (chicken and vegan)  Refried pinto beans  Spanish style rice  Tortilla chips and salsa  Assorted cookies/dessert  Assorted soda, bottled water   Afternoon:   Coffee, tea",
            "title": "Thursday"
        },
        {
            "location": "/logistics/meals/#friday",
            "text": "Breakfast:   Spinach and feta quiche  Fruit salad  Pastries and baked goods (muffins, bagels, danish)  Coffee, tea, juice, bottled water   Lunch:   Curry (chicken and vegetarian)  Rice  Vegetable samosas with yogurt sauce  Naan bread  Assorted cookies/dessert  Assorted soda, bottled water   Afternoon:   Coffee, tea   Dinner:   Linguine and cavatappi pastas (vegetarian)  Wild mushroom ravioli with butternut cream sauce (vegetarian)  Grilled chicken breast (gluten-free)  Homemade beef and pork meatballs  Italian vegetable blend (vegan)  Breadsticks  Marinara (vegan/gluten-free) and Alfredo sauces (vegetarian)  Salad  Tiramisu (vegetarian)  Cannoli (vegetarian)  Assorted beverages",
            "title": "Friday"
        },
        {
            "location": "/logistics/fun/",
            "text": "Fun Things to Do in Madison\n\u00b6\n\n\nSummer is a great time to be in Madison.  Below are just a few suggestions of things to do or look into while you are\nhere.\n\n\n\n\n\n\nMemorial Union\n is a great place to hang out and a reasonable walk\n    from the School hotel.  Many people like to get a drink and relax on\n    \nthe Terrace\n, which looks out over Lake Mendota.\n    Check \nthe Wisconsin Union events calendar\n for more\n    information.\n\n\n\n\n\n\nState Street connects the UW\u2013Madison campus to the State Capitol building (and the hotel), and is a\n    student-friendly street and outdoor pedestrian mall with tons of great businesses \u2014 restaurants, stores, bars, and\n    more.\n\n\n\n\n\n\nCatch a show at the historic \nOrpheum Theatre\n.\n\n\n\n\n\n\nCheck out the \nOverture Center for the Arts\n.\n\n\n\n\n\n\nRead \nThe Isthumus Daily Page\n for current events in town.\n\n\n\n\n\n\nGo to a \nfree outdoor classical music \u201cConcert on the Square\u201d on\n    Wednesday\n, by the State\n    Capitol building.\n\n\n\n\n\n\nCheck out other ideas on our page of suggestions for \nWednesday afternoon activities\n.",
            "title": "Fun in Madison"
        },
        {
            "location": "/logistics/fun/#fun-things-to-do-in-madison",
            "text": "Summer is a great time to be in Madison.  Below are just a few suggestions of things to do or look into while you are\nhere.    Memorial Union  is a great place to hang out and a reasonable walk\n    from the School hotel.  Many people like to get a drink and relax on\n     the Terrace , which looks out over Lake Mendota.\n    Check  the Wisconsin Union events calendar  for more\n    information.    State Street connects the UW\u2013Madison campus to the State Capitol building (and the hotel), and is a\n    student-friendly street and outdoor pedestrian mall with tons of great businesses \u2014 restaurants, stores, bars, and\n    more.    Catch a show at the historic  Orpheum Theatre .    Check out the  Overture Center for the Arts .    Read  The Isthumus Daily Page  for current events in town.    Go to a  free outdoor classical music \u201cConcert on the Square\u201d on\n    Wednesday , by the State\n    Capitol building.    Check out other ideas on our page of suggestions for  Wednesday afternoon activities .",
            "title": "Fun Things to Do in Madison"
        },
        {
            "location": "/logistics/wednesday-activities/",
            "text": "Wednesday Afternoon Options\n\u00b6\n\n\nAfter Wednesday morning instruction and lunch, there are no required instructional activities.  Therefore, you are free\nto explore Madison, rest, or stick around and talk to instructors or work on your projects.\n\n\nPlease note that \nSchool-Organized Activities\n are all free and walkable.  If you are interested in the \nOn Your Own\n\nactivities also listed, or prefer not to walk, you will be responsible for you own transportation costs and activity\ncosts.  We cannot reimburse transportation costs for these Wednesday afternoon activities.\n\n\nOptional Learning Activities\n\u00b6\n\n\nAfter lunch, some optional learning activities will continue in Room 1240.\n\n\n\n\n\n\nHTMap Demo & Workshop\n\n\nStarting at 1:30 p.m., this a brief demo of \nHTMap\n, \na Python package for easily running Python functions as HTCondor jobs.\nThe demo itself will last about 15 minutes, \nwith time afterwards to discuss possible applications to your own projects if desired.\n\n\n\n\n\n\nSchool-Organized Activities\n\u00b6\n\n\nAfter lunch, select School instructors will accompany interested students for the below activities, which are all free\nand walkable.  You are welcome to arrange alternative transportation, but are responsible for any transportation costs.\n\n\n\n\n\n\nUW\u2013Madison Campus Tour\n\n\nStarting at 3 p.m., this approximately 1-hour tour will begin at Union South and end at Memorial Union.  FREE.\nGather at the Union South information desk starting around 2:50 p.m.\n\n\n\n\n\n\nTour of Wisconsin State Capitol\n\n\nTours start at 1, 2, 3, and 4 p.m. and last about 45 minutes.  FREE.  \n\n\n\n\n\n\nConcert on the Square\n\n\nThe Wisconsin Chamber Orchestra plays classical music outdoors near the State Capitol building.  We will start\ngathering at 5 p.m.; the music starts at 7 p.m. and lasts an hour and a half to two hours.  One of the instructors\nwill be holding a spot on the Capitol lawn with blankets well in advance, so there should be room for many people.\n\n\n\n\n\n\nOn Your Own\n\u00b6\n\n\nThough the OSG User School has organized some activities (above) you are also welcome to strike out on your own.  Just\nremember that you'll be responsible for any/all costs, including transportation.  Some ideas are listed below.\n\n\nFREE!\n\u00b6\n\n\n\n\n\n\nChazen Museum of Art\n.\n Large collections of art from a variety of eras.  On campus\n    (750 Univesity Avenue) and free (donations welcome).\n\n\n\n\n\n\nUW\u2013Madison Geology Museum\n.\n Large collection of geological specimens.\n    Across Dayton Street from the School building (1215 Dayton Street).\n\n\n\n\n\n\nL.R. Ingersoll Physics Museum\n.\n Small museum of Physics objects\n    and demonstrations.  Very short walk from the School building: Chamberlin Hall, 1150 University Avenue.\n\n\n\n\n\n\nHenry Vilas Zoo\n.\n One-mile walk south of Computer Sciences: 702 South Randall\n    Avenue.  Bus routes 4 and 44 go there.\n\n\n\n\n\n\nSome Cost\n\u00b6\n\n\n\n\n\n\nThe Sett at Union South\n.\n Recreation\n    activities of varying costs, including a sports pub, rock climbing, pool, bowling.  Next to the School building.\n\n\n\n\n\n\nTour of First Unitarian Society\u2019s Meeting House\n.\n The\n    Landmark Auditorium was designed by Frank Lloyd Wright.  900 University Bay Drive, $15 per person ($12.50 if booked\n    online in advance), up to 10 people.  Bus routes 2, 10, 15, 56, 57, 70, 71, and 72.\n\n\n\n\n\n\nOlbrich Botanical Gardens\n.\n 3330 Atwood Avenue; 16 acres outdoor (FREE); indoor: $2\n    conservatory; $8 butterfly house.  Bus routes 3, 37, and 38.\n\n\n\n\n\n\nCapital Brewery Tour\n.\n 7734 Terrace Ave., Middleton; $7 walk-in, includes\n    commemorative glass and 4 samples.  Bus routes 70, 71, and 72.\n\n\n\n\n\n\nPaddling rentals on Lake Mendota\n.\n\n    Paddling rentals, including paddleboards, kayak, and canoes.  Memorial Union Terrace, $17 per hour.",
            "title": "Wednesday activities"
        },
        {
            "location": "/logistics/wednesday-activities/#wednesday-afternoon-options",
            "text": "After Wednesday morning instruction and lunch, there are no required instructional activities.  Therefore, you are free\nto explore Madison, rest, or stick around and talk to instructors or work on your projects.  Please note that  School-Organized Activities  are all free and walkable.  If you are interested in the  On Your Own \nactivities also listed, or prefer not to walk, you will be responsible for you own transportation costs and activity\ncosts.  We cannot reimburse transportation costs for these Wednesday afternoon activities.",
            "title": "Wednesday Afternoon Options"
        },
        {
            "location": "/logistics/wednesday-activities/#optional-learning-activities",
            "text": "After lunch, some optional learning activities will continue in Room 1240.    HTMap Demo & Workshop  Starting at 1:30 p.m., this a brief demo of  HTMap , \na Python package for easily running Python functions as HTCondor jobs.\nThe demo itself will last about 15 minutes, \nwith time afterwards to discuss possible applications to your own projects if desired.",
            "title": "Optional Learning Activities"
        },
        {
            "location": "/logistics/wednesday-activities/#school-organized-activities",
            "text": "After lunch, select School instructors will accompany interested students for the below activities, which are all free\nand walkable.  You are welcome to arrange alternative transportation, but are responsible for any transportation costs.    UW\u2013Madison Campus Tour  Starting at 3 p.m., this approximately 1-hour tour will begin at Union South and end at Memorial Union.  FREE.\nGather at the Union South information desk starting around 2:50 p.m.    Tour of Wisconsin State Capitol  Tours start at 1, 2, 3, and 4 p.m. and last about 45 minutes.  FREE.      Concert on the Square  The Wisconsin Chamber Orchestra plays classical music outdoors near the State Capitol building.  We will start\ngathering at 5 p.m.; the music starts at 7 p.m. and lasts an hour and a half to two hours.  One of the instructors\nwill be holding a spot on the Capitol lawn with blankets well in advance, so there should be room for many people.",
            "title": "School-Organized Activities"
        },
        {
            "location": "/logistics/wednesday-activities/#on-your-own",
            "text": "Though the OSG User School has organized some activities (above) you are also welcome to strike out on your own.  Just\nremember that you'll be responsible for any/all costs, including transportation.  Some ideas are listed below.",
            "title": "On Your Own"
        },
        {
            "location": "/logistics/wednesday-activities/#free",
            "text": "Chazen Museum of Art .  Large collections of art from a variety of eras.  On campus\n    (750 Univesity Avenue) and free (donations welcome).    UW\u2013Madison Geology Museum .  Large collection of geological specimens.\n    Across Dayton Street from the School building (1215 Dayton Street).    L.R. Ingersoll Physics Museum .  Small museum of Physics objects\n    and demonstrations.  Very short walk from the School building: Chamberlin Hall, 1150 University Avenue.    Henry Vilas Zoo .  One-mile walk south of Computer Sciences: 702 South Randall\n    Avenue.  Bus routes 4 and 44 go there.",
            "title": "FREE!"
        },
        {
            "location": "/logistics/wednesday-activities/#some-cost",
            "text": "The Sett at Union South .  Recreation\n    activities of varying costs, including a sports pub, rock climbing, pool, bowling.  Next to the School building.    Tour of First Unitarian Society\u2019s Meeting House .  The\n    Landmark Auditorium was designed by Frank Lloyd Wright.  900 University Bay Drive, $15 per person ($12.50 if booked\n    online in advance), up to 10 people.  Bus routes 2, 10, 15, 56, 57, 70, 71, and 72.    Olbrich Botanical Gardens .  3330 Atwood Avenue; 16 acres outdoor (FREE); indoor: $2\n    conservatory; $8 butterfly house.  Bus routes 3, 37, and 38.    Capital Brewery Tour .  7734 Terrace Ave., Middleton; $7 walk-in, includes\n    commemorative glass and 4 samples.  Bus routes 70, 71, and 72.    Paddling rentals on Lake Mendota . \n    Paddling rentals, including paddleboards, kayak, and canoes.  Memorial Union Terrace, $17 per hour.",
            "title": "Some Cost"
        }
    ]
}